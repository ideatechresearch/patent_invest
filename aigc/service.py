import httpx, aiohttp, aiofiles, requests
import asyncio
import logging
import json, uuid, os, time
from typing import Callable, Optional, Type, Dict, List, Tuple, Any, Union, Awaitable, AsyncIterator, AsyncGenerator, \
    Coroutine

from contextlib import asynccontextmanager
from starlette.requests import Request
from starlette.responses import JSONResponse, Response
# from starlette.routing import Route, Mount
from logging.handlers import RotatingFileHandler
from utils import async_to_sync, generate_hash_key, parse_database_uri, is_port_open, chunks_iterable, get_file_type_wx
from config import Config, AI_Models, model_api_keys

# Config.load('config.yaml')
# if os.getenv('AIGC_DEBUG', '0').lower() in ('1', 'true', 'yes'):
#     Config.debug()
import pymysql, aiomysql
from neo4j import GraphDatabase, AsyncGraphDatabase
from dask.distributed import Client as DaskClient, LocalCluster
from qdrant_client import AsyncQdrantClient, QdrantClient
from openai import AsyncOpenAI, OpenAI
from fastmcp import FastMCP, Context as MCPContext, Client as MCPClient, settings
import oss2
# https://gofastmcp.com/servers/context
from redis.asyncio import Redis, StrictRedis, ConnectionPool
from functools import partial, wraps
import inspect

_httpx_clients: Dict[str, httpx.AsyncClient] = {}
_graph_driver: Optional[GraphDatabase] = None
# _graph_driver_lock = asyncio.Lock()  # é˜²æ­¢å¹¶å‘åˆå§‹åŒ–
_redis_client: Optional[Redis] = None  # StrictRedis(host='localhost', port=6379, db=0)
_redis_pool: Optional[ConnectionPool] = None
_dask_cluster: Optional[LocalCluster | str] = None
_dask_client: Optional[DaskClient] = None
logger = logging.getLogger(__name__)
AI_Client: Dict[str, Optional[AsyncOpenAI]] = {}  # OpenAI
QD_Client = AsyncQdrantClient(host=Config.QDRANT_HOST, grpc_port=Config.QDRANT_GRPC_PORT,
                              prefer_grpc=True) if Config.QDRANT_GRPC_PORT else AsyncQdrantClient(url=Config.QDRANT_URL)

AliyunBucket = oss2.Bucket(oss2.Auth(Config.ALIYUN_oss_AK_ID, Config.ALIYUN_oss_Secret_Key), Config.ALIYUN_oss_endpoint,
                           Config.ALIYUN_Bucket_Name)
mcp = FastMCP(name="FastMCP Server")  # Create a server instance,main_mcp


# dependencies=["pandas", "matplotlib", "requests"]


def setup_logging(file_name="app.log", level=logging.WARNING):
    log_handler = RotatingFileHandler(file_name, maxBytes=1_000_000, backupCount=3, encoding='utf-8')
    # æ–‡ä»¶æ—¥å¿—logging.FileHandler('errors.log')
    logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=level, encoding='utf-8',
                        handlers=[
                            logging.StreamHandler(),  # è¾“å‡ºåˆ°ç»ˆç«¯,æ§åˆ¶å°è¾“å‡º
                            log_handler
                        ])


def get_httpx_client(time_out: float = None, proxy: str = None) -> httpx.AsyncClient:
    # @asynccontextmanager
    key = proxy or "default"
    global _httpx_clients
    if key not in _httpx_clients or _httpx_clients[key].is_closed:
        transport = httpx.AsyncHTTPTransport(proxy=proxy or None)
        limits = httpx.Limits(max_connections=Config.MAX_HTTP_CONNECTIONS,
                              max_keepalive_connections=Config.MAX_KEEPALIVE_CONNECTIONS)
        timeout = httpx.Timeout(timeout=time_out or Config.HTTP_TIMEOUT_SEC, read=60.0, write=30.0, connect=5.0)
        _httpx_clients[key] = httpx.AsyncClient(transport=transport, limits=limits, timeout=timeout)
    # try:
    #     yield _httpx_clients[key] #Depends(get_httpx_client)
    # finally:
    #     # æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œå…³é—­å®¢æˆ·ç«¯ï¼Œå› ä¸ºå®ƒæ˜¯å•ä¾‹ï¼Œå…¨å±€ç”¨çš„
    #     pass

    return _httpx_clients[key]


async def shutdown_httpx():
    for key, _client in _httpx_clients.items():
        if _client and not _client.is_closed:
            await _client.aclose()


def get_redis(db: int = 0) -> Optional[Redis]:
    global _redis_client, _redis_pool
    if _redis_client is None:
        _redis_pool = ConnectionPool(host=Config.REDIS_HOST, port=Config.REDIS_PORT, db=db,
                                     decode_responses=True,  # è‡ªåŠ¨è§£ç ä¸ºå­—ç¬¦ä¸²
                                     max_connections=Config.REDIS_MAX_CONCURRENT
                                     )
        _redis_client = Redis(connection_pool=_redis_pool)

    return _redis_client


async def shutdown_redis():
    global _redis_client, _redis_pool
    if _redis_client:
        await _redis_client.aclose()
        _redis_client = None

    if _redis_pool:
        await _redis_pool.disconnect()
        _redis_pool = None


async def check_redis_connection(redis):
    try:
        await redis.ping()
        print("âœ… Redis connected.")
        return True
    except ConnectionError as e:
        print(f"âŒ Redis connection failed: {e}")
    return False


async def get_redis_connection():
    redis = get_redis()
    if not await check_redis_connection(redis):
        return None
    return redis


async def get_redis_retry(redis, key: str, retry: int = 3, delay: float = 0.1):
    for attempt in range(retry):
        try:
            return await redis.get(key)
        except Exception as e:
            print(f"[Redis GET] attempt {attempt + 1} failed: {e}")
            await asyncio.sleep(delay)
    raise Exception(f"Redis GET failed after {retry} retries.")


async def get_redis_value(redis, key: str) -> Union[Dict, list, str, int, float, None]:
    """
    Redis å€¼è·å–æ–¹æ³•ï¼Œæ”¯æŒé€šé…ç¬¦æŸ¥è¯¢å’Œ JSON è§£æ

    Args:
        key: è¦æŸ¥è¯¢çš„é”®åï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰
        redis: Redis å®¢æˆ·ç«¯å®ä¾‹

    Returns:
        æ ¹æ®å†…å®¹è¿”å›è§£æåçš„æ•°æ®
    """

    def parse_json(value: str) -> Any:
        if not isinstance(value, str):
            return value

        try:
            return json.loads(value)
        except (json.JSONDecodeError, TypeError):
            return value

    try:
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é€šé…ç¬¦
        if any(x in key for x in ("*", "?", "[")):
            keys = await redis.keys(key)
            if not keys:
                return None

            values = await redis.mget(*keys)
            result = {}
            for k, v in zip(keys, values):
                # é”®åè§£ç 
                key_str = k.decode("utf-8") if isinstance(k, bytes) else k
                if v is None:
                    result[key_str] = None
                else:  # å€¼å¤„ç†å’Œ JSON è§£æå°è¯•
                    result[key_str] = parse_json(v.decode("utf-8") if isinstance(v, bytes) else v)

            return result

        else:
            # å•ä¸ªé”®æŸ¥è¯¢
            value = await redis.get(key)
            if value is None:
                return None
            if isinstance(value, bytes):
                value = value.decode('utf-8')

            return parse_json(value)

    except (ConnectionError, TimeoutError) as e:
        # Connect call failed,redis.exceptions.ConnectionError
        raise Exception(f"Redis è¿æ¥å¤±è´¥,detail:{e}")
    except Exception as e:
        raise Exception(f"Redis æŸ¥è¯¢é”™è¯¯,detail:{e}")


async def scan_from_redis(redis, key: str = "funcmeta", user: str = None, batch_count: int = 0):
    """
    ä» Redis ä¸­è·å–åŒ¹é…çš„æ‰€æœ‰å…ƒæ•°æ®è®°å½•ï¼Œæ”¯æŒ scan æˆ– keys æ–¹å¼ã€‚

    Args:
        redis: Redis å®ä¾‹ã€‚
        key: Redis key å‰ç¼€ï¼ˆå¦‚ "funcmeta"ï¼‰ã€‚
        user: ç”¨æˆ· IDï¼ˆå¯é€‰ï¼‰ï¼Œè‹¥æŒ‡å®šåˆ™æ‹¼æ¥ä¸º funcmeta:{user}:*
        batch_count: æ¯æ‰¹ scan çš„æ•°é‡ï¼ˆå¤§äº 0 ä½¿ç”¨ scanï¼Œå¦åˆ™ç”¨ keysï¼‰ã€‚

    Returns:
        List[dict]: åŒ¹é…åˆ°çš„ JSON æ•°æ®åˆ—è¡¨ã€‚
    """
    if user:
        match_pattern = f"{key}:{user}:*"
    else:
        match_pattern = f"{key}:*"

    data = []
    if batch_count > 0:
        cursor = b'0'
        while cursor:
            cursor, keys = await redis.scan(cursor=cursor, match=match_pattern, count=batch_count)
            if keys:
                values = await redis.mget(*keys)
                data.extend(json.loads(v) for v in values if v)
    else:
        keys = await redis.keys(match_pattern)
        if keys:
            cached_values = await redis.mget(*keys)
            data = [json.loads(v) for v in cached_values if v]  # set(cached_values
    return data


async def stream_to_redis(redis, batch: list, key: str = 'streams'):
    pipe = redis.pipeline()
    for stream_id, chunk in batch:
        stream_name = f"{key}:{stream_id % 3}"  # é€‰æ‹©åˆ†ç‰‡æµå
        pipe.xadd(stream_name, fields=chunk, id="*", maxlen=10000, approximate=True)  # {"data": json.dumps(chunk)}
    try:
        await pipe.execute()
    except Exception as e:
        raise


async def run_with_lock(func_call: Callable, *args, lock_timeout: int = 600, lock_key: str = None, redis=None,
                        **kwargs):
    redis = redis or get_redis()
    if not redis:
        return await func_call(*args, **kwargs)
    func_name = getattr(func_call, "__qualname__", getattr(func_call, "__name__", repr(func_call)))
    lock_key = lock_key or f'lock:{func_name}'
    lock_value = str(uuid.uuid4())  # str(time.time())ï¼Œæ¯ä¸ªworkerä½¿ç”¨å”¯ä¸€çš„lock_value
    lock_acquired = await redis.set(lock_key, lock_value, nx=True, ex=lock_timeout)
    if not lock_acquired:
        logger.info(f"âš ï¸ åˆ†å¸ƒå¼é”å·²è¢«å ç”¨ï¼Œè·³è¿‡ä»»åŠ¡: {func_name}")
        return None

    result = None
    try:
        logger.info(f"ğŸ”’ è·å–é”æˆåŠŸï¼Œå¼€å§‹æ‰§è¡Œä»»åŠ¡: {func_name}")
        result = await func_call(*args, **kwargs)
    except Exception as e:
        logger.error(f"âš ï¸ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {func_name} -> {e}")
    finally:
        # current_lock_value = await redis.get(redis_key)
        # if current_lock_value and current_lock_value == lock_value:
        #     await redis.delete(redis_key)
        # ä½¿ç”¨ Lua è„šæœ¬ä¿è¯åŸå­æ€§ï¼Œç¡®ä¿åªæœ‰é”æŒæœ‰è€…èƒ½é‡Šæ”¾ï¼Œåªæœ‰æœ€åˆè·å–é”çš„é‚£ä¸ªworkeræ‰èƒ½æˆåŠŸåˆ é™¤é”
        lua_script = """
           if redis.call("get", KEYS[1]) == ARGV[1] then
               return redis.call("del", KEYS[1])
           else
               return 0
           end
           """
        await redis.eval(lua_script, 1, lock_key, lock_value)

    return result


def distributed_lock(lock_timeout: int = 600, redis_key: Optional[str] = None):
    '''
    locked_operation = distributed_lock(lock_timeout=300)(my_task)    æ‰‹åŠ¨åº”ç”¨è£…é¥°å™¨,ä¸´æ—¶éœ€è¦åŠ é”çš„å‡½æ•°
    await locked_operation(123, {"name": "John"})
    @distributed_lock(300) é•¿æœŸä½¿ç”¨çš„ä»»åŠ¡å‡½æ•°
    :param lock_timeout:
    :param redis_key:
    :return:
    '''

    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            redis = get_redis()
            if not redis:
                return await func(*args, **kwargs)

            lock_key = redis_key or f"lock:{func.__qualname__}"

            # å°è¯•è·å–é”
            async with with_distributed_lock(lock_key, None, lock_timeout * 1000, redis) as lock_acquired:
                if not lock_acquired:
                    logger.info(f"âš ï¸ åˆ†å¸ƒå¼é”å·²è¢«å ç”¨ï¼Œè·³è¿‡ä»»åŠ¡: {func.__qualname__}")
                    return None

                logger.info(f"ğŸ”’ è·å–é”æˆåŠŸï¼Œå¼€å§‹æ‰§è¡Œä»»åŠ¡: {func.__qualname__}")
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    logger.error(f"âš ï¸ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {func.__qualname__} -> {e}")
                    raise

        return wrapper

    return decorator


@asynccontextmanager
async def with_distributed_lock(lock_key: str, lock_value: str = None, lock_timeout=10000, redis=None,
                                release: bool = True):
    """
    åˆ†å¸ƒå¼é”ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    lock_timeout # æ¯«ç§’
    ç”¨æ³•ï¼š
    async with with_distributed_lock( "my_lock",None,10000,redis_conn) as lock_acquired:
        if lock_acquired:
            # æ‰§è¡Œå—ä¿æŠ¤çš„æ“ä½œ
    """
    redis_conn = redis or get_redis()
    lock_identifier = lock_value or str(uuid.uuid4())
    acquired = await redis_conn.set(lock_key, lock_identifier, nx=True, px=lock_timeout)

    try:
        yield acquired
    finally:
        if acquired and release:
            # åŸå­æ€§é‡Šæ”¾é”
            script = """
            if redis.call("get", KEYS[1]) == ARGV[1] then
                return redis.call("del", KEYS[1])
            else
                return 0
            end
            """
            await redis_conn.eval(script, 1, lock_key, lock_identifier)


async def is_main_worker(worker_id: str = None, redis=None):
    async with with_distributed_lock("lock:main_worker", worker_id, lock_timeout=60 * 1000, redis=redis,
                                     release=False) as acquired:
        return acquired


def get_dask_client(cluster=None, n_workers: int = 3):
    global _dask_client, _dask_cluster
    if _dask_client:
        return _dask_client

    if cluster is None:
        if not _dask_cluster:
            if is_port_open("127.0.0.1", 8786):
                _dask_cluster = "tcp://127.0.0.1:8786"
                print("Dask Scheduler ç«¯å£è¢«å ç”¨ï¼Œè¿æ¥å·²æœ‰é›†ç¾¤")
            else:
                # å¯åŠ¨æœ¬åœ° Dask é›†ç¾¤,æœ¬æœºä¸Šå¯åŠ¨è‹¥å¹²ä¸ª worker è¿›ç¨‹,ä½¿ç”¨çº¿ç¨‹è€Œä¸æ˜¯è¿›ç¨‹ï¼ˆå’Œä¸€ä¸ª scheduler) http://127.0.0.1:8787
                _dask_cluster = LocalCluster(scheduler_port=8786, dashboard_address=":8787",
                                             n_workers=n_workers, threads_per_worker=1, processes=True)

        cluster = _dask_cluster

    try:
        _dask_client = DaskClient(cluster, timeout=3)  # åˆ›å»ºDaskå®¢æˆ·ç«¯, compression=None
        print(_dask_client.ncores())  # _dask_client.get_versions(check=True)
    except Exception as e:
        print(f"âŒ æ— æ³•åˆ›å»º Dask Client: {e}")
        # raise RuntimeError(f"âŒ æ— æ³•åˆ›å»º Dask Client: {e}")

    return _dask_client


def close_dask_client():
    global _dask_client, _dask_cluster
    # print("Closing Dask client or cluster...")
    if _dask_client:
        _dask_client.close()
        _dask_client = None
    if _dask_cluster:
        if isinstance(_dask_cluster, LocalCluster):
            _dask_cluster.close()
        _dask_cluster = None


def get_neo_driver():
    global _graph_driver
    if _graph_driver is None:
        _graph_driver = AsyncGraphDatabase.driver(uri=Config.NEO_URI,  # uri="bolt://localhost:7687"
                                                  auth=(Config.NEO_Username, Config.NEO_Password),
                                                  max_connection_lifetime=3600,  # å•è¿æ¥ç”Ÿå‘½å‘¨æœŸ
                                                  max_connection_pool_size=30,  # æœ€å¤§è¿æ¥æ± æ•°é‡
                                                  connection_timeout=30  # è¶…æ—¶
                                                  )
    return _graph_driver


def get_w3():
    try:
        from web3 import Web3

        w3 = Web3(
            Web3.HTTPProvider(f'https://mainnet.infura.io/v3/{Config.INFURA_PROJECT_ID}'))  # ("http://127.0.0.1:8545")
        return w3
    except ImportError:
        print("[Web3 Init] Web3 not installed.")
    except Exception as e:
        print(f"[Web3 Init] Failed to get web3: {e}")

    return None


def error_logger(extra_msg=None):
    """
    é”™è¯¯æ—¥å¿—è£…é¥°å™¨ @error_logger()
    """

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                # logger.debug(f"Entering {func.__name__}")
                return func(*args, **kwargs)
            except Exception as e:
                msg = f"Error in {func.__name__}: {e}"
                if extra_msg:
                    msg += f" | Extra: {extra_msg}"
                logger.error(msg, exc_info=True)
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸

        return wrapper

    return decorator


def async_error_logger(max_retries: int = 0, delay: int | float = 1, backoff: int | float = 2,
                       exceptions: Type[Exception] | tuple[Type[Exception], ...] = Exception,
                       extra_msg: str = None, log_level: int = logging.ERROR):
    """
    å¼‚æ­¥å‡½æ•°çš„é”™è¯¯é‡è¯•å’Œæ—¥å¿—è®°å½•è£…é¥°å™¨

    å‚æ•°:
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆä¸å«é¦–æ¬¡å°è¯•ï¼‰ï¼Œé»˜è®¤ä¸º 0ï¼Œè¡¨ç¤ºä¸é‡è¯•ï¼›è®¾ä¸º 1 è¡¨ç¤ºå¤±è´¥åé‡è¯•ä¸€æ¬¡ï¼ˆå…±å°è¯• 2 æ¬¡ï¼‰ã€‚
        delay (int/float): åˆå§‹å»¶è¿Ÿæ—¶é—´(ç§’)ï¼Œé»˜è®¤ä¸º1
        backoff (int/float): å»¶è¿Ÿæ—¶é—´å€å¢ç³»æ•°ï¼Œé»˜è®¤ä¸º2
        exceptions (Exception/tuple): è¦æ•è·çš„å¼‚å¸¸ç±»å‹ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰å¼‚å¸¸
        log_level (int): æ—¥å¿—çº§åˆ«
    """

    def decorator(func: Callable[..., Awaitable]):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ä¼˜å…ˆä½¿ç”¨è°ƒç”¨æ—¶çš„å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰å°±ç”¨è£…é¥°å™¨é»˜è®¤å€¼
            _max_retries = kwargs.pop("max_retries", max_retries)
            _delay = kwargs.pop("delay", delay)
            _backoff = kwargs.pop("backoff", backoff)
            _extra_msg = kwargs.pop("extra_msg", extra_msg)

            attempt = 0
            current_delay = _delay

            while True:
                try:
                    return await func(*args, **kwargs)
                except exceptions as e:
                    attempt += 1
                    msg = f"Async function {func.__name__} failed with error: {str(e)}."
                    if _extra_msg:
                        msg += f" | Extra: {_extra_msg}"
                    if attempt > _max_retries:
                        logger.log(log_level, f"{msg} After {_max_retries} retries", exc_info=True)
                        raise  # é‡è¯•æ¬¡æ•°ç”¨å°½åé‡æ–°æŠ›å‡ºå¼‚å¸¸

                    logger.log(log_level, f"{msg} Retrying {attempt}/{_max_retries} in {current_delay} seconds...")
                    await asyncio.sleep(current_delay)
                    current_delay *= _backoff  # æŒ‡æ•°é€€é¿

        return wrapper

    return decorator


def task_processor_worker(max_retries: int = 0, delay: float = 1.0, backoff: int | float = 2,
                          timeout: int | float = -1):
    """
    ä»»åŠ¡å¤„ç†è£…é¥°å™¨ï¼Œå°è£…ä»»åŠ¡æ‰§è¡Œã€é‡è¯•å’Œå¼‚å¸¸å¤„ç†é€»è¾‘
    @task_processor_worker(max_retries=3, delay=0.5,timeout=10)
    async def handle_task(task: tuple[Any, int], queue: asyncio.Queue[tuple[Any, int]], **kwargs):
        print(f"å¤„ç†ä»»åŠ¡: {task}")
        ...

        task_data,X,Y= task
        result = await async_processing(task_data, **kwargs)
        if task == "error":
            queue.put_nowait(task_data)
            #raise ValueError("å¼‚å¸¸")
        return True

    @task_processor_worker(max_retries=2)
    async def handle_task(task: tuple, **kwargs):
        print(f"ä»»åŠ¡æ•°æ®: {task}")
        if result == "error":
            raise ValueError("å¼‚å¸¸")

    queue = asyncio.Queue()  asyncio.Queue æ˜¯å†…å­˜å¯¹è±¡ï¼Œåªå­˜åœ¨äºå½“å‰è¿›ç¨‹çš„äº‹ä»¶å¾ªç¯ä¸­
    worker_task = asyncio.create_task(handle_task(queue=queue))
    å¯åŠ¨ worker:
        workers_task_background = [asyncio.create_task(handle_task(queue)) for _ in range(4)],
        #await asyncio.gather(*tasks)
        #await queue.put((f"task-{i}", 0))
        #await queue.put(None)

    :param queue:asyncio.Queue()
    :param func:æ³¨å…¥å¤„ç†å‡½æ•° async def f(task: tuple, **kwargs),è¿”å›å€¼ true,false..
    :param max_retries,0ä¸é‡è¯•
    :param delay: é‡è¯•å‰å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    :param backoff:
    :param timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    :return:
    """

    def decorator(func: Callable):
        async def wrapper(queue: asyncio.Queue, **kwargs):
            # åˆ¤æ–­ func æ˜¯å¦æ¥æ”¶ queue å‚æ•°
            accepts_queue = 'queue' in inspect.signature(func).parameters

            while True:
                task_data = await queue.get()
                if task_data is None:
                    logger.info("[Info] Received shutdown signal")
                    queue.task_done()
                    break

                try:
                    process_func = func
                    if max_retries > 0:
                        process_func = async_error_logger(max_retries=max_retries, delay=delay, backoff=backoff)(
                            func)  # å¤„ç†é‡è¯•é€»è¾‘
                    if timeout > 0:
                        if accepts_queue:
                            success = await asyncio.wait_for(process_func(task_data, queue, **kwargs), timeout=timeout)
                        else:
                            success = await asyncio.wait_for(process_func(task_data, **kwargs), timeout=timeout)
                    else:
                        if accepts_queue:
                            success = await process_func(task_data, queue, **kwargs)
                        else:
                            success = await process_func(task_data, **kwargs)  # æ‰§è¡Œå®é™…çš„ä»»åŠ¡å¤„ç†

                    if not success:
                        logger.error(f"[Task Failed] {task_data}")

                except asyncio.TimeoutError:
                    logger.error(f"[Timeout] Task {task_data} timed out")
                    # await queue.put(task),.put_nowait(x)
                except asyncio.CancelledError:
                    logger.info("[Cancel] Worker shutting down...")
                    break
                except Exception as e:
                    logger.error(f"[Error] Unexpected error processing task: {e}")
                finally:
                    queue.task_done()  # å¿…é¡»è°ƒç”¨ï¼Œæ ‡è®°ä»»åŠ¡å®Œæˆ

        return wrapper

    return decorator


def start_consumer_workers(queue: asyncio.Queue, worker_func: Callable, num_workers: int = 4, **kwargs) -> list:
    # worker å¯åŠ¨å™¨ï¼Œworkers_task_background
    return [asyncio.create_task(worker_func(queue=queue, **kwargs)) for _ in range(num_workers)]


async def stop_worker(queue: asyncio.Queue, worker_tasks: list):
    '''ä¼˜é›…åœæ­¢æ‰€æœ‰ worker'''
    try:
        await queue.join()  # ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º

        logger.info("All tasks processed. Stopping consumers...")
        for _ in worker_tasks:
            await queue.put(None)  # å‘é€åœæ­¢ä¿¡å·
    except Exception as e:
        logger.error(f"[Tasks Error] {e}, attempting to cancel workers...")
        for c in worker_tasks:
            c.cancel()

    finally:
        # ç»Ÿä¸€å›æ”¶æ‰€æœ‰ä»»åŠ¡
        await asyncio.gather(*worker_tasks, return_exceptions=True)


def async_timer_cron(interval: float = 60) -> Callable[
    [Callable[..., Coroutine[Any, Any, None]]], Callable[..., Coroutine[Any, Any, None]]]:
    """
    å®šæ—¶æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡çš„è£…é¥°å™¨

    :param interval: æ‰§è¡Œé—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
    """

    def decorator(func: Callable[..., Coroutine[Any, Any, None]]) -> Callable[..., Coroutine[Any, Any, None]]:
        @wraps(func)
        async def wrapper(*args, **kwargs) -> None:
            while True:
                try:
                    await func(*args, **kwargs)
                except Exception as e:
                    logging.error(f"Error in timed task {func.__name__}: {e}")

                await asyncio.sleep(interval)

        return wrapper

    return decorator


def upload_file_to_oss(bucket, file_obj, object_name, expires: int = 604800):
    """
      ä¸Šä¼ æ–‡ä»¶åˆ° OSS æ”¯æŒ `io` å¯¹è±¡ã€‚
      :param bucket: OSS bucket å®ä¾‹
      :param file_obj: æ–‡ä»¶å¯¹è±¡ï¼Œå¯ä»¥æ˜¯ `io.BytesIO` æˆ– `io.BufferedReader`
      :param object_name: OSS ä¸­çš„å¯¹è±¡å
      :param expires: ç­¾åæœ‰æ•ˆæœŸï¼Œé»˜è®¤ä¸€å‘¨ï¼ˆç§’ï¼‰
    """
    file_obj.seek(0, os.SEEK_END)
    total_size = file_obj.tell()  # os.path.getsize(file_path)
    file_obj.seek(0)
    if total_size > 1024 * 1024 * 16:
        part_size = oss2.determine_part_size(total_size, preferred_size=128 * 1024)
        upload_id = bucket.init_multipart_upload(object_name).upload_id
        parts = []
        part_number = 1
        offset = 0
        while offset < total_size:
            size_to_upload = min(part_size, total_size - offset)
            result = bucket.upload_part(object_name, upload_id, part_number,
                                        oss2.SizedFileAdapter(file_obj, size_to_upload))
            parts.append(oss2.models.PartInfo(part_number, result.etag, size=size_to_upload, part_crc=result.crc))
            offset += size_to_upload
            part_number += 1

        # å®Œæˆåˆ†ç‰‡ä¸Šä¼ 
        bucket.complete_multipart_upload(object_name, upload_id, parts)
    else:
        # OSS ä¸Šçš„å­˜å‚¨è·¯å¾„, æœ¬åœ°å›¾ç‰‡è·¯å¾„
        bucket.put_object(object_name, file_obj)
        # bucket.put_object_from_file(object_name, str(file_path))

    if 0 < expires <= 604800:  # å¦‚æœç­¾åsigned_URL
        url = bucket.sign_url("GET", object_name, expires=expires)
    else:  # ä½¿ç”¨åŠ é€ŸåŸŸå
        url = f"{Config.ALIYUN_Bucket_Domain}/{object_name}"
        # bucket.bucket_name
    # è·å–æ–‡ä»¶å¯¹è±¡
    # result = bucket.get_object(object_name)
    # result.read()è·å–æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹,result.headerså…ƒæ•°æ®ï¼ˆå¤´éƒ¨ä¿¡æ¯ï¼‰
    return url


# è·å–æ–‡ä»¶åˆ—è¡¨
def oss_list_files(bucket, prefix='upload/', max_keys: int = 100, max_pages: int = 1):
    """
    åˆ—å‡º OSS ä¸­çš„æ–‡ä»¶ã€‚
    :param bucket: oss2.Bucket å®ä¾‹
    :param prefix: æ–‡ä»¶åå‰ç¼€ï¼Œç”¨äºç­›é€‰
    :param max_keys: æ¯æ¬¡è¿”å›çš„æœ€å¤§æ•°é‡
    :param max_pages:
    :return: æ–‡ä»¶ååˆ—è¡¨
    """
    file_list = []
    if max_pages <= 1:
        for obj in oss2.ObjectIterator(bucket, prefix=prefix, max_keys=max_keys):
            file_list.append(obj.key)
    else:
        i = 0
        next_marker = ''
        while i < max_pages:
            result = bucket.list_objects(prefix=prefix, max_keys=max_keys, marker=next_marker)
            for obj in result.object_list:
                file_list.append(obj.key)
            if not result.is_truncated:  # å¦‚æœæ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œé€€å‡ºå¾ªç¯
                break
            next_marker = result.next_marker
            i += 1

    return file_list


class BaseMysql:
    def __init__(self, host: str, user: str, password: str, db_name: str,
                 port: int = 3306, charset: str = "utf8mb4"):
        self.host = host
        self.user = user
        self.password = password
        self.db_name = db_name
        self.port = port
        self.charset = charset

    def close(self):
        """åŒæ­¥å®ç°ä¸­å…³é—­è¿æ¥ï¼›å­ç±»å®ç°"""
        raise NotImplementedError

    async def close_pool(self):
        """å¼‚æ­¥å®ç°ä¸­å…³é—­è¿æ¥æ± ï¼›å­ç±»å®ç°"""
        raise NotImplementedError

    def table_schema(self, table_name: str) -> tuple[str, tuple]:
        if not table_name:
            raise ValueError("[Schema] è¡¨åä¸èƒ½ä¸ºç©º")
        sql = """
                 SELECT column_name, data_type, is_nullable, column_type, column_comment
                 FROM information_schema.columns
                 WHERE table_schema = %s AND table_name = %s
                 ORDER BY ordinal_position
             """
        params = (self.db_name, table_name)
        return sql, params

    @staticmethod
    def format_value(value) -> Union[str, int, float, bool, None]:
        """
        ä¿ç•™ä½ åŸæ¥çš„ format_value è¡Œä¸ºï¼ˆdict->json, list/tuple->\n\n if all str else json, set->; or jsonï¼‰
        """
        if value is None or isinstance(value, (str, int, float, bool)):
            return value

        if isinstance(value, dict):
            return json.dumps(value, ensure_ascii=False)  # å¤„ç†å­—å…¸ç±»å‹ â†’ JSONåºåˆ—åŒ–, indent=2

        if isinstance(value, (tuple, list)):
            if all(isinstance(item, str) for item in value):
                return "\n\n---\n\n".join(value)  # "\n\n"
            return json.dumps(value, ensure_ascii=False)  # éå…¨å­—ç¬¦ä¸²å…ƒç´ åˆ™JSONåºåˆ—åŒ–

        if isinstance(value, set):
            if all(isinstance(item, str) for item in value):
                return ";".join(sorted(value))
            return json.dumps(list(value), ensure_ascii=False)

        return str(value)  # å…¶ä»–ç±»å‹ä¿æŒåŸæ · (Noneç­‰)

    @staticmethod
    def build_insert(table_name: str, params_data: dict, update_fields: list[str] | None = None) -> tuple[str, tuple]:
        """
        ç”Ÿæˆæ’å…¥ SQLï¼ˆå¯é€‰ ON DUPLICATE KEY UPDATEï¼‰

        Args:
            table_name: è¡¨å
            params_data: æ•°æ®å­—å…¸
            update_fields: å†²çªæ—¶æ›´æ–°çš„å­—æ®µï¼ŒNone è¡¨ç¤ºé»˜è®¤æ›´æ–°éä¸»é”®å­—æ®µ

        Returns:
            tuple: (sql, values)
        """
        if not params_data:
            raise ValueError("params_data ä¸èƒ½ä¸ºç©º")

        fields = list(params_data.keys())
        values = tuple(params_data.values())  # [tuple(row[f] for f in fields) for row in params_data]
        field_str = ', '.join(f"`{field}`" for field in fields)  # columns_str
        placeholder_str = ', '.join(['%s'] * len(fields))
        sql = f"INSERT INTO `{table_name}` ({field_str}) VALUES ({placeholder_str})"

        if update_fields is None:
            update_fields = [f for f in fields if f.lower() not in ("id", "created_at", "created_time")]

        if update_fields:
            sql += " AS new"
            update_str = ', '.join(f"`{field}` = new.`{field}`" for field in update_fields)
            if "updated_at" in fields and "updated_at" not in update_fields:
                update_str += ", `updated_at` = CURRENT_TIMESTAMP"
            sql += f" ON DUPLICATE KEY UPDATE {update_str}"

        return sql, values

    @staticmethod
    def get_dataframe(db_config: dict, sql: str, params: tuple | dict = None):
        import pandas as pd
        # with create_engine(SQLALCHEMY_DATABASE_URI).connect() as conn:
        conn = pymysql.connect(**db_config)
        try:
            with conn.cursor() as cursor:
                cursor.execute(sql, params or ())
                rows = cursor.fetchall()
                cols = [desc[0] for desc in cursor.description]
                return pd.DataFrame(rows, columns=cols)
        except Exception as e:
            return pd.read_sql(sql, conn, params=params)
        finally:
            conn.close()

    @staticmethod
    def query_dataframe_process(conn, sql: str, process_chunk: Callable = None, params: tuple | dict = None,
                                chunk_size: int = 100000, aggregate: bool = False):
        """
        é€šç”¨å¤§è¡¨åˆ†é¡µæŸ¥è¯¢å¹¶å¤„ç†æ¯å—æ•°æ®ã€‚

        Args:
            conn: æ•°æ®åº“è¿æ¥å¯¹è±¡ï¼ˆéœ€æ”¯æŒ conn.cursor()ï¼‰
            sql (str): åŸå§‹ SQL æŸ¥è¯¢è¯­å¥ï¼ˆä¸å« LIMIT å’Œ OFFSETï¼‰
            process_chunk (Callable): å¯¹æ¯ä¸ªæ‰¹æ¬¡çš„è¿›è¡Œå¤„ç†çš„å‡½æ•°
            chunk_size (int): æ¯æ‰¹æ¬¡è¯»å–çš„è®°å½•æ•°
            params(tuple|dict|None): SQL æŸ¥è¯¢å‚æ•°
            aggregate (bool): æ˜¯å¦æŠŠæ‰€æœ‰ DataFrame åˆå¹¶æˆä¸€ä¸ªå¤§ DataFrame
        Returns:
            list | pd.DataFrame: è¿”å›æ‰€æœ‰å¤„ç†ç»“æœçš„åˆ—è¡¨
        """
        import pandas as pd

        offset = 0
        results = []
        chunk_query = f"{sql} LIMIT %s OFFSET %s"

        with conn.cursor() as cur:
            while True:
                if isinstance(params, tuple):
                    query_params = (*params, chunk_size, offset)
                elif isinstance(params, dict):
                    query_params = {**params, "limit": chunk_size, "offset": offset}
                else:
                    query_params = (chunk_size, offset)

                cur.execute(chunk_query, query_params)
                rows = cur.fetchall()
                if not rows:
                    break

                df = pd.DataFrame(rows)
                if not df.empty:
                    results.append(process_chunk(df) if process_chunk else df)

                offset += chunk_size

        if aggregate and not process_chunk:
            return pd.concat(results, ignore_index=True) if results else pd.DataFrame()

        return results


class SyncMysql(BaseMysql):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.conn: Optional[pymysql.connections.Connection] = None
        self.cur = None

    def __del__(self):
        if self.conn or self.cur:
            print("[OperationMysql] Warning:æ­£åœ¨è‡ªåŠ¨æ¸…ç†,å…³é—­æ•°æ®åº“è¿æ¥ã€‚")
            self.close()

    def __enter__(self):
        # æ‰“å¼€è¿æ¥
        self.connect()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # å…³é—­æ¸¸æ ‡å’Œè¿æ¥
        self.close()

    def connect(self):
        self.close()
        try:
            self.conn = pymysql.connect(
                host=self.host,
                port=self.port,
                user=self.user,
                password=self.password,
                db=self.db_name,
                charset=self.charset,
                cursorclass=pymysql.cursors.DictCursor  # è¿™ä¸ªå®šä¹‰ä½¿æ•°æ®åº“é‡ŒæŸ¥å‡ºæ¥çš„å€¼ä¸ºå­—å…¸ç±»å‹
            )
            self.cur = self.conn.cursor()  # åŸç”Ÿæ•°æ®åº“è¿æ¥æ–¹å¼
        except Exception as e:
            print(f"[Sync] è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
            self.conn = None
            self.cur = None

    def get_conn(self):
        if not self.conn:
            self.connect()
        return self.conn

    def close(self):
        if self.cur:
            self.cur.close()
            self.cur = None
        if self.conn:
            self.conn.close()
            self.conn = None

    def ensure_connection(self):
        try:
            if self.conn:
                self.conn.ping(reconnect=True)  # å·²è¿æ¥ä¸”å¥åº·
            else:
                self.connect()
        except Exception as e:
            print(f"[Sync] è‡ªåŠ¨é‡è¿å¤±è´¥: {e}")
            self.connect()

    def run(self, sql: str, params: tuple | dict | list = None):
        sql_type = (sql or "").strip().split()[0].lower()
        self.ensure_connection()
        try:
            if isinstance(params, list):
                self.cur.executemany(sql, params)  # æ‰¹é‡æ‰§è¡Œ
            else:
                self.cur.execute(sql, params or ())  # å•æ¡æ‰§è¡Œ

            if sql_type == "select":
                return self.cur.fetchall()

            elif sql_type in {"insert", "update", "delete", "replace"}:
                self.conn.commit()
                if sql_type == "insert":
                    return self.cur.lastrowid
                return True

        except Exception as e:
            if self.conn:
                self.conn.rollback()
            print(f"[Sync] æ•°æ®åº“æ‰§è¡Œå‡ºé”™: {e}")
        return None

    def search(self, sql: str, params: tuple | dict = None):
        if not sql.lower().startswith("select"):
            raise ValueError("search æ–¹æ³•åªèƒ½æ‰§è¡Œ SELECT è¯­å¥")
        if params is None:
            params = ()
        self.cur.execute(sql, params)
        result = self.cur.fetchall()
        return result

    def execute(self, sql: str, params: tuple | dict | list = None):
        # INSERT,UPDATE,DELETE
        try:
            if isinstance(params, list):
                self.cur.executemany(sql, params)  # æ‰¹é‡æ‰§è¡Œ
            else:
                self.cur.execute(sql, params or ())  # å•æ¡æ‰§è¡Œ
            self.conn.commit()
        except Exception as e:
            self.conn.rollback()
            print(f"æ‰§è¡Œ SQL å‡ºé”™: {e}")

    def insert(self, sql: str = None, params: tuple | dict = None, table_name: str = None):
        # å•æ¡ INSERT è¯­å¥ï¼Œä¸”ç›®æ ‡è¡¨æœ‰ AUTO_INCREMENT å­—æ®µ
        if isinstance(params, dict) and table_name:
            fields = tuple(params.keys())
            values = tuple(params.values())
            field_str = ', '.join(f"`{field}`" for field in fields)
            placeholder_str = ', '.join(['%s'] * len(fields))
            sql = f"INSERT INTO `{table_name}` ({field_str}) VALUES ({placeholder_str})"
            params = values
        try:
            self.cur.execute(sql, params or ())
            self.conn.commit()
            return self.cur.lastrowid or int(self.conn.insert_id())
        except Exception as e:
            if self.conn:
                self.conn.rollback()
            print(f"æ‰§è¡Œ SQL å‡ºé”™: {e}")
            print(f"SQL: {repr(sql)} \n å‚æ•°: {params}")
        # finally:
        #     self.cur.close()
        #     self.conn.close()
        return -1

    def query_batches(self, ids: list | tuple, index_key: str, table_name: str, fields: list = None, chunk_size=10000):
        """
        await asyncio.to_thread
        å¤§æ‰¹é‡ IN æŸ¥è¯¢ï¼Œåˆ†æ‰¹æ‰§è¡Œï¼Œé¿å… SQL å‚æ•°æº¢å‡ºã€‚
        Args:
            ids (list | tuple): è¦æŸ¥æ‰¾çš„ ID åˆ—è¡¨
            index_key (str): ä½œä¸ºç­›é€‰æ¡ä»¶çš„å­—æ®µ
            table_name (str): è¡¨å
            fields (list): è¿”å›çš„å­—æ®µ
            chunk_size (int): æ¯æ¬¡ IN çš„æœ€å¤§æ•°é‡<65535
        Returns:
            list[dict]: æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        if not ids:
            return []
        field_str = ', '.join(f"`{field}`" for field in fields) if fields else '*'

        result_rows = []
        for batch in chunks_iterable(ids, chunk_size):
            placeholders = ', '.join(['%s'] * len(batch))
            sql = f"SELECT {field_str} FROM `{table_name}` WHERE `{index_key}` IN ({placeholders})"
            self.cur.execute(sql, tuple(batch))
            result_rows.extend(self.cur.fetchall())
            # filtered_chunk = pd.read_sql_query(text(sql+'`{index_key}` in :ids'), conn, params={'ids': batch})
            # df_chunk.to_sql(table_name, con=engine,chunksize=chunk_size, if_exists='append', index=False)
        return result_rows

    def get_table_schema(self, table_name: str) -> list:
        """
        è·å–æŒ‡å®šè¡¨çš„ç»“æ„ä¿¡æ¯ï¼ˆåŸå§‹ implementationï¼‰ï¼Œç”¨äºè‡ªç„¶è¯­è¨€æè¿°
        å‚æ•°:
            table_name: è¡¨åï¼ˆä¸å«æ•°æ®åº“åï¼‰
        è¿”å›:
            è¡¨ç»“æ„åˆ—è¡¨ï¼Œæ¯åˆ—åŒ…å« column_name, data_type, is_nullable, column_type, column_comment
        """
        try:
            self.ensure_connection()
            sql, params = self.table_schema(table_name)
            return self.search(sql, params)
        except Exception as e:
            print(f"[Schema] è·å–è¡¨ç»“æ„å¤±è´¥: {str(e)}")
        return []


class AsyncMysql(BaseMysql):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.pool: Optional[aiomysql.Pool] = None
        self.conn = None  # used only when temporarily acquiring

    async def __aenter__(self):
        if self.pool is None:
            await self.init_pool()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close_pool()

    async def init_pool(self, minsize: int = 1, maxsize: int = 30, autocommit: bool = True):
        if self.pool is not None:
            return
        try:
            self.pool = await aiomysql.create_pool(
                host=self.host,
                port=self.port,
                user=self.user,
                password=self.password,
                db=self.db_name,
                charset=self.charset,
                autocommit=autocommit,
                minsize=minsize,
                maxsize=maxsize
            )
        except Exception as e:
            print(f"[Async] åˆ›å»ºè¿æ¥æ± å¤±è´¥: {e}")
            self.pool = None

    async def close_pool(self):
        if self.pool is not None:
            self.pool.close()
            await self.pool.wait_closed()
            self.pool = None

    async def get_conn(self):
        if self.pool is None:
            await self.init_pool()
        return await self.pool.acquire()

    def release(self, conn):
        if self.pool is not None:
            self.pool.release(conn)

    @asynccontextmanager
    async def get_cursor(self, conn=None) -> AsyncIterator[aiomysql.Cursor]:
        """è·å–æ¸¸æ ‡ï¼ˆæ”¯æŒè‡ªåŠ¨é‡Šæ”¾ï¼‰
        é‡Šæ”¾ await cursor.close()
        Args:
            conn: å¤–éƒ¨ä¼ å…¥çš„è¿æ¥å¯¹è±¡ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨åˆ›å»ºæ–°è¿æ¥

        Yields:
            aiomysql.Cursor: æ•°æ®åº“æ¸¸æ ‡

        æ³¨æ„ï¼š
            - å½“connä¸ºNoneæ—¶ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºå¹¶æœ€ç»ˆé‡Šæ”¾è¿æ¥
            - å½“connç”±å¤–éƒ¨ä¼ å…¥æ—¶ï¼Œä¸ä¼šè‡ªåŠ¨é‡Šæ”¾è¿æ¥
        """
        should_release = False
        if conn is None:
            conn = await self.get_conn()
            should_release = True

        try:
            async with conn.cursor(aiomysql.DictCursor) as cursor:
                yield cursor
        finally:
            if should_release:
                self.release(conn)

    async def async_run(self, sql: str, params: tuple | dict | list = None, conn=None):
        async def _run(c):
            sql_type = (sql or "").strip().split()[0].lower()
            async with c.cursor(aiomysql.DictCursor) as cur:
                if isinstance(params, list):
                    await cur.executemany(sql, params)
                else:
                    await cur.execute(sql, params or ())

                if sql_type == "select":
                    return await cur.fetchall()
                elif sql_type in {"insert", "update", "delete", "replace"}:
                    await c.commit()  # æ˜¾å¼ä¿é™©,autocommit=True
                    if sql_type == "insert":
                        return cur.lastrowid or int(c.insert_id())
                    else:
                        return True

                return None

        try:
            if conn:
                return await _run(conn)
            if self.pool is None:
                await self.init_pool()
            async with self.pool.acquire() as conn:
                return await _run(conn)

        except Exception as e:
            print(f"[Async] SQLæ‰§è¡Œé”™è¯¯: {e}, SQL={sql}\nVALUE={params}")
        return None

    async def async_execute(self, sql_list: list[tuple[str, tuple | dict | list | None]], conn=None):
        """
        æ‰¹é‡æ‰§è¡Œå¤šæ¡ SQL å¹¶è‡ªåŠ¨æäº¤æˆ–å›æ»šï¼ˆåŒä¸€ä¸ªäº‹åŠ¡ï¼‰
        :param sql_list: å½¢å¦‚ [(sql1, params1), (sql2, params2), ...]
        :param conn
        """
        if conn:
            should_release = False
        else:
            if self.pool is None:
                await self.init_pool()
            conn = await self.pool.acquire()
            should_release = True

        try:
            async with conn.cursor() as cur:
                for sql, params in sql_list:
                    if not sql.strip():
                        continue
                    if isinstance(params, list):
                        await cur.executemany(sql, params)
                    else:
                        await cur.execute(sql, params or ())
                await conn.commit()
                return True

        except Exception as e:
            print(f"[Async] æ‰¹é‡ SQL æ‰§è¡Œå¤±è´¥: {e}")
            try:
                await conn.rollback()
            except Exception as rollback_err:
                print(f"[Async] å›æ»šå¤±è´¥: {rollback_err}")
            return False

        finally:
            if should_release:
                self.release(conn)

    async def async_query(self, query_list: list[tuple[str, tuple | dict]], fetch_all: bool = True,
                          cursor=None) -> list:
        """
        æ‰§è¡Œå¤šä¸ªæŸ¥è¯¢ï¼Œåˆ†åˆ«è¿”å›ç»“æœåˆ—è¡¨
        :param query_list: [(sql1, params1), (sql2, params2), ...]
        :param fetch_all: True è¡¨ç¤º fetchallï¼ŒFalse è¡¨ç¤º fetchone
        :param cursor: å¯é€‰å¤–éƒ¨ cursor
        :return: [result1, result2, ...]
        """

        async def _run(c) -> list:
            results = []
            for sql, params in query_list:
                await c.execute(sql, params or ())
                result = await c.fetchall() if fetch_all else await c.fetchone()
                results.append(result)
            return results

        if cursor:
            return await _run(cursor)

        if self.pool is None:
            await self.init_pool()

        async with self.pool.acquire() as conn:
            async with conn.cursor(aiomysql.DictCursor) as cur:
                return await _run(cur)

    async def query_one(self, sql: str, params: tuple | dict = (), cursor=None) -> dict | None:
        results = await self.async_query([(sql, params)], fetch_all=False, cursor=cursor)
        return results[0] if results else None

    async def async_insert(self, table_name: str, params_data: dict, conn=None):
        """
        æ’å…¥æ•°æ®
        Args:
            table_name (str): è¡¨å
            params_data (dict): è¦æ’å…¥çš„æ•°æ®ï¼ˆæ›´æ–°å¿…é¡»åŒ…å«ä¸»é”®/å”¯ä¸€ç´¢å¼•å­—æ®µï¼‰
            conn:å¯é€‰å¤–éƒ¨ä¼ å…¥è¿æ¥
        """
        sql, values = self.build_insert(table_name, params_data, update_fields=[])
        return await self.async_run(sql, values, conn=conn)

    async def async_merge(self, table_name: str, params_list: list[dict], update_fields: list[str] = None, conn=None):
        """
        æ‰¹é‡æ’å…¥/æ›´æ–°æ•°æ®,è¿”å›trueï¼ˆæ ¹æ®ä¸»é”®æˆ–å”¯ä¸€é”®è‡ªåŠ¨åˆå¹¶ï¼‰
        update_fields (list): éœ€è¦æ›´æ–°çš„å­—æ®µåˆ—è¡¨ï¼Œé»˜è®¤ä¸ºé™¤äº†ä¸»é”®ä»¥å¤–çš„å­—æ®µ,åœ¨å‘ç”Ÿå†²çªæ—¶è¢«æ›´æ–°çš„å­—æ®µåˆ—è¡¨,[]ä¸ºæ’å…¥
        """
        if not params_list:
            raise ValueError("å‚æ•°åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        sql_list = [self.build_insert(table_name, row, update_fields=update_fields or []) for row in params_list]
        return await self.async_execute(sql_list, conn=conn)  # list[tuple[str, tuple]]

    async def async_update(self, table_name: str, params_data: dict, row_id: int, primary_key: str = "id", conn=None):
        """
        æ ¹æ®ä¸»é”®å­—æ®µæ›´æ–°æŒ‡å®šè¡Œæ•°æ®ã€‚

        Args:
            table_name (str): è¡¨å
            row_id: ä¸»é”®å€¼ï¼ˆé€šå¸¸æ˜¯ idï¼‰
            params_data (dict): è¦æ›´æ–°çš„å­—æ®µåŠæ–°å€¼
            primary_key (str): ä¸»é”®å­—æ®µåï¼Œé»˜è®¤æ˜¯ 'id'
            conn:å¯é€‰å¤–éƒ¨ä¼ å…¥è¿æ¥
        """
        if not params_data:
            raise ValueError("æ›´æ–°æ•°æ®ä¸èƒ½ä¸ºç©º")
        if not row_id:
            raise ValueError("row_id ä¸èƒ½ä¸ºç©º")

        update_fields = ', '.join(f"`{k}` = %s" for k in params_data.keys())  # æ„å»ºæ›´æ–°å­—æ®µåˆ—è¡¨
        values = tuple(params_data.values()) + (row_id,)
        sql = f"UPDATE `{table_name}` SET {update_fields} WHERE `{primary_key}` = %s"
        return await self.async_run(sql, values, conn=conn)

    async def get_offset(self, table_name: str, page: int = None, per_page: int = 10, cursor=None,
                         use_estimate: bool = True):
        total = 0
        if use_estimate:
            row = await self.query_one(
                "SELECT TABLE_ROWS AS estimate "
                "FROM information_schema.TABLES "
                "WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = %s",
                params=(table_name,), cursor=cursor)
            total = int(row["estimate"]) if row and row.get("estimate") is not None else 0
        if (not use_estimate) or total <= 0:
            count_res = await self.query_one(f"SELECT COUNT(*) as count FROM {table_name}", cursor=cursor)
            total = int(count_res["count"]) if count_res else 0

        total_pages = (total + per_page - 1) // per_page if total > 0 else 1
        if page is None:  # default to last page when page not provided
            page = total_pages
        if page < 1:
            page = 1
        if page > total_pages:
            page = total_pages
        offset = (page - 1) * per_page
        return offset, page, total_pages, total

    async def get_table_columns(self, table_name: str, cursor=None) -> list[dict]:
        async def _run(c) -> list:
            await c.execute(f"DESCRIBE {table_name}")
            columns = await c.fetchall()
            # è½¬æ¢åˆ—ä¿¡æ¯ä¸ºæ›´å‹å¥½çš„æ ¼å¼
            formatted_columns = [{
                "name": col["Field"],
                "type": col["Type"],
                "nullable": col["Null"] == "YES",
                "key": col["Key"],
                "default": col["Default"],
                "extra": col["Extra"]
            } for col in columns]
            return formatted_columns

        if cursor:
            return await _run(cursor)

        if self.pool is None:
            await self.init_pool()

        async with self.pool.acquire() as conn:
            async with conn.cursor(aiomysql.DictCursor) as cur:
                return await _run(cur)

    async def get_primary_key(self, table_name: str, cursor=None) -> Optional[str]:
        columns = await self.get_table_columns(table_name, cursor=cursor)
        for col in columns:
            if col["key"] == "PRI":  # Primary Key,"UNI","MUL"
                return col["name"]
        return None

    async def get_table_schema(self, table_name: str, conn=None) -> list:
        """å¼‚æ­¥ç‰ˆæœ¬è·å–è¡¨ç»“æ„ä¿¡æ¯"""
        sql, params = self.table_schema(table_name)
        return await self.async_run(sql, params, conn) or []


class OperationMysql(AsyncMysql, SyncMysql, BaseMysql):
    db_config = parse_database_uri(Config.SQLALCHEMY_DATABASE_URI)

    """
    dbop = OperationMysql(persistent=True,async_mode=True)
    await dbop.init_pool()
    result = await dbop.async_run("SELECT * FROM users WHERE id=%s", (...,))
    await dbop.close_pool()

    async with OperationMysql(async_mode=True) as dbop:
        result = await dbop.async_run("SELECT ...")
        
    with SyncMysql(host, user, password, db) as db:
        result = db.run("SELECT * FROM users WHERE id=%s", (...,))

    async with AsyncMysql(...) as dbop:
        await dbop.async_run(...)
    """

    def __init__(self, persistent: bool = False, async_mode: bool = False, db_config: dict = None):
        """
        persistent: æ˜¯å¦ç«‹å³è¿æ¥æ•°æ®åº“ä»¥ä¾¿é•¿æœŸæŒä¹…åŒ–ä½¿ç”¨ï¼ˆä¸éœ€è¦ç”¨ withï¼‰ï¼Œ...close()ï¼Œè¿æ¥ä¸çº¿ç¨‹å®‰å…¨ï¼Œä¸åŒçº¿ç¨‹åº”ç”¨ä¸åŒå®ä¾‹
        """

        self.persistent = persistent
        self.async_mode = async_mode
        self.config = db_config or type(self).db_config
        # super().__init__(**self.config)

        if async_mode:
            AsyncMysql.__init__(self, **self.config)
        else:
            SyncMysql.__init__(self, **self.config)

        if persistent and not async_mode:
            self.ensure_connection()
            print('[MysqlData] Sync connected.')

    async def __aenter__(self):
        if self.async_mode:
            return await AsyncMysql.__aenter__(self)
        raise RuntimeError("Use 'with' for sync mode")

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.async_mode and not self.persistent:
            await AsyncMysql.__aexit__(self, exc_type, exc_val, exc_tb)

    def __enter__(self):
        if not self.async_mode:
            return SyncMysql.__enter__(self)
        raise RuntimeError("Use 'async with' for async mode")

    def __exit__(self, exc_type, exc_val, exc_tb):
        if not self.async_mode:
            SyncMysql.__exit__(self, exc_type, exc_val, exc_tb)

    @classmethod
    async def get_async_conn(cls, **kwargs):
        async with cls(async_mode=True, **kwargs) as dbop:
            yield dbop


DB_Client = OperationMysql(persistent=True, async_mode=True)


class CollectorMysql(AsyncMysql):
    def __init__(self, *,
                 batch_size: int = 1000,
                 queue_maxsize: int = 10000,
                 max_wait_seconds: float = 1.0,
                 worker_count: int = 1,
                 retry_times: int = 2, retry_backoff: float = 1.0,
                 instance: AsyncMysql = None):
        '''
        collector = CollectorMysql(instance=DB_Client)
        await collector.start()
        await collector.enqueue(
        ok = collector.enqueue_nowait(
        wait collector.stop(flush=True)# åœæœºå‰æ¸…ç†
        '''
        if instance is not None:
            super().__init__(host=instance.host, user=instance.user, password=instance.password,
                             db_name=instance.db_name, port=instance.port, charset=instance.charset)
            self.pool = instance.pool
        else:
            config = parse_database_uri(Config.SQLALCHEMY_DATABASE_URI)
            super().__init__(**config)

        self.batch_size = batch_size
        self.batch_interval = max_wait_seconds
        self.worker_count = worker_count
        self.retry_times = retry_times
        self.retry_backoff = retry_backoff

        self._queue: asyncio.Queue = asyncio.Queue(maxsize=queue_maxsize)
        self._workers: list[asyncio.Task] = []
        self._stopped = asyncio.Event()
        self._started = False

    async def start(self):
        if self._started:
            return
        self._stopped.clear()
        for _ in range(self.worker_count):
            self._workers.append(asyncio.create_task(self._worker_loop()))
        self._started = True

    async def stop(self, flush: bool = True, timeout: float = 10.0):
        """
        ä¼˜é›…åœæœºï¼šå¯é€‰å…ˆ flush æ‰€æœ‰é˜Ÿåˆ—å†é€€å‡º
        """
        if not self._started:
            return

        if flush:
            if timeout > 0:  # è½®è¯¢ç­‰å¾…é˜Ÿåˆ—ä¸ºç©º
                start = time.time()
                while not self._queue.empty() and (time.time() - start) < timeout:
                    await asyncio.sleep(0.05)

        self._stopped.set()
        # cancel workers if they block on queue.get
        for w in self._workers:
            w.cancel()
        await asyncio.gather(*self._workers, return_exceptions=True)
        self._workers.clear()
        self._started = False

        if flush and self._queue.qsize() > 0:  # flush é˜Ÿåˆ—å‰©ä½™æ•°æ®
            try:
                await asyncio.wait_for(self.flush_all(), timeout=timeout)
            except asyncio.TimeoutError:
                print("[Collector] flush timeout, some data may not be written")

    @asynccontextmanager
    async def context(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºå®‰å…¨åœ°åˆå§‹åŒ–å’Œå…³é—­å¤„ç†å™¨"""
        await self.start()
        try:
            yield self
        finally:
            await self.stop()

    async def enqueue(self, table_name: str, params_data: dict, update_fields: list[str] = None):
        """
        å°†ä¸€æ¡è®°å½•åŠ å…¥é˜Ÿåˆ—ï¼ˆé»˜è®¤é˜»å¡ç›´åˆ°æ”¾å…¥ï¼Œäº§ç”Ÿå›å‹ï¼‰ï¼Œç›´åˆ°é˜Ÿåˆ—æœ‰ç©ºä½å¯ä»¥æ”¾å…¥
        """
        sql, values = self.build_insert(table_name, params_data, update_fields=update_fields or [])
        await self._queue.put((sql, values))

    def enqueue_nowait(self, table_name: str, params_data: dict, update_fields: list[str] = None) -> bool:
        """
        å°è¯•éé˜»å¡å…¥é˜Ÿï¼Œå¤±è´¥è¿”å› Falseï¼ˆå¯ç”¨äºé‡‡æ ·/èˆå¼ƒç­–ç•¥ï¼‰ï¼Œé˜Ÿåˆ—æ»¡ä¸ç­‰å¾…ï¼Œç›´æ¥æŠ›å‡º QueueFull
        """
        try:
            sql, values = BaseMysql.build_insert(table_name, params_data, update_fields=update_fields or [])
            self._queue.put_nowait((sql, values))
            return True
        except asyncio.QueueFull:
            return False  # é€‰æ‹©ï¼šä¸¢å¼ƒã€é™é‡‡æ ·ã€å†™å…¥å¤‡ä»½æ–‡ä»¶ã€æˆ–å†™åˆ° Redis ç­‰æŒä¹…é˜Ÿåˆ—

    async def _worker_loop(self):
        """
        æ¶ˆè´¹è€…ä¸»å¾ªç¯ï¼šæŒ‰ batch_size æˆ– max_wait_seconds åˆ·ç›˜
        """
        while not self._stopped.is_set():
            try:
                # ç”¨ batch_interval ç­‰å¾…ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œé¿å… busy loop
                item = await asyncio.wait_for(self._queue.get(), timeout=self.batch_interval)
                # if item is None:
                #     break # propagate sentinel to stop
            except asyncio.TimeoutError:
                continue  # æ²¡æœ‰æ–°æ•°æ®ï¼Œæ£€æŸ¥ _stopped åç»§ç»­
            batch = [item]
            deadline = time.time() + self.batch_interval

            while len(batch) < self.batch_size:  # å°è¯•åœ¨å‰©ä½™æ—¶é—´å†…å‡‘æ»¡ batch_size
                timeout = deadline - time.time()
                if timeout <= 0:
                    break
                try:
                    item = self._queue.get_nowait()
                    batch.append(item)
                except asyncio.QueueEmpty:
                    try:  # ç­‰å¾…ç›´åˆ°å‰©ä½™æ—¶é—´ç»“æŸæˆ–æ–°æ•°æ®åˆ°æ¥
                        item = await asyncio.wait_for(self._queue.get(), timeout=timeout)
                        batch.append(item)
                    except asyncio.TimeoutError:
                        break

            if not batch:
                continue

            conn = await self.get_conn()
            try:
                await self._flush_batch(batch, conn)  # å¤„ç† batchï¼ˆå¼‚æ­¥æ‰§è¡Œ DB å†™ï¼‰
            except Exception as e:
                # æ­¤å¤„åº”è¯¥è®°å½•æ—¥å¿—/å‘Šè­¦ï¼›ä¸ºé˜²æ­¢æ•°æ®ä¸¢å¤±ï¼Œå¯è€ƒè™‘æŠŠå¤±è´¥çš„ batch æ”¾åˆ°åç«¯æŒä¹…åŒ–æˆ–é‡è¯•é˜Ÿåˆ—
                logger.error(f"[Collector] flush batch failed: {e}")
                # è¿™é‡Œä¸ raiseï¼Œè®© loop ç»§ç»­æ¶ˆè´¹åç»­ batch
            finally:
                for _ in batch:  # æ ‡è®°ä»»åŠ¡å®Œæˆï¼Œç¡®ä¿é‡Šæ”¾è¿æ¥
                    self._queue.task_done()
                self.release(conn)

    async def _flush_batch(self, batch: list, conn=None):
        # åˆ†ç»„ï¼šå°†å­—æ®µä¸€è‡´çš„è¡Œåˆå¹¶åˆ°ä¸€èµ·ä»¥ä¾¿ç”¨ executemany
        groups: dict[str, list] = {}
        for sql, params in batch:
            groups.setdefault(sql, []).append(params)

        # prepare sql_list for async_execute: if a group has same fields -> executemany
        sql_list = [(sql, params_list) for sql, params_list in groups.items()]

        # æ‰§è¡Œå¹¶å¸¦é‡è¯•é€»è¾‘
        process_execute = async_error_logger(max_retries=self.retry_times, delay=1, backoff=self.retry_backoff,
                                             extra_msg="InsertCollector flush batch")(self.async_execute)
        ok = await process_execute(sql_list, conn=conn)
        if not ok:
            raise RuntimeError("flush batch failed after retries")

    async def flush_all(self):
        items = []
        while not self._queue.empty():
            items.append(await self._queue.get())
        if not items:
            return
        async with self.pool.acquire() as conn:
            await self._flush_batch(items, conn)


class AsyncBatchAdd:
    """
    é€šç”¨çš„å¼‚æ­¥æ‰¹é‡å¤„ç†å™¨ï¼Œå¯ç”¨äºä»»ä½• SQLAlchemy ORM ç±»
    """

    def __init__(
            self,
            model_class: Type,
            batch_size: int = 100,
            batch_timeout: float = 5.0,
            queue_maxsize: int = 10000,
            get_session_func: Optional[Callable] = None
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨

        Args:
            model_class: SQLAlchemy ORM ç±»
            batch_size: æ¯æ‰¹å¤„ç†çš„è®°å½•æ•°é‡
            batch_timeout: æ‰¹å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            get_session_func: è·å–æ•°æ®åº“ä¼šè¯çš„å‡½æ•°,AsyncSessionLocal
        """
        self.model_class = model_class
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.get_session_func = get_session_func  # coro

        self._queue = asyncio.Queue(maxsize=queue_maxsize)
        self._task = None
        self._is_running = False

    async def initialize(self):
        """åˆå§‹åŒ–å¤„ç†å™¨ï¼Œå¯åŠ¨åå°ä»»åŠ¡"""
        if not self._is_running:
            self._is_running = True
            self._task = asyncio.create_task(self._worker())
            logger.info(f"Initialized batch processor for {self.model_class.__name__}")

    async def shutdown(self):
        """å…³é—­å¤„ç†å™¨ï¼Œåœæ­¢åå°ä»»åŠ¡å¹¶å¤„ç†å‰©ä½™æ•°æ®"""
        if self._is_running:
            self._is_running = False
            if self._task:
                self._task.cancel()
                try:
                    await self._task
                except asyncio.CancelledError:
                    pass
                self._task = None
            logger.info(f"Shutdown batch processor for {self.model_class.__name__}")

    async def put_nowait(self, data: Dict[str, Any]):
        """å°†æ•°æ®æ”¾å…¥é˜Ÿåˆ—ï¼ˆéé˜»å¡ï¼‰"""
        if not self._is_running:
            await self.initialize()
        await self._queue.put(data)

    def put_many_nowait(self, data_list: List[Dict]) -> int:
        """å°†å¤šæ¡æ•°æ®æ”¾å…¥é˜Ÿåˆ—ï¼ˆéé˜»å¡ï¼‰"""
        count = 0
        for data in data_list:
            try:
                self._queue.put_nowait(data)
                count += 1
            except asyncio.QueueFull:
                break
        return count

    async def _worker(self):
        """åå°æ‰¹é‡æ’å…¥å·¥ä½œå™¨"""
        batch = []
        last_insert_time = time.time()

        while self._is_running:
            try:
                # ç­‰å¾…æ–°æ¶ˆæ¯æˆ–è¶…æ—¶
                try:
                    item = await asyncio.wait_for(self._queue.get(), timeout=self.batch_timeout)
                    batch.append(item)
                    self._queue.task_done()
                except asyncio.TimeoutError:
                    pass  # è¶…æ—¶ï¼Œç»§ç»­å¤„ç†å½“å‰æ‰¹æ¬¡

                current_time = time.time()
                # å¦‚æœæ‰¹æ¬¡è¾¾åˆ°æŒ‡å®šå¤§å°æˆ–è¶…æ—¶ï¼Œæ‰§è¡Œæ’å…¥
                if (len(batch) >= self.batch_size or
                        (batch and current_time - last_insert_time >= self.batch_timeout)):
                    await self.process_batch(batch)
                    batch.clear()
                    last_insert_time = current_time

            except asyncio.CancelledError:
                if batch:  # ä»»åŠ¡è¢«å–æ¶ˆï¼Œå¤„ç†å‰©ä½™æ‰¹æ¬¡
                    await self.process_batch(batch)
                break
            except Exception as e:
                logger.error(f"Unexpected error in batch insert worker: {e}")

    async def process_batch(self, batch: List[Dict[str, Any]]):
        """å¤„ç†ä¸€æ‰¹æ•°æ®ï¼Œæ’å…¥åˆ°æ•°æ®åº“"""
        if not batch:
            return

        if not self.get_session_func:
            logger.error("No session function provided for batch processor")
            return

        try:
            async with self.get_session_func() as session:
                session.add_all([self.model_class(**data) for data in batch])
                await session.commit()
                logger.info(f"Successfully inserted {len(batch)} records for {self.model_class.__name__}")
        except Exception as e:
            logger.error(f"Error inserting batch for {self.model_class.__name__}: {e}")
            # await session.rollback()
            # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ é‡è¯•é€»è¾‘æˆ–é”™è¯¯å¤„ç†

    async def process_one(self, data):
        """ç›´æ¥æ’å…¥ï¼ˆä¸ä½¿ç”¨é˜Ÿåˆ—ï¼‰"""
        async with self.get_session_func() as session:
            try:
                session.add(self.model_class(**data))
                await session.commit()
                return True
            except Exception as e:
                await session.rollback()
                logger.error(f"Error insert for {self.model_class.__name__}: {e}")
        return False

    async def execute_batch(self, stmt, values_list: List[tuple | dict]):
        """
           æ‰¹é‡æ’å…¥æˆ–æ›´æ–°å†å²è®°å½•ï¼ˆæ”¯æŒ executemanyï¼‰ï¼Œä½¿ç”¨ MySQL çš„ ON DUPLICATE KEY UPDATEã€‚
        """
        total = 0
        try:
            async with self.get_session_func() as session:
                for chunk in chunks_iterable(values_list, self.batch_size):
                    await session.execute(stmt, chunk)
                    total += len(chunk)
                await session.commit()
            logger.info(f"Inserted/Updated {total} records")
            return total

        except Exception as e:
            logger.error(f"Error during history upsert: {e}\n{stmt}")
        return total

    @asynccontextmanager
    async def context(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºå®‰å…¨åœ°åˆå§‹åŒ–å’Œå…³é—­å¤„ç†å™¨"""
        await self.initialize()
        try:
            yield self
        finally:
            await self.shutdown()


async def aiohttp_request(method: str, url: str, session: aiohttp.ClientSession = None, json=None, data=None,
                          params=None, headers=None, timeout=30, **kwargs):
    headers = headers or {}

    # æ‹¼æ¥ GET query
    if method.upper() == "GET" and params:
        from urllib.parse import urlencode
        query_string = urlencode(params)
        url = f"{url}?{query_string}"

    async def fetch_url(session: aiohttp.ClientSession):
        async with session.request(method, url, json=json, data=data, headers=headers, timeout=timeout,
                                   **kwargs) as resp:
            try:
                resp.raise_for_status()
                return await resp.json()
            except aiohttp.ContentTypeError:
                return {"status": resp.status, "error": "Non-JSON response", "body": await resp.text()}

    try:
        if session:
            return await fetch_url(session)

        async with aiohttp.ClientSession(headers=headers, timeout=aiohttp.ClientTimeout(total=timeout)) as ts:
            return await fetch_url(ts)

    except aiohttp.ClientResponseError as e:
        logging.error(f"[HTTP Error] {method} {url} | Status: {e.status}, Message: {e.message}, Body: {json or data}")
        return {"status": e.status, "error": e.message, "url": url}
    except (aiohttp.ClientConnectorError, asyncio.TimeoutError) as e:
        logging.error(f"[Connection Error] {method} {url} | Message: {e}")
        return {"status": 503, "error": f"Connection failed: {str(e)}", "url": url}
    except Exception as e:
        logging.error(f"[Unknown Error] {method} {url} | Message: {e}")
        return {"status": 500, "error": str(e), "url": url}


async def post_aiohttp_stream(url, payload: dict = None, time_out=60, headers: dict = None, **kwargs):
    timeout = aiohttp.ClientTimeout(total=time_out, sock_read=max(60, time_out), sock_connect=5.0)
    async with aiohttp.ClientSession(timeout=timeout, headers=headers or {}) as session:
        async with session.post(url, json=payload, **kwargs) as response:
            response.raise_for_status()
            buffer = bytearray()  # b""
            async for chunk in response.content.iter_any():  # .iter_chunked(1024)
                if not chunk:
                    continue
                buffer.extend(chunk.tobytes() if isinstance(chunk, memoryview) else chunk)
                while b"\n" in buffer:  # å¤„ç†ç¼“å†²åŒºä¸­çš„æ‰€æœ‰å®Œæ•´è¡Œ
                    line, buffer = buffer.split(b"\n", 1)
                    if line.strip():
                        try:
                            yield json.loads(line.decode("utf-8").strip())
                        except (UnicodeDecodeError, json.JSONDecodeError):
                            continue

            if buffer:
                tail_bytes = bytes(buffer).rstrip(b"\r\n")
                if tail_bytes and tail_bytes.strip():
                    try:
                        yield json.loads(tail_bytes.decode("utf-8").strip())  # if decoded_line.startswith(('{', '[')):
                    except (UnicodeDecodeError, json.JSONDecodeError):
                        pass


async def post_httpx_sse(url, payload: dict = None, headers: dict = None, time_out=60,
                         client: httpx.AsyncClient = None, **kwargs):
    def parse_line(data):
        try:
            return json.loads(data)
        except json.JSONDecodeError:
            return None

    async def _run(cx: httpx.AsyncClient):
        try:
            async with cx.stream('POST', url, json=payload, headers=headers, **kwargs) as response:
                response.raise_for_status()
                buffer = ""
                async for line in response.aiter_lines():  # ä½¿ç”¨ aiter_bytes() å¤„ç†åŸå§‹å­—èŠ‚æµ
                    line = line.strip()
                    if not line:  # ç©ºè¡Œ -> äº‹ä»¶ç»“æŸ,å¼€å¤´çš„è¡Œ not line
                        if buffer:
                            if buffer == "[DONE]":
                                yield {"type": "done"}
                                return
                            parse = parse_line(buffer)  # ä¸€ä¸ªæ•°æ®å—çš„ç»“æŸ
                            if parse:
                                yield {"type": "data", "data": parse}
                            else:
                                yield {"type": "text", "data": buffer}
                            buffer = ""  # é‡ç½®æ¸…ç©º
                        continue

                    if line.startswith("data: "):
                        content = line[6:]  # line.lstrip("data: ")
                        if content in ("[DONE]", '"[DONE]"', "DONE"):
                            yield {"type": "done"}
                            return
                        parse = parse_line(content)  # å•è¡Œ JSON
                        if parse:
                            yield {"type": "data", "data": parse}
                    else:  # å¤„ç†é data: è¡Œæˆ– JSON è§£æå¤±è´¥æ—¶
                        buffer += ("\n" + line) if buffer else line

                if buffer:  # å¤„ç†æœ€åé—ç•™çš„ buffer
                    parse = parse_line(buffer)
                    if parse:
                        yield {"type": "data", "data": parse}
                    else:
                        yield {"type": "text", "data": buffer}

        except Exception as e:
            # yield error event and return
            yield {"type": "error", "data": f"HTTP error: {e}"}

    if client:
        async for item in _run(client):
            yield item
        return

    timeout = httpx.Timeout(time_out, read=60.0, connect=5.0)
    async with httpx.AsyncClient(timeout=timeout) as client:
        async for item in _run(client):
            yield item


async def call_http_request(url: str, headers=None, timeout=100.0, httpx_client: httpx.AsyncClient = None, **kwargs):
    """
    å¼‚æ­¥è°ƒç”¨HTTPè¯·æ±‚å¹¶è¿”å›JSONå“åº”ï¼Œå¦‚æœå“åº”ä¸æ˜¯JSONæ ¼å¼åˆ™è¿”å›æ–‡æœ¬å†…å®¹
    å›½å†…ï¼Œæ— ä»£ç†ï¼Œå¯èƒ½å†…å®¹æ— è§£æ
    :param url: è¯·æ±‚åœ°å€
    :param headers: è¯·æ±‚å¤´
    :param timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
    :param httpx_client: å¤–éƒ¨ä¼ å…¥çš„ AsyncClient å®ä¾‹ï¼ˆå¯å¤ç”¨è¿æ¥ï¼‰
    :param kwargs: å…¶ä»–ä¼ é€’ç»™ httpx çš„å‚æ•°
    :return: dict æˆ– None
    """

    async def fetch_url(cx: httpx.AsyncClient):
        response = await cx.get(url, headers=headers, timeout=timeout, **kwargs)  # è¯·æ±‚çº§åˆ«çš„è¶…æ—¶ä¼˜å…ˆçº§æ›´é«˜
        response.raise_for_status()
        content_type = response.headers.get("content-type", "")
        if "application/json" in content_type or not content_type:
            try:
                return response.json()
            except json.JSONDecodeError:
                pass

        return {'text': response.text}

    if httpx_client:
        return await fetch_url(httpx_client)

    async with httpx.AsyncClient() as cx:
        return await fetch_url(cx)


async def download_by_aiohttp(url: str, session: aiohttp.ClientSession, save_path, chunk_size=4096, in_decode=False):
    async with session.get(url) as response:
        # response = await asyncio.wait_for(session.get(url), timeout=timeout)
        if response.status == 200:
            if save_path:
                save_path.parent.mkdir(parents=True, exist_ok=True)
                async with aiofiles.open(save_path, mode='wb') as f:
                    # response.content å¼‚æ­¥è¿­ä»£å™¨ï¼ˆæµå¼è¯»å–ï¼‰,iter_chunked éé˜»å¡è°ƒç”¨ï¼Œé€å—è¯»å–
                    async for chunk in response.content.iter_chunked(chunk_size):
                        if isinstance(chunk, (bytes, bytearray)):
                            await f.write(chunk)
                        elif isinstance(chunk, str):
                            await f.write(chunk.encode('utf-8'))  # å°†å­—ç¬¦ä¸²è½¬ä¸ºå­—èŠ‚
                        else:
                            raise TypeError(
                                f"Unexpected chunk type: {type(chunk)}. Expected bytes or bytearray.")

                return save_path

            content = await response.read()  # å•æ¬¡å¼‚æ­¥è¯»å–å®Œæ•´å†…å®¹ï¼Œå°æ–‡ä»¶,await response.content.read(chunk_size)
            return content.decode('utf-8') if in_decode else content  # å°†å­—èŠ‚è½¬ä¸ºå­—ç¬¦ä¸²,è§£ç åçš„å­—ç¬¦ä¸² await response.text()

        print(f"Failed to download {url}: {response.status}")

    return None


async def download_by_httpx(url: str, client: httpx.AsyncClient, save_path, chunk_size=4096, in_decode=False):
    async with client.stream("GET", url) as response:  # é•¿æ—¶é—´çš„æµå¼è¯·æ±‚
        # response = await client.get(url, stream=True)
        response.raise_for_status()  # å¦‚æœå“åº”ä¸æ˜¯2xx  response.status_code == 200:
        if save_path:
            save_path.parent.mkdir(parents=True, exist_ok=True)
            async with aiofiles.open(save_path, mode="wb") as f:
                # response.aiter_bytes() å¼‚æ­¥è¿­ä»£å™¨
                async for chunk in response.aiter_bytes(chunk_size=chunk_size):
                    await f.write(chunk)

            return save_path

        content = bytearray()  # ä½¿ç”¨ `bytearray` æ›´é«˜æ•ˆåœ°æ‹¼æ¥äºŒè¿›åˆ¶å†…å®¹  b""  # raw bytes
        async for chunk in response.aiter_bytes(chunk_size=chunk_size):
            content.extend(chunk)
            # content += chunk
            # yield chunk

        return content.decode(response.encoding or 'utf-8') if in_decode else bytes(content)
        # return response.text if in_decode else response.content


async def upload_by_httpx(url: str, client: httpx.AsyncClient = None, files_path=('example.txt', b'Hello World')):
    '''
    with open("local.txt", "rb") as f:
        await upload_by_httpx("http://127.0.0.1:8000/upload", files_path=("local.txt", f))
    '''
    files = {'file': files_path}  # (filename, bytes/æ–‡ä»¶å¯¹è±¡)

    if client:
        resp = await client.post(url, files=files)
    else:
        async with httpx.AsyncClient() as client:
            resp = await client.post(url, files=files)

    resp.raise_for_status()
    return resp.json()


def download_by_requests(url: str, save_path, chunk_size=4096, in_decode=False, timeout=30):
    """
    åŒæ­¥ä¸‹è½½çš„æµå¼æ–¹æ³•
    å¦‚æœç›®æ ‡æ˜¯ä¿å­˜åˆ°æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨ contentï¼ˆæ— éœ€è§£ç ï¼‰ã€‚ï¼ˆå¦‚å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ã€PDF ç­‰ï¼‰
    å¦‚æœç›®æ ‡æ˜¯å¤„ç†å’Œè§£ææ–‡æœ¬æ•°æ®ï¼Œä¸”ç¡®å®šç¼–ç æ­£ç¡®ï¼Œä½¿ç”¨ textã€‚ï¼ˆå¦‚ HTMLã€JSONï¼‰
    """
    with requests.get(url, stream=True, timeout=timeout) as response:
        response.raise_for_status()  # ç¡®ä¿è¯·æ±‚æˆåŠŸ
        if save_path:
            save_path.parent.mkdir(parents=True, exist_ok=True)
            with open(save_path, "wb") as f:
                # requests iter_content åŒæ­¥ä½¿ç”¨,é˜»å¡è°ƒç”¨
                for chunk in response.iter_content(chunk_size=chunk_size):
                    if chunk:  # <class 'bytes'>
                        f.write(chunk)

            return save_path

        return response.text if in_decode else response.content  # ç›´æ¥è·å–æ–‡æœ¬,åŒæ­¥ç›´æ¥è¿”å›å…¨éƒ¨å†…å®¹


def upload_by_requests(url: str, file_path, file_key='snapshot'):
    with open(file_path, "rb") as f:
        files = {file_key: f}
        response = requests.post(url, files=files)
    return response.json()


async def send_to_wechat(user_name: str, context: str = None, link: str = None, object_name: str = None):
    url = f"{Config.WECHAT_URL}/sendToChat"
    headers = {'accept': 'application/json', 'Content-Type': 'application/json'}
    body = {'user': user_name, 'context': context, 'url': link,
            'object_name': object_name, 'file_type': get_file_type_wx(object_name)}

    try:
        cx = get_httpx_client(time_out=Config.HTTP_TIMEOUT_SEC)
        response = await cx.post(url, json=body, headers=headers)
        response.raise_for_status()
        return response.json()

    except Exception as e:
        logger.error(f'send_to_wechat{body}')
        logger.error(f"Error occurred while sending message: {e}")
        # with httpx.Client(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        #     response = cx.post(url, json=body, headers=headers)
        #     response.raise_for_status()
        # return response.json()

    return None


class OperationHttp:
    def __init__(self, use_sync=False, time_out: int | float = 100.0, proxy: str = None):
        self.client = None  # httpx.AsyncClient | aiohttp.ClientSession | requests.Session
        self._is_httpx: bool = False
        self._is_sync: bool = use_sync
        self._timeout: int = time_out
        self._proxy: str = proxy  # æ”¯æŒ None / "http://host:port" / "socks5://host:port"
        # self._mode: str = mode.lower()

        try:
            import httpx
            self._is_httpx = True
        except ImportError:
            try:
                import aiohttp
                self._is_httpx = False
            except ImportError:
                import requests
                self._is_sync = True

    async def __aenter__(self):
        self.init_client()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close_async()

    def __enter__(self):
        self.init_client()
        return self

    def __exit__(self, *args):
        self.close_sync()

    def init_client(self):
        if self.client is not None:
            return

        if self._is_sync:
            if self._is_httpx:
                transport = httpx.HTTPTransport(proxy=self._proxy or None)
                self.client = httpx.Client(timeout=self._timeout, transport=transport)  # åº•å±‚ä¸º httpcore
            else:
                self.client = requests.Session()  # åŸºäº urllib3
                if self._proxy:
                    self.client.proxies.update({"http": self._proxy, "https": self._proxy})
        else:
            if self._is_httpx:
                transport = httpx.AsyncHTTPTransport(proxy=self._proxy or None)
                limits = httpx.Limits(max_connections=100, max_keepalive_connections=20)
                timeout = httpx.Timeout(self._timeout, read=self._timeout, write=30.0, connect=5.0)
                self.client = httpx.AsyncClient(limits=limits, timeout=timeout, transport=transport)
            else:
                connector = aiohttp.TCPConnector(limit=100, limit_per_host=20)
                timeout = aiohttp.ClientTimeout(total=self._timeout, sock_read=self._timeout, sock_connect=5.0)
                self.client = aiohttp.ClientSession(connector=connector, timeout=timeout)  # headers=headers or {}

            # self.semaphore = asyncio.Semaphore(30)

    async def close_client(self):
        if self._is_sync:
            self.close_sync()
        else:
            await self.close_async()

    def close_sync(self):
        if self.client:
            self.client.close()
            self.client = None

    async def close_async(self):
        if self.client:
            if self._is_httpx:
                if not self.client.is_closed:
                    await self.client.aclose()
            else:
                if not self.client.closed:
                    await self.client.close()
            self.client = None

    async def get(self, url, headers=None, **kwargs):
        if self._is_sync:
            if self._is_httpx:
                resp = self.client.get(url, headers=headers, **kwargs)  # params=data
            else:
                resp = self.client.get(url, headers=headers, timeout=(self._timeout, self._timeout), **kwargs)
            resp.raise_for_status()
            return resp.json()
        else:
            if self._is_httpx:
                resp = await self.client.get(url, headers=headers, **kwargs)
                resp.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
                return resp.json()
            else:
                async with self.client.get(url, headers=headers or {}, **kwargs) as resp:
                    resp.raise_for_status()
                    return await resp.json()  # await resp.text()

    async def post(self, url, json=None, headers=None, **kwargs):
        if self._is_sync:
            if self._is_httpx:
                resp = self.client.post(url, json=json, headers=headers, **kwargs)
            else:
                resp = self.client.post(url, json=json, headers=headers, timeout=(self._timeout, self._timeout),
                                        **kwargs)
            resp.raise_for_status()
            return resp.json()
        else:
            if self._is_httpx:
                resp = await self.client.post(url, json=json, headers=headers, **kwargs)
                resp.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
                return resp.json()
            else:
                async with self.client.post(url, json=json, headers=headers or {}, **kwargs) as resp:
                    resp.raise_for_status()  # æŠ›å‡º 4xx/5xx é”™è¯¯
                    return await resp.json()

    def fallback_post(self, url, json_payload, headers=None, stream=False):
        try:
            resp = requests.post(url, headers=headers, json=json_payload, timeout=(5, self._timeout), stream=stream,
                                 proxies={"http": self._proxy, "https": self._proxy} if self._proxy else None)
            if resp.status_code == 200:
                return resp.json()  # json.loads(resp.content)
            else:
                raise RuntimeError(f"[requests fallback] è¿”å›å¼‚å¸¸: {resp.status_code}, å†…å®¹: {resp.text}")
        except Exception as e:
            print(f"[requests fallback] è¯·æ±‚å¤±è´¥: {e}")
            return None


@async_error_logger(max_retries=1, delay=3, exceptions=(Exception, httpx.HTTPError))
async def follow_http_html(url, time_out: float = 100.0, **kwargs):
    async with httpx.AsyncClient(timeout=time_out, follow_redirects=True) as cx:
        response = await cx.get(url, **kwargs)
        response.raise_for_status()
        return response.text


async def proxy_http_html(base_url: str, full_path: str, request: Request):
    url = f"{base_url}/{full_path}"
    async with httpx.AsyncClient(follow_redirects=True) as cx:
        method = request.method
        headers = dict(request.headers)
        body = await request.body()

        proxy_req = cx.build_request(method, url, headers=headers, content=body)
        resp = await cx.send(proxy_req)

        return Response(content=resp.content, status_code=resp.status_code, headers=dict(resp.headers))


@async_error_logger(1)
async def get_data_for_model(model: dict):
    """è·å–æ¯ä¸ªæ¨¡å‹çš„æ•°æ®"""
    model_name = model.get('name')
    client = AI_Client.get(model_name)

    if client:
        try:
            models = await client.models.list()
            return [m.model_dump() for m in models.data]
        except Exception as e:
            print(f"OpenAI error occurred:{e},name:{model_name}")
    else:
        url = model.get('model_url') or model['base_url'] + '/models'
        headers = {}
        api_key = model.get('api_key')
        if api_key:
            headers["Authorization"] = f'Bearer {api_key}'
            if model['type'] == 'anthropic':
                headers = {"x-api-key": api_key, "anthropic-version": "2023-06-01"}

        cx = get_httpx_client(proxy=Config.HTTP_Proxy if model.get('proxy') else None)
        models = await call_http_request(url, headers, httpx_client=cx)
        if models:
            return models.get('data')

    return None


class ModelList:
    models = []
    _redis = None
    _worker_id: str = None
    _list_key = "model_list"
    _data_key = "model_data_list"
    _hash_key = "model_data_hash"

    @classmethod
    def extract(cls, ai_models: list):
        """
        æå– AI_Models ä¸­çš„ name ä»¥åŠ search_field ä¸­çš„æ‰€æœ‰å€¼ï¼Œå¹¶å­˜å…¥ä¸€ä¸ªå¤§åˆ—è¡¨ã€‚

        è¿”å›ï¼š
        - List[str]: åŒ…å«æ‰€æœ‰æ¨¡å‹åç§°åŠå…¶å­æ¨¡å‹çš„åˆ—è¡¨
        """
        # list(itertools.chain(*[sublist[1] for sublist in extract_ai_model("model")]))
        extracted_data = extract_ai_model("model", ai_models)
        return [i for item in extracted_data for i in [item[0]] + item[1]]  # flattened_list

    @classmethod
    async def set(cls, redis=None, worker_id: str = None, ai_models: list = AI_Models):
        """æ›´æ–° MODEL_LIST,å¹¶ä¿å­˜åˆ° Redis"""
        cls.models = cls.extract(ai_models)
        if cls._redis is None:
            cls._redis = redis or get_redis()
        if worker_id:
            cls._worker_id = worker_id

        await cls.to_redis(cls._list_key, cls.models)

        await cls.set_datas(ai_models)

    @classmethod
    async def to_redis(cls, key: str, value, **kwargs):
        """
        åˆ†å¸ƒå¼å†™å…¥ï¼šè°å…ˆæŠ¢åˆ°é”è°å†™å…¥ï¼Œå¤±è´¥è‡ªåŠ¨é‡Šæ”¾é”ï¼ŒæˆåŠŸä¸é‡Šæ”¾
        kwargs: ä¼ é€’ç»™ redis.set çš„å…¶ä»–å‚æ•°ï¼Œå¦‚ ex/px/nx/xx ç­‰
        """
        if not cls._redis:
            return False

        lock_key = f"lock:{key}"
        async with with_distributed_lock(lock_key, cls._worker_id, 60000, redis=cls._redis, release=False) as acquired:
            if not acquired:  # é”å·²è¢«å ç”¨ï¼Œç›´æ¥è¿”å›å¤±è´¥
                return False

            try:
                await cls._redis.set(key, json.dumps(value, ensure_ascii=False), **kwargs)
                return True
            except Exception as e:
                logging.error(f"[Redis SET Error] key={key}, error={e}")
                if cls._worker_id:  # æœ‰æš—å·ä¸é‡Šæ”¾ï¼Œå†™å…¥å¤±è´¥æ‰é‡Šæ”¾é”
                    current_lock_value = await cls._redis.get(lock_key)
                    if current_lock_value and current_lock_value == cls._worker_id:
                        await cls._redis.delete(lock_key)

        # await run_with_lock(cls._redis.set, cls._list_key, json.dumps(cls.models, ensure_ascii=False),
        #                     lock_key=f"lock:{cls._list_key}", lock_timeout=60)
        return False

    @classmethod
    async def get(cls, updated=True):
        if cls._redis:
            data = await cls._redis.get(cls._list_key)
            if data:
                return json.loads(data)
        if not cls.models and updated:
            await cls.set()
            print("model_list updated:", cls.models)

        return cls.models

    @classmethod
    def contains(cls, value):
        models = cls.models or async_to_sync(cls.get)
        if ':' in value:
            owner, name = value.split(':')
            return owner in models or name in models

        return value in models

    @classmethod
    async def set_model_data(cls, model: dict, hash_data: dict = None) -> tuple:
        name = model.get('name')
        key = f"{cls._data_key}:{name}"
        hash_key = generate_hash_key(model.get('model', []))
        old_hash_key = (hash_data or {}).get(name)

        if old_hash_key == hash_key:  # å¦‚æœæ—§çš„ hash ç›¸åŒï¼Œåˆ™æ— éœ€æ›´æ–°
            data_raw = await cls._redis.get(key) if cls._redis else None
            if data_raw:
                model['data'] = json.loads(data_raw)
                return name, hash_key

        data = await get_data_for_model(model)  # å¦åˆ™é‡æ–°æ‹‰å–æ•°æ®å¹¶ç¼“å­˜
        if data:
            model['data'] = data
            await cls.to_redis(key, data)
            print('model:', model.get('name'), 'data:', data)
        return name, hash_key

    @classmethod
    async def set_datas(cls, ai_models=AI_Models):
        hash_data_raw = await cls._redis.get(cls._hash_key) if cls._redis else None
        hash_data = json.loads(hash_data_raw) if hash_data_raw else {}

        tasks = [cls.set_model_data(model, hash_data) for model in ai_models
                 if model.get('supported_list') and model.get('api_key')]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        for r in results:  # è¿‡æ»¤å‡ºæˆåŠŸçš„ç»“æœï¼Œå¹¶æ›´æ–° hash_data
            if isinstance(r, Exception):
                print(f"[set_model_datas] error {r}")
                continue

            name, hash_key = r
            if name and hash_key:
                hash_data[name] = hash_key

        await cls.to_redis(cls._hash_key, hash_data)

    @staticmethod
    def get_model_data(model: str):
        try:
            model_info, model_id = find_ai_model(model, 0, 'model')
            model_data = next((item for item in model_info.get('data', []) if item['id'] == model_id), {})
            data = {
                "id": model_id,
                "object": "model",
                "created": 0,
                "owned_by": model_info['name'],
                "type": "chat",
                "permission": [
                    {
                        "id": f"modelperm-{model_info['name']}:{model_id}",
                    }
                ],
                "supported_parameters": ["max_tokens", "stop", "temperature", "tool_choice", "tools",
                                         "top_k", "top_p"],
            }
            for k, v in model_data.items():
                if k not in {"id", "owned_by"}:
                    data[k] = v
        except ValueError as e:
            data = {'error': str(e)}
        return data

    @staticmethod
    def get_models(ai_models: list = AI_Models):
        extracted_data = extract_ai_model("model", ai_models)
        models_data = []
        for owner, models in extracted_data:
            owner_data = next((item.get('data', []) for item in ai_models if item['name'] == owner), [])
            for i, model_id in enumerate(models):
                data = {
                    "id": f"{owner}:{model_id}",  # å”¯ä¸€æ¨¡å‹ID,  ç”¨äºæŒ‡å®šæ¨¡å‹è¿›è¡Œè¯·æ±‚ fine-tuned-model
                    "object": "model",
                    "type": "chat",
                    "created": 1740386673,
                    "owned_by": owner,  # æ‹¥æœ‰è¯¥æ¨¡å‹çš„ç»„ç»‡
                    "root": model_id,  # æ ¹ç‰ˆæœ¬ï¼Œä¸ ID ç›¸åŒ
                    "parent": None,  # å¦‚æœæ²¡æœ‰çˆ¶æ¨¡å‹ï¼Œåˆ™ä¸º None
                    # "max_model_len": 4096,#GPUå†…å­˜é™åˆ¶è€Œéœ€è¦è°ƒæ•´æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦
                    "permission": [
                        {
                            "id": f"modelperm-{owner}:{model_id}",
                            "object": "model_permission",
                        }
                    ],
                    "supported_parameters": ["max_tokens", "stop", "temperature", "tool_choice", "tools",
                                             "top_k", "top_p"],
                }  # åŸºç¡€ç»“æ„
                model_data = next((item for item in owner_data if item['id'] == model_id), {})
                for k, v in model_data.items():  # è¦†ç›–æ¨¡å‹ä¿¡æ¯
                    if k not in {"id", "owned_by"}:
                        data[k] = v
                models_data.append(data)

        return {"object": "list", "data": models_data}


async def init_ai_clients(ai_models: list = AI_Models):
    limits = httpx.Limits(max_connections=max(Config.MAX_HTTP_CONNECTIONS, Config.MAX_KEEPALIVE_CONNECTIONS),
                          max_keepalive_connections=Config.MAX_KEEPALIVE_CONNECTIONS)
    transport = httpx.AsyncHTTPTransport(proxy=Config.HTTP_Proxy)
    # proxies = {"http://": Config.HTTP_Proxy, "https://": Config.HTTP_Proxy}
    # http_client = DefaultHttpxClient(proxy="http://my.test.proxy.example.com", transport=httpx.HTTPTransport(local_address="0.0.0.0"))
    for model in ai_models:
        model_name = model.get('name')
        api_key = model_api_keys(model_name)
        if api_key:
            model['api_key'] = api_key
            if model_name not in AI_Client and model.get('supported_openai'):  # model_name in SUPPORTED_OPENAI_MODELS
                http_client = None
                time_out = model.get('timeout', Config.LLM_TIMEOUT_SEC)
                if model.get('proxy'):  # proxies=proxies
                    timeout = httpx.Timeout(time_out, read=time_out, write=100.0, connect=10.0)
                    http_client = httpx.AsyncClient(transport=transport, limits=limits, timeout=timeout)

                AI_Client[model_name]: AsyncOpenAI = AsyncOpenAI(api_key=api_key, base_url=model['base_url'],
                                                                 http_client=http_client,
                                                                 max_retries=Config.MAX_RETRY_COUNT)
                if http_client is None:
                    AI_Client[model_name] = AI_Client[model_name].with_options(timeout=time_out,
                                                                               max_retries=Config.MAX_RETRY_COUNT)


# client = AI_Client['deepseek']
# print(dir(client.chat.completions))# 'create', 'with_raw_response', 'with_streaming_response'
# print(dir(client.completions))
# print(dir(client.embeddings))
# print(dir(client.files)) #'content', 'create', 'delete', 'list', 'retrieve', 'retrieve_content', 'wait_for_processing'


def find_ai_model(name: str, model_id: int = 0, search_field: str = 'model') -> Tuple[dict, str]:
    """
    åœ¨ AI_Models ä¸­æŸ¥æ‰¾æ¨¡å‹ã€‚å¦‚æœæ‰¾åˆ°åç§°åŒ¹é…çš„æ¨¡å‹ï¼Œè¿”å›æ¨¡å‹åŠå…¶ç±»å‹æˆ–å…·ä½“çš„å­æ¨¡å‹åç§°ã€‚

    å‚æ•°:
    - name: è¦æŸ¥æ‰¾çš„æ¨¡å‹åç§°
    - model_id: å¯é€‰å‚æ•°ï¼ŒæŒ‡å®šè¿”å›çš„å­æ¨¡å‹ç´¢å¼•ï¼Œé»˜è®¤ä¸º 0
    - search_field: è¦åœ¨å…¶ä¸­æŸ¥æ‰¾åç§°çš„å­—æ®µï¼ˆé»˜è®¤ä¸º 'model'ï¼‰
     è¿”å›:
    - Tuple[Dict[str, Any], Union[str, None]]: æ¨¡å‹åŠå…¶å¯¹åº”çš„å­æ¨¡å‹åç§°ï¼ˆæˆ– Noneï¼‰

    å¼‚å¸¸:
    - ValueError: å¦‚æœæœªæ‰¾åˆ°æ¨¡å‹
    """

    if ':' in name:
        parts = name.split(':', 1)
        owner, model_name = parts[0], parts[1]
        model = next((item for item in AI_Models if item['name'] == owner), None)
        if model:
            model_items = model.get(search_field, [])
            if model_name in model_items:
                return model, model_items[model_name] if isinstance(model_items, dict) else model_name

    model = next(
        (item for item in AI_Models if item['name'] == name or name in item.get(search_field, [])),
        None
    )
    if model:
        model_items = model.get(search_field, [])

        if isinstance(model_items, (list, tuple)):
            if name in model_items:
                return model, name
            if model_items:
                model_id %= len(model_items)
                return model, model_items[model_id]
        elif isinstance(model_items, dict):
            if name in model_items:
                return model, model_items[name]
            # å¦‚æœæä¾›äº†åºå·ï¼Œè¿”å›åºå·å¯¹åº”çš„å€¼
            keys = list(model_items.keys())
            model_id = model_id if abs(model_id) < len(keys) else 0
            return model, model_items[keys[model_id]]

        return model, name if model_items == name else ''

    raise ValueError(f"Model with name {name} not found.")
    # HTTPException(status_code=400, detail=f"Model with name {name} not found.")


def extract_ai_model(search_field: str = "model", ai_models: list = AI_Models):
    """
    æå– AI_Models ä¸­çš„ name ä»¥åŠ search_field ä¸­çš„æ‰€æœ‰å€¼ï¼ˆåˆ—è¡¨æˆ–å­—å…¸ keyï¼‰ã€‚

    è¿”å›ï¼š
    - List[Tuple[str, List[str]]]: æ¯ä¸ªæ¨¡å‹çš„åç§°åŠå…¶å¯¹åº”çš„æ¨¡å‹åˆ—è¡¨
    """
    extracted_data = []

    for model in ai_models:
        name = model["name"]
        field_value = model.get(search_field, [])
        if model.get('supported_openai', True) and not model.get('api_key'):
            continue

        if isinstance(field_value, list):
            extracted_data.append((name, list(dict.fromkeys(field_value))))
        elif isinstance(field_value, dict):
            extracted_data.append((name, list(field_value.keys())))
        else:
            extracted_data.append((name, [field_value]))

    return extracted_data


async def run_mcp_task(transport="streamable-http", port=7007, **kwargs):
    # subprocess.Popen(["python", "-m", "mcp", "--port", "7007"])
    shutdown_event = asyncio.Event()

    async def _run():
        await mcp.run_async(transport=transport, port=port, host="127.0.0.1", path="/mcp", **kwargs)
        await shutdown_event.wait()  # ç­‰å¾…å…³é—­ä¿¡å·

    task = asyncio.create_task(_run())
    return task, shutdown_event


def create_openai_mcp(base_url="http://127.0.0.1:7000", timeout=Config.HTTP_TIMEOUT_SEC,
                      instructions: str | None = None) -> FastMCP:
    from fastmcp.server.openapi import RouteMap, MCPType
    api_client = httpx.AsyncClient(base_url=base_url, headers={"Authorization": "Bearer YOUR_TOKEN"})
    # import requests
    # resp = requests.get(f"{base_url}/openapi.json")
    # print("TEXT:", resp.text)
    # openapi_spec = resp.json()
    from utils import load_dictjson

    # openapi_spec = httpx.get(f"{base_url}/openapi.json").json()  # Load your OpenAPI spec
    openapi_spec = load_dictjson('openapi.json', encoding='utf-8')
    print(openapi_spec)
    DEFAULT_ROUTE_MAPPINGS = [
        # custom mapping logic goes here
        # ... your specific route maps ...
        RouteMap(methods=["GET"], pattern=r".*\{.*\}.*", mcp_type=MCPType.RESOURCE_TEMPLATE),
        RouteMap(methods=["GET"], pattern=r".*", mcp_type=MCPType.RESOURCE),
        RouteMap(pattern=r"^/admin/.*", mcp_type=MCPType.EXCLUDE),
        # exclude all remaining routes
        RouteMap(mcp_type=MCPType.EXCLUDE),
    ]
    api_mcp = FastMCP.from_openapi(
        openapi_spec=openapi_spec,
        client=api_client,
        timeout=timeout,  # 30 second timeout for all requests
        route_maps=DEFAULT_ROUTE_MAPPINGS,
        instructions=instructions,
    )
    return api_mcp


# mcp.mount(openai_mcp("http://47.110.156.41:7000"), prefix="openapi")

async def call_mcp_tool(config: dict[str, Any] | str, name: str, **kwargs):
    async with MCPClient(config) as client:
        # Access tools and resources with server prefixes
        # answer = await client.call_tool("assistant_answer_question", {"query": "What is MCP?"})
        return await client.call_tool(name, **kwargs)


@mcp.custom_route("/", methods=["GET"])
async def health_check(request: Request) -> Response:
    return JSONResponse({"status": "ok"})


@mcp.resource("config://version")
def get_version():
    return Config.Version


@mcp.tool
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b


if __name__ == "__main__":
    import nest_asyncio

    # nest_asyncio.apply()
    import threading


    # mcp.run(transport="sse", log_level="debug")
    # from fastmcp.server.proxy import FastMCPProxy
    # mcp.mount(FastMCPProxy("http://other-host:8001/mcp"), prefix="remote")

    # mcp.run(transport="streamable-http", host="127.0.0.1", port=8000, path="/mcp")
    # _redis_client = Redis(host='47.110.156.41', port=7007, db=0, decode_responses=True)
    # import uvicorn
    # uvicorn.run(mcp, host="0.0.0.0", port=7007)

    async def main():
        # è¿›ç¨‹é—´é€šä¿¡ï¼Œé€‚ç”¨äºå‘½ä»¤è¡Œæˆ–è„šæœ¬å·¥å…·æ‰§è¡Œ
        # await asyncio.to_thread(mcp.run, transport="stdio", **kwargs)
        async with MCPClient("http://127.0.0.1:7000/mcp/mcp") as client:
            # ... use the client
            tools = await client.list_tools()
            print(f"Available tools: {tools}")
            result = await client.call_tool("add", {"a": 5, "b": 3})
            print(f"Result: {result}")
            resources = await client.list_resources()
            # Read a resource from the server
            data = await client.read_resource(resources[0].uri)
            print(f"Result: {data[0].text}")

        # map_task = await run_mcp_task("stdio")
        mcp_task, exit_event = await run_mcp_task(port=7007)
        # kk = await _redis_client.get('dd')
        # print(kk)
        # async with MCPClient("utils.py") as client:
        #     tools = await client.list_tools()
        #     print(f"Available tools: {tools}")
        #     result = await client.call_tool("add", {"a": 5, "b": 3})
        #     print(f"Result: {result.text}")

        # Connect via in-memory transport
        async with MCPClient(mcp) as client:
            tools = await client.list_tools()
            print(f"Available tools: {tools}")
            result = await client.call_tool("add", {"a": 5, "b": 3})
            print(f"Result: {result}")
            resources = await client.list_resources()
            results = await client.read_resource(resources[0].uri)
            print(f"Result: {results[0].text}")

        # Connect via SSE
        # async with MCPClient("http://localhost:8000/sse") as client:
        #     # ... use the client
        #     tools = await client.list_tools()
        #     print(f"Available tools: {tools}")

        # å‘å‡ºé€€å‡ºä¿¡å·å¹¶å–æ¶ˆä»»åŠ¡
        exit_event.set()
        # mcp_task.cancel()
        # await mcp_task await asyncio.shield(mcp_task)
        try:
            await mcp_task
        except asyncio.CancelledError:
            print("åå°ä»»åŠ¡å·²å–æ¶ˆ")


    Config.debug()


    async def test_r():
        redis = get_redis()
        result = await get_redis_value(redis, 'model_data_list:zzz')  # tokenflux,aihubmix
        print([item.get('id') for item in result])
        await shutdown_redis()


    # asyncio.run(main())

    asyncio.run(test_r())
