import httpx
import asyncio
import logging
import json, uuid
from logging.handlers import RotatingFileHandler
from utils import call_http_request, async_to_sync
from config import Config, AI_Models, model_api_keys, extract_ai_model

# Config.load('../config.yaml')
# if os.getenv('AIGC_DEBUG', '0').lower() in ('1', 'true', 'yes'):
#  Config.debug()
from neo4j import GraphDatabase, AsyncGraphDatabase
from dask.distributed import Client as DaskClient, LocalCluster
from qdrant_client import AsyncQdrantClient, QdrantClient
from openai import AsyncOpenAI, OpenAI

from redis.asyncio import Redis, StrictRedis, ConnectionPool
# from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
# from sqlalchemy.pool import NullPool, QueuePool
from database import MysqlData, BaseReBot
from abc import ABC, abstractmethod
from functools import partial, wraps
from typing import Callable, Optional, Type, Awaitable, Dict, Tuple

_httpx_clients: Dict[str, httpx.AsyncClient] = {}
_graph_driver: Optional[GraphDatabase] = None
# _graph_driver_lock = asyncio.Lock()  # é˜²æ­¢å¹¶å‘åˆå§‹åŒ–
_redis_client: Optional[Redis] = None  # StrictRedis(host='localhost', port=6379, db=0)
_redis_pool: Optional[ConnectionPool] = None
_dask_client: Optional[DaskClient] = None

logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.WARNING,
                    handlers=[
                        logging.StreamHandler(),  # è¾“å‡ºåˆ°ç»ˆç«¯,æ§åˆ¶å°è¾“å‡º
                        RotatingFileHandler("app.log", maxBytes=1_000_000, backupCount=3)
                        # æ–‡ä»¶æ—¥å¿—logging.FileHandler('errors.log')
                    ])
AI_Client: Dict[str, Optional[AsyncOpenAI]] = {}  # OpenAI
QD_Client = AsyncQdrantClient(host=Config.QDRANT_HOST, grpc_port=Config.QDRANT_GRPC_PORT,
                              prefer_grpc=True) if Config.QDRANT_GRPC_PORT else AsyncQdrantClient(url=Config.QDRANT_URL)

DB_Client = MysqlData(persistent=True, async_mode=True)


# echo=True ä»…ç”¨äºè°ƒè¯• poolclass=NullPool,æ¯æ¬¡è¯·æ±‚éƒ½æ–°å»ºè¿æ¥ï¼Œç”¨å®Œå°±æ–­ï¼Œä¸ç¼“å­˜
# async_engine = create_async_engine(Config.ASYNC_SQLALCHEMY_DATABASE_URI)
# AsyncSessionLocal = sessionmaker(autocommit=False, autoflush=False, expire_on_commit=False, bind=async_engine,
#                                  class_=AsyncSession)
# poolclass=QueuePool,å¤šçº¿ç¨‹å®‰å…¨çš„è¿æ¥æ± ï¼Œå¤ç”¨è¿æ¥


class DataProcessor(ABC):
    @abstractmethod
    def process(self, intermediate: dict) -> dict:
        raise NotImplementedError("å¿…é¡»å®ç°æ­¤æ–¹æ³•")


def get_httpx_client(time_out: float = Config.HTTP_TIMEOUT_SEC, proxy: str = None) -> httpx.AsyncClient:
    # @asynccontextmanager
    key = proxy or "default"
    global _httpx_clients
    if key not in _httpx_clients or _httpx_clients[key].is_closed:
        transport = httpx.AsyncHTTPTransport(proxy=proxy or None)
        limits = httpx.Limits(max_connections=100, max_keepalive_connections=50)
        timeout = httpx.Timeout(timeout=time_out, read=60.0, write=30.0, connect=5.0)
        _httpx_clients[key] = httpx.AsyncClient(transport=transport, limits=limits, timeout=timeout)
    # try:
    #     yield _httpx_clients[key] #Depends(get_httpx_client)
    # finally:
    #     # æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œå…³é—­å®¢æˆ·ç«¯ï¼Œå› ä¸ºå®ƒæ˜¯å•ä¾‹ï¼Œå…¨å±€ç”¨çš„
    #     pass

    return _httpx_clients[key]


async def shutdown_httpx():
    for key, _client in _httpx_clients.items():
        if _client and not _client.is_closed:
            await _client.aclose()


# async def get_async_session() -> AsyncSession:
#     # å¼‚æ­¥ä¾èµ–
#     async with AsyncSessionLocal() as session:
#         yield session  # è‡ªåŠ¨ await ç”Ÿæˆå™¨
#     # finally:
#     #   await session.close()

def get_redis() -> Optional[Redis]:
    global _redis_client, _redis_pool
    if _redis_client is None:
        _redis_pool = ConnectionPool(host=Config.REDIS_HOST, port=Config.REDIS_PORT, db=0,
                                     decode_responses=True,  # è‡ªåŠ¨è§£ç ä¸ºå­—ç¬¦ä¸²
                                     max_connections=Config.REDIS_MAX_CONCURRENT
                                     # connection_pool=pool
                                     )
        _redis_client = Redis(connection_pool=_redis_pool)

    return _redis_client


async def shutdown_redis():
    global _redis_client, _redis_pool
    if _redis_client:
        await _redis_client.close()
        _redis_client = None

    if _redis_pool:
        await _redis_pool.disconnect()
        _redis_pool = None


async def check_redis_connection(redis):
    try:
        await redis.ping()
        print("Redis connected.")
        return True
    except ConnectionError as e:
        print(f"âŒ Redis connection failed: {e}")
    return False


async def get_redis_connection():
    redis = get_redis()
    if not await check_redis_connection(redis):
        return None
    return redis


async def get_redis_retry(redis, key, retry: int = 3, delay: float = 0.1):
    for attempt in range(retry):
        try:
            return await redis.get(key)
        except Exception as e:
            print(f"[Redis GET] attempt {attempt + 1} failed: {e}")
            await asyncio.sleep(delay)
    raise Exception(f"Redis GET failed after {retry} retries.")


async def scan_from_redis(redis, key: str = "funcmeta", user: str = None, batch_count: int = 0):
    """
    ä» Redis ä¸­è·å–åŒ¹é…çš„æ‰€æœ‰å…ƒæ•°æ®è®°å½•ï¼Œæ”¯æŒ scan æˆ– keys æ–¹å¼ã€‚

    Args:
        redis: Redis å®ä¾‹ã€‚
        key: Redis key å‰ç¼€ï¼ˆå¦‚ "funcmeta"ï¼‰ã€‚
        user: ç”¨æˆ· IDï¼ˆå¯é€‰ï¼‰ï¼Œè‹¥æŒ‡å®šåˆ™æ‹¼æ¥ä¸º funcmeta:{user}:*
        batch_count: æ¯æ‰¹ scan çš„æ•°é‡ï¼ˆå¤§äº 0 ä½¿ç”¨ scanï¼Œå¦åˆ™ç”¨ keysï¼‰ã€‚

    Returns:
        List[dict]: åŒ¹é…åˆ°çš„ JSON æ•°æ®åˆ—è¡¨ã€‚
    """
    if user:
        match_pattern = f"{key}:{user}:*"
    else:
        match_pattern = f"{key}:*"

    data = []
    if batch_count > 0:
        cursor = b'0'
        while cursor:
            cursor, keys = await redis.scan(cursor=cursor, match=match_pattern, count=batch_count)
            if keys:
                values = await redis.mget(*keys)
                data.extend(json.loads(v) for v in values if v)
    else:
        keys = await redis.keys(match_pattern)
        if keys:
            cached_values = await redis.mget(*keys)
            data = [json.loads(v) for v in cached_values if v]  # set(cached_values
    return data


async def do_job_by_lock(func_call: Callable, redis_key: str = None, lock_timeout: int = 600,
                         logger_name: Optional[str] = None, **kwargs):
    redis = get_redis()
    if not redis:
        return await func_call(**kwargs)
    logger = logging.getLogger(logger_name)
    func_name = getattr(func_call, "__qualname__", getattr(func_call, "__name__", repr(func_call)))
    if not redis_key:
        redis_key = f'lock:{func_name}'
    lock_value = str(uuid.uuid4())  # str(time.time())ï¼Œæ¯ä¸ªworkerä½¿ç”¨å”¯ä¸€çš„lock_value
    lock_acquired = await redis.set(redis_key, lock_value, nx=True, ex=lock_timeout)
    if not lock_acquired:
        logger.info(f"âš ï¸ åˆ†å¸ƒå¼é”å·²è¢«å ç”¨ï¼Œè·³è¿‡ä»»åŠ¡: {func_name}")
        return None

    res = None
    try:
        logger.info(f"ğŸ”’ è·å–é”æˆåŠŸï¼Œå¼€å§‹æ‰§è¡Œä»»åŠ¡: {func_name}")
        res = await func_call(**kwargs)
    except Exception as e:
        logger.error(f"âš ï¸ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {func_name} -> {e}")
    finally:
        # current_lock_value = await redis.get(redis_key)
        # if current_lock_value and current_lock_value == lock_value:
        #     await redis.delete(redis_key)
        # ä½¿ç”¨ Lua è„šæœ¬ä¿è¯åŸå­æ€§ï¼Œç¡®ä¿åªæœ‰é”æŒæœ‰è€…èƒ½é‡Šæ”¾ï¼Œåªæœ‰æœ€åˆè·å–é”çš„é‚£ä¸ªworkeræ‰èƒ½æˆåŠŸåˆ é™¤é”
        lua_script = """
           if redis.call("get", KEYS[1]) == ARGV[1] then
               return redis.call("del", KEYS[1])
           else
               return 0
           end
           """
        await redis.eval(lua_script, 1, redis_key, lock_value)

    return res


def get_dask_client(cluster=None):
    global _dask_client
    if not _dask_client:
        try:
            if not cluster:
                # æœ¬æœºä¸Šå¯åŠ¨è‹¥å¹²ä¸ª worker è¿›ç¨‹ï¼ˆå’Œä¸€ä¸ª scheduler) http://127.0.0.1:8787
                cluster = LocalCluster(scheduler_port=8786, n_workers=4, threads_per_worker=1)
            _dask_client = DaskClient(cluster)  # åˆ›å»ºDaskå®¢æˆ·ç«¯ 'tcp://127.0.0.1:8786
            # print(_dask_client.get_versions(check=True))
        except OSError:
            print("Schedulerç«¯å£è¢«å ç”¨ï¼Œè¿æ¥å·²æœ‰é›†ç¾¤")
            _dask_client = DaskClient("tcp://127.0.0.1:8786", compression=None)
    return _dask_client


def get_neo_driver():
    global _graph_driver
    if _graph_driver is None:
        _graph_driver = AsyncGraphDatabase.driver(uri=Config.NEO_URI, auth=(Config.NEO_Username, Config.NEO_Password),
                                                  max_connection_lifetime=3600,  # å•è¿æ¥ç”Ÿå‘½å‘¨æœŸ
                                                  max_connection_pool_size=30,  # æœ€å¤§è¿æ¥æ± æ•°é‡
                                                  connection_timeout=30  # è¶…æ—¶
                                                  )
    return _graph_driver


def get_w3():
    try:
        from web3 import Web3

        w3 = Web3(
            Web3.HTTPProvider(f'https://mainnet.infura.io/v3/{Config.INFURA_PROJECT_ID}'))  # ("http://127.0.0.1:8545")
        return w3
    except ImportError:
        print("[Web3 Init] Web3 not installed.")
    except Exception as e:
        print(f"[Web3 Init] Failed to get web3: {e}")

    return None


def error_logger(extra_msg=None, logger_name=None):
    """
    é”™è¯¯æ—¥å¿—è£…é¥°å™¨ @error_logger()
    å‚æ•°:
        logger_name (str): æ—¥å¿—å™¨åç§°
    """
    logger = logging.getLogger(logger_name)

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                # logger.debug(f"Entering {func.__name__}")
                return func(*args, **kwargs)
            except Exception as e:
                msg = f"Error in {func.__name__}: {e}"
                if extra_msg:
                    msg += f" | Extra: {extra_msg}"
                logger.error(msg, exc_info=True)
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸

        return wrapper

    return decorator


def async_error_logger(max_retries: int = 0, delay: int | float = 1, backoff: int | float = 2,
                       exceptions: Type[Exception] | tuple[Type[Exception], ...] = Exception,
                       extra_msg: str = None, logger_name: Optional[str] = None, log_level: int = logging.ERROR):
    """
    å¼‚æ­¥å‡½æ•°çš„é”™è¯¯é‡è¯•å’Œæ—¥å¿—è®°å½•è£…é¥°å™¨

    å‚æ•°:
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆä¸å«é¦–æ¬¡å°è¯•ï¼‰ï¼Œé»˜è®¤ä¸º 0ï¼Œè¡¨ç¤ºä¸é‡è¯•ï¼›è®¾ä¸º 1 è¡¨ç¤ºå¤±è´¥åé‡è¯•ä¸€æ¬¡ï¼ˆå…±å°è¯• 2 æ¬¡ï¼‰ã€‚
        delay (int/float): åˆå§‹å»¶è¿Ÿæ—¶é—´(ç§’)ï¼Œé»˜è®¤ä¸º1
        backoff (int/float): å»¶è¿Ÿæ—¶é—´å€å¢ç³»æ•°ï¼Œé»˜è®¤ä¸º2
        exceptions (Exception/tuple): è¦æ•è·çš„å¼‚å¸¸ç±»å‹ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰å¼‚å¸¸
        logger_name (Logger): è‡ªå®šä¹‰loggerï¼Œé»˜è®¤ä½¿ç”¨æ¨¡å—logger
        log_level (int): æ—¥å¿—çº§åˆ«
    """
    logger = logging.getLogger(logger_name)

    def decorator(func: Callable[..., Awaitable]):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ä¼˜å…ˆä½¿ç”¨è°ƒç”¨æ—¶çš„å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰å°±ç”¨è£…é¥°å™¨é»˜è®¤å€¼
            _max_retries = kwargs.pop("max_retries", max_retries)
            _delay = kwargs.pop("delay", delay)
            _backoff = kwargs.pop("backoff", backoff)
            _extra_msg = kwargs.pop("extra_msg", extra_msg)

            retry_count = 0
            current_delay = _delay

            while True:
                try:
                    return await func(*args, **kwargs)
                except exceptions as e:
                    retry_count += 1
                    msg = f"Async function {func.__name__} failed with error: {str(e)}."
                    if _extra_msg:
                        msg += f" | Extra: {_extra_msg}"
                    if retry_count > _max_retries:
                        logger.log(log_level, f"{msg} After {_max_retries} retries", exc_info=True)
                        raise  # é‡è¯•æ¬¡æ•°ç”¨å°½åé‡æ–°æŠ›å‡ºå¼‚å¸¸

                    logger.log(log_level, f"{msg} Retrying {retry_count}/{_max_retries} in {current_delay} seconds...")
                    await asyncio.sleep(current_delay)
                    current_delay *= _backoff  # æŒ‡æ•°é€€é¿

        return wrapper

    return decorator


@async_error_logger(1)
async def get_data_for_model(model: dict):
    """è·å–æ¯ä¸ªæ¨¡å‹çš„æ•°æ®"""
    model_name = model.get('name')
    client = AI_Client.get(model_name)

    if client:
        try:
            models = await client.models.list()
            return [m.model_dump() for m in models.data]
        except Exception as e:
            print(f"OpenAI error occurred:{e},name:{model_name}")
    else:
        url = model['base_url'] + '/models'
        models = await call_http_request(url)
        if models:
            return models.get('data')

    return None


async def set_data_for_model(model: dict, redis=None):
    key = f"model_data_list:{model.get('name')}"
    data = await redis.get(key) if redis else None
    if data:
        model['data'] = json.loads(data)
        return

    data = await get_data_for_model(model)
    if data:
        model['data'] = data
        if redis:
            await redis.set(key, json.dumps(data, ensure_ascii=False))
        print('model:', model.get('name'), 'data:', data)


class ModelListExtract:
    models = []
    _redis = None

    @classmethod
    def extract(cls):
        """
        æå– AI_Models ä¸­çš„ name ä»¥åŠ search_field ä¸­çš„æ‰€æœ‰å€¼ï¼Œå¹¶å­˜å…¥ä¸€ä¸ªå¤§åˆ—è¡¨ã€‚

        è¿”å›ï¼š
        - List[str]: åŒ…å«æ‰€æœ‰æ¨¡å‹åç§°åŠå…¶å­æ¨¡å‹çš„åˆ—è¡¨
        """
        # list(itertools.chain(*[sublist[1] for sublist in extract_ai_model("model")]))
        extracted_data = extract_ai_model("model")
        return [i for item in extracted_data for i in [item[0]] + item[1]]  # flattened_list

    @classmethod
    async def set(cls, redis=None):
        """æ›´æ–° MODEL_LIST,å¹¶ä¿å­˜åˆ° Redis"""
        cls.models = cls.extract()
        if cls._redis is None:
            cls._redis = redis or get_redis()
        if cls._redis:
            await cls._redis.set("model_list", json.dumps(cls.models, ensure_ascii=False))

    @classmethod
    async def get(cls):
        if cls._redis:
            data = await cls._redis.get("model_list")
            if data:
                return json.loads(data)
        if not cls.models:
            await cls.set()
            print("model_list updated:", cls.models)

        return cls.models

    @classmethod
    def contains(cls, value):
        models = cls.models or async_to_sync(cls.get)
        if ':' in value:
            owner, name = value.split(':')
            return owner in models or name in models

        return value in models


async def init_ai_clients(ai_models=AI_Models, get_data=False, redis=None):
    limits = httpx.Limits(max_connections=100, max_keepalive_connections=50)
    transport = httpx.AsyncHTTPTransport(proxy=Config.HTTP_Proxy)
    # proxies = {"http://": Config.HTTP_Proxy, "https://": Config.HTTP_Proxy}
    # http_client = DefaultHttpxClient(proxy="http://my.test.proxy.example.com", transport=httpx.HTTPTransport(local_address="0.0.0.0"))
    for model in ai_models:
        model_name = model.get('name')
        api_key = model_api_keys(model_name)
        if api_key:
            model['api_key'] = api_key
            if model_name not in AI_Client and model.get('supported_openai'):  # model_name in SUPPORTED_OPENAI_MODELS
                http_client = None
                time_out = model.get('timeout', Config.HTTP_TIMEOUT_SEC * 2)
                if model.get('proxy'):  # proxies=proxies
                    timeout = httpx.Timeout(time_out, read=time_out, write=60.0, connect=10.0)
                    http_client = httpx.AsyncClient(transport=transport, limits=limits, timeout=timeout)

                AI_Client[model_name]: AsyncOpenAI = AsyncOpenAI(api_key=api_key, base_url=model['base_url'],
                                                                 http_client=http_client)  # OpenAI
                if http_client is None:
                    AI_Client[model_name] = AI_Client[model_name].with_options(timeout=time_out, max_retries=3)

    if get_data:
        tasks = [set_data_for_model(model, redis=redis) for model in ai_models if
                 model.get('supported_list') and model.get('api_key')]
        await asyncio.gather(*tasks)

    await ModelListExtract.set(redis)
    # print(len(ModelListExtract.models))


# client = AI_Client['deepseek']
# print(dir(client.chat.completions))# 'create', 'with_raw_response', 'with_streaming_response'
# print(dir(client.completions))
# print(dir(client.embeddings))
# print(dir(client.files)) #'content', 'create', 'delete', 'list', 'retrieve', 'retrieve_content', 'wait_for_processing'


def find_ai_model(name, model_id: int = 0, search_field: str = 'model') -> Tuple[dict, str]:
    """
    åœ¨ AI_Models ä¸­æŸ¥æ‰¾æ¨¡å‹ã€‚å¦‚æœæ‰¾åˆ°åç§°åŒ¹é…çš„æ¨¡å‹ï¼Œè¿”å›æ¨¡å‹åŠå…¶ç±»å‹æˆ–å…·ä½“çš„å­æ¨¡å‹åç§°ã€‚

    å‚æ•°:
    - name: è¦æŸ¥æ‰¾çš„æ¨¡å‹åç§°
    - model_id: å¯é€‰å‚æ•°ï¼ŒæŒ‡å®šè¿”å›çš„å­æ¨¡å‹ç´¢å¼•ï¼Œé»˜è®¤ä¸º 0
    - search_field: è¦åœ¨å…¶ä¸­æŸ¥æ‰¾åç§°çš„å­—æ®µï¼ˆé»˜è®¤ä¸º 'model'ï¼‰
     è¿”å›:
    - Tuple[Dict[str, Any], Union[str, None]]: æ¨¡å‹åŠå…¶å¯¹åº”çš„å­æ¨¡å‹åç§°ï¼ˆæˆ– Noneï¼‰

    å¼‚å¸¸:
    - ValueError: å¦‚æœæœªæ‰¾åˆ°æ¨¡å‹
    """
    if ':' in name:
        parts = name.split(':', 1)
        owner, model_name = parts[0], parts[1]
        model = next((item for item in AI_Models if item['name'] == owner), None)
        if model and model_name in model.get(search_field, []):
            return model, model_name

    model = next(
        (item for item in AI_Models if item['name'] == name or name in item.get(search_field, [])),
        None
    )
    if model:
        model_items = model.get(search_field, [])

        if isinstance(model_items, (list, tuple)):
            if name in model_items:
                return model, name
            if model_items:
                model_id %= len(model_items)
                return model, model_items[model_id]
        elif isinstance(model_items, dict):
            if name in model_items:
                return model, model_items[name]
            # å¦‚æœæä¾›äº†åºå·ï¼Œè¿”å›åºå·å¯¹åº”çš„å€¼
            keys = list(model_items.keys())
            model_id = model_id if abs(model_id) < len(keys) else 0
            return model, model_items[keys[model_id]]

        return model, name if model_items == name else ''

    raise ValueError(f"Model with name {name} not found.")
    # HTTPException(status_code=400, detail=f"Model with name {name} not found.")
