from faster_whisper import WhisperModel
import ffmpeg
import os
from datetime import datetime

# æ¨¡å‹é€‰æ‹©é…ç½®
# å¯é€‰æ¨¡å‹: "tiny", "base", "small", "medium", "large-v3", "turbo"
# ç²¾åº¦æ’åº: tiny < base < small < medium < large-v3 â‰ˆ turbo
# é€Ÿåº¦æ’åº: tiny > base > small > medium > turbo > large-v3

# æ¨èé…ç½®ï¼š
# - å¦‚æœè¿½æ±‚é€Ÿåº¦: "small"
# - å¦‚æœè¿½æ±‚ç²¾åº¦: "medium" æˆ– "large-v3"
# - å¦‚æœè¦å¹³è¡¡: "turbo" (æœ€æ–°ä¼˜åŒ–ç‰ˆæœ¬)

# os.environ['HTTP_PROXY'] = 'http://..:.7@10.10.10.3:7890'
# os.environ['HTTPS_PROXY'] = 'http://..:.7@10.10.10.3:7890'
# transfer.xethub.hf.co
MODEL_SIZE = "medium"  # ğŸ”¥tiny, base, small, medium, large-v3
COMPUTE_TYPE = "int8"  # å¯é€‰: "int8", "float16", "float32"

print(f"ä½¿ç”¨æ¨¡å‹: {MODEL_SIZE}")
print(f"è®¡ç®—ç±»å‹: {COMPUTE_TYPE}")


# Step 1: Convert MP3/WAV to 16kHz mono wav
def convert_to_wav16k(input_path, output_path="converted.wav"):
    if os.path.exists(output_path):
        print(f"WAVæ–‡ä»¶ '{output_path}' å·²å­˜åœ¨ï¼Œè·³è¿‡è½¬æ¢æ­¥éª¤")
        return output_path

    print(f"æ­£åœ¨è½¬æ¢ '{input_path}' åˆ° '{output_path}'...")
    (
        ffmpeg.input(input_path)
        .output(output_path, ac=1, ar=16000)
        .overwrite_output()
        .run(quiet=True)
    )
    print(f"è½¬æ¢å®Œæˆ: {output_path}")
    return output_path


# Step 2: Load model
print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ '{MODEL_SIZE}'...")
model = WhisperModel(MODEL_SIZE, compute_type=COMPUTE_TYPE)  # device="cuda",
print("æ¨¡å‹åŠ è½½å®Œæˆ!")

name = '11369449831'  # '11389271598' ,11389284145','11389274136'
# Step 3: è½¬æ¢éŸ³é¢‘
wav_path = convert_to_wav16k(f"../data/{name}.mp3", f"data/{name}_converted.wav")  # æˆ– your_long_audio.wav

# Step 4: åˆ›å»ºæ–‡æœ¬æ–‡ä»¶å¹¶å†™å…¥å¤´éƒ¨ä¿¡æ¯
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_filename = f"transcription_{MODEL_SIZE}_{name}_{timestamp}.txt"

print(f"æ­£åœ¨åˆ›å»ºæ–‡æœ¬æ–‡ä»¶: {output_filename}")
with open(output_filename, "w", encoding="utf-8") as f:
    f.write(f"éŸ³é¢‘æ–‡ä»¶: {wav_path}\n")
    f.write(f"ä½¿ç”¨æ¨¡å‹: {MODEL_SIZE}\n")
    f.write(f"è®¡ç®—ç±»å‹: {COMPUTE_TYPE}\n")
    f.write(f"è¯†åˆ«å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write("-" * 50 + "\n\n")

# Step 5: è¯†åˆ«å¹¶é€å¥è¾“å‡ºå’Œå†™å…¥
segments, info = model.transcribe(wav_path, beam_size=5, language="zh", word_timestamps=False)

print("å¼€å§‹è¯†åˆ«...")
print(f"å®æ—¶ä¿å­˜åˆ°: {output_filename}")

with open(output_filename, "a", encoding="utf-8") as f:
    for segment in segments:
        segment_text = f"[{segment.start:.1f}s - {segment.end:.1f}s] {segment.text}"
        print(segment_text)
        f.write(segment_text + "\n")
        f.flush()  # ç¡®ä¿ç«‹å³å†™å…¥ç£ç›˜

    # åœ¨æœ€åæ·»åŠ å®Œæˆä¿¡æ¯
    f.write(f"\n" + "-" * 50 + "\n")
    f.write(f"è¯†åˆ«å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"è¯­è¨€: {info.language}\n")
    f.write(f"æ€»æ—¶é•¿: {info.duration:.1f}ç§’\n")

print(f"\nè¯†åˆ«å®Œæˆï¼å®Œæ•´ç»“æœä¿å­˜åœ¨: {output_filename}")
