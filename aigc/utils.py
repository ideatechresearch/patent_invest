import re, json, io, os, sys, threading, time, pickle, struct
import httpx, aiohttp, aiofiles, asyncio, joblib
from urllib.parse import urlencode
import inspect, importlib, ast, requests
from itertools import groupby
import yaml
from pathlib import Path
from contextlib import redirect_stdout
import xml.etree.ElementTree as ET
from difflib import get_close_matches, SequenceMatcher
from functools import partial, wraps  # cache, lru_cache, partial, wraps
from collections import OrderedDict, Counter, deque
import numpy as np
from enum import Enum
import math
import base64
from pypinyin import lazy_pinyin
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from langdetect import detect, detect_langs
import jieba


# import tiktoken
# tokenizer = tiktoken.encoding_for_model("gpt-3.5-turbo")

def get_function_parameters(func):
    signature = inspect.signature(func)
    parameters = signature.parameters
    return [param for param in parameters]


def get_function_string(func):
    try:
        function_code = inspect.getsource(func)
        return function_code
    except Exception as e:
        return f"{e}"


# é€’å½’åœ°å°†å­—å…¸ä¸­çš„æ‰€æœ‰é”®åè½¬æ¢ä¸ºé¦–å­—æ¯å¤§å†™çš„é©¼å³°å‘½å
def convert_keys_to_pascal_case(d):
    if isinstance(d, dict):  # user_name->UserName
        return {''.join(x.title() for x in k.split('_')): convert_keys_to_pascal_case(v) for k, v in d.items()}
    elif isinstance(d, list):
        return [convert_keys_to_pascal_case(item) for item in d]
    else:
        return d


def convert_keys_to_lower_case(d):
    """é€’å½’åœ°å°†å­—å…¸çš„æ‰€æœ‰é”®è½¬æ¢ä¸ºå°å†™"""
    if isinstance(d, dict):
        return {k.lower(): convert_keys_to_lower_case(v) for k, v in d.items()}
    elif isinstance(d, list):
        return [convert_keys_to_lower_case(item) for item in d]
    else:
        return d


def serialize(obj):
    if isinstance(obj, Enum):  # å¤„ç† Enum ç±»å‹
        return obj.value
    elif hasattr(obj, "dict"):  # å¤„ç† Pydantic æ¨¡å‹
        return obj.dict()
    elif hasattr(obj, "__dict__"):  # é€’å½’è½¬æ¢å¯¹è±¡
        return {key: serialize(value) for key, value in obj.__dict__.items()}
    elif isinstance(obj, list):  # å¤„ç†åˆ—è¡¨
        return [serialize(item) for item in obj]
    elif isinstance(obj, dict):  # å¤„ç†å­—å…¸
        return {key: serialize(value) for key, value in obj.items()}
    else:
        return obj  # asdict


def parse_json(inputs: dict | str) -> dict:
    # æ”¯æŒå¤šç§æ ¼å¼ï¼ˆMarkdown JSON å—ã€æ™®é€š JSON å­—ç¬¦ä¸²ã€å­—å…¸ç­‰ï¼‰,æ”¯æŒå·²ç»æ˜¯å­—å…¸çš„è¾“å…¥
    if not isinstance(inputs, dict):
        try:
            match = re.search(r'^\s*(```json\n)?(.*)\n```\s*$', inputs, re.S)
            if match:
                inputs = match.group(2).strip()
            inputs = json.loads(inputs)
        except json.JSONDecodeError as exc:
            raise Exception(f'invalid json format: {inputs}') from exc

    return inputs


def extract_json_str(json_code: str):
    """
    æ¨¡å‹è¿”å›çš„å†…å®¹ï¼Œå…¶ä¸­ JSON æ•°æ®é€šå¸¸è¢«åŒ…è£¹åœ¨ Markdown çš„ä»£ç å—æ ‡è®°ä¸­ï¼ˆå³ä»¥ json å¼€å§‹ï¼Œä»¥ ç»“æŸï¼‰
    å¦‚æœæœªæ‰¾åˆ°èµ·å§‹æˆ–ç»“æŸæ ‡è®°ï¼Œå°è¯•ç›´æ¥è§£ææ•´ä¸ªå­—ç¬¦ä¸²ä¸º JSON
    :param json_code:
    :return:
    """
    start = json_code.find("```json")
    # ä»startå¼€å§‹æ‰¾åˆ°ä¸‹ä¸€ä¸ª```ç»“æŸ
    end = json_code.find("```", start + 1)
    if start == -1 or end == -1:
        try:
            json.loads(json_code)
            return json_code
        except Exception as e:
            print("Error:", e)
        return ""
    return json_code[start + 7:end]


def extract_method_calls(text):
    """
    æå– Python ä»£ç ä¸­çš„æ–¹æ³•è°ƒç”¨ï¼ˆæ–¹æ³•å+æ‹¬å·ï¼‰ã€‚
    æ”¯æŒï¼š
    - æ™®é€šå‡½æ•°è°ƒç”¨ func()
    - å¯¹è±¡/ç±»æ–¹æ³•è°ƒç”¨ obj.method()
    Args:
        text (str): ä»£ç æ–‡æœ¬
    Returns:
        list: ä»£ç ä¸­çš„æ–¹æ³•è°ƒç”¨åˆ—è¡¨
    """
    # åŒ¹é…æ–¹æ³•è°ƒç”¨ï¼ˆæ–¹æ³•å+æ‹¬å·å†…å®¹ï¼‰
    pattern = r"\b(?:[a-zA-Z_][a-zA-Z0-9_]*\.)?([a-zA-Z_][a-zA-Z0-9_]*)\s*\([^)]*\)"  # r"\b[a-zA-Z_][a-zA-Z0-9_]*\b"
    # æ£€æŸ¥åŒ¹é…é¡¹
    return re.findall(pattern, text)


def execute_code_results(text):
    code_blocks = extract_python_code(text)
    results = []
    global_namespace = globals()  # å¼•ç”¨å…¨å±€å‘½åç©ºé—´ {"__builtins__": dict(__builtins__)}
    for code in code_blocks:
        local_namespace = {}  # ç”¨äºå­˜å‚¨ä»£ç çš„å±€éƒ¨å˜é‡
        captured_output = io.StringIO()  # ç”¨äºæ•è· `print` è¾“å‡º
        try:
            with redirect_stdout(captured_output):  # é‡å®šå‘ `print` è¾“å‡º
                exec(code, global_namespace, local_namespace)
                # exec(code, globals=None, locals=None)ç”¨äºåŠ¨æ€æ‰§è¡Œè¾ƒå¤æ‚çš„ä»£ç å—,ä¸è¿”å›ç»“æœ,éœ€è¦é€šè¿‡å…¨å±€æˆ–å±€éƒ¨å˜é‡è·å–ç»“æœ
            output = captured_output.getvalue()  # è·å– `print` çš„å†…å®¹
            results.append({
                "output": output.strip(),
                "namespace": local_namespace,
                "error": None
            })
        except Exception as e:
            results.append({
                "output": captured_output.getvalue().strip(),
                "namespace": local_namespace,
                "error": f"Error executing code block: {e}"
            })
        finally:
            captured_output.close()

    return results


def pip_install(*args):
    import subprocess  # nosec
    # import platform
    cli_args = []
    for arg in args:
        cli_args.extend(str(arg).split(" "))  # export_command.split(" "),

    subprocess.run([sys.executable, "-m", "pip", "install", *cli_args],
                   # shell=(platform.system() == "Windows"),
                   check=True)


def git_repo_clone(repo_url: str, revision: str = None, add_to_sys_path: bool = True) -> Path:
    # è‡ªåŠ¨åŒ–ç¯å¢ƒéƒ¨ç½²ã€åŠ¨æ€åŠ è½½ Git ä»“åº“ä»£ç 
    repo_path = Path(repo_url.split("/")[-1].replace(".git", ""))
    import subprocess
    if not repo_path.exists():  # æœ¬åœ°æ²¡æœ‰è¯¥ä»“åº“
        try:
            subprocess.run(["git", "clone", repo_url], check=True,
                           stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        except Exception as exc:
            print(f"Failed to clone the repository: {exc.stderr}")
            raise

        if revision:  # åˆ‡æ¢åˆ†æ”¯æˆ–æäº¤
            subprocess.Popen(["git", "checkout", revision], cwd=str(repo_path))
    if add_to_sys_path and str(repo_path.resolve()) not in sys.path:
        sys.path.insert(0, str(repo_path.resolve()))
        # åŠ å…¥ Python çš„ sys.pathï¼Œä»¥ä¾¿ importè¯¥ä»“åº“çš„æ¨¡å—

    return repo_path


async def download_by_aiohttp(url: str, save_path, chunk_size=4096, in_decode=False):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            # response = await asyncio.wait_for(session.get(url), timeout=timeout)
            if response.status == 200:
                if save_path:
                    save_path.parent.mkdir(parents=True, exist_ok=True)
                    async with aiofiles.open(save_path, mode='wb') as f:
                        # response.content å¼‚æ­¥è¿­ä»£å™¨ï¼ˆæµå¼è¯»å–ï¼‰,iter_chunked éé˜»å¡è°ƒç”¨ï¼Œé€å—è¯»å–
                        async for chunk in response.content.iter_chunked(chunk_size):
                            if isinstance(chunk, (bytes, bytearray)):
                                await f.write(chunk)
                            elif isinstance(chunk, str):
                                await f.write(chunk.encode('utf-8'))  # å°†å­—ç¬¦ä¸²è½¬ä¸ºå­—èŠ‚
                            else:
                                raise TypeError(
                                    f"Unexpected chunk type: {type(chunk)}. Expected bytes or bytearray.")

                    return save_path

                content = await response.read()  # å•æ¬¡å¼‚æ­¥è¯»å–å®Œæ•´å†…å®¹ï¼Œå°æ–‡ä»¶,await response.content.read(chunk_size)
                return content.decode('utf-8') if in_decode else content  # å°†å­—èŠ‚è½¬ä¸ºå­—ç¬¦ä¸²,è§£ç åçš„å­—ç¬¦ä¸² await response.text()

            print(f"Failed to download {url}: {response.status}")

    return None


async def download_by_httpx(url: str, save_path, chunk_size=4096, in_decode=False):
    async with httpx.AsyncClient() as client:
        async with client.stream("GET", url) as response:  # é•¿æ—¶é—´çš„æµå¼è¯·æ±‚
            # response = await client.get(url, stream=True)
            response.raise_for_status()  # å¦‚æœå“åº”ä¸æ˜¯2xx  response.status_code == 200:
            if save_path:
                save_path.parent.mkdir(parents=True, exist_ok=True)
                async with aiofiles.open(save_path, mode="wb") as f:
                    # response.aiter_bytes() å¼‚æ­¥è¿­ä»£å™¨
                    async for chunk in response.aiter_bytes(chunk_size=chunk_size):
                        await f.write(chunk)

                return save_path

            content = bytearray()  # ä½¿ç”¨ `bytearray` æ›´é«˜æ•ˆåœ°æ‹¼æ¥äºŒè¿›åˆ¶å†…å®¹  b""  # raw bytes
            async for chunk in response.aiter_bytes(chunk_size=chunk_size):
                content.extend(chunk)
                # content += chunk
                # yield chunk

            return content.decode(response.encoding or 'utf-8') if in_decode else bytes(content)
            # return response.text if in_decode else response.content


def download_by_requests(url: str, save_path, chunk_size=4096, in_decode=False):
    """
    åŒæ­¥ä¸‹è½½çš„æµå¼æ–¹æ³•
    å¦‚æœç›®æ ‡æ˜¯ä¿å­˜åˆ°æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨ contentï¼ˆæ— éœ€è§£ç ï¼‰ã€‚ï¼ˆå¦‚å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ã€PDF ç­‰ï¼‰
    å¦‚æœç›®æ ‡æ˜¯å¤„ç†å’Œè§£ææ–‡æœ¬æ•°æ®ï¼Œä¸”ç¡®å®šç¼–ç æ­£ç¡®ï¼Œä½¿ç”¨ textã€‚ï¼ˆå¦‚ HTMLã€JSONï¼‰
    """
    with requests.get(url, stream=True, timeout=30) as response:
        response.raise_for_status()  # ç¡®ä¿è¯·æ±‚æˆåŠŸ
        if save_path:
            save_path.parent.mkdir(parents=True, exist_ok=True)
            with open(save_path, "wb") as f:
                # requests iter_content åŒæ­¥ä½¿ç”¨,é˜»å¡è°ƒç”¨
                for chunk in response.iter_content(chunk_size=chunk_size):
                    if chunk:  # <class 'bytes'>
                        f.write(chunk)

            return save_path

        return response.text if in_decode else response.content  # ç›´æ¥è·å–æ–‡æœ¬,åŒæ­¥ç›´æ¥è¿”å›å…¨éƒ¨å†…å®¹


async def upload_by_httpx(url: str, files_path=('example.txt', b'Hello World')):
    files = {'file': files_path}
    async with httpx.AsyncClient() as client:
        resp = await client.post(url, files=files)
        return resp.json()


def upload_by_requests(url: str, file_path, file_key='snapshot'):
    with open(file_path, "rb") as f:
        files = {file_key: f}
        response = requests.post(url, files=files)
    return response.json()


# ä¿å­˜æ‰€æœ‰æ¨¡å‹åˆ°æ–‡ä»¶
def save_models(models: dict, model_dir='data/models/'):
    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„
    :param models: ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰éœ€è¦ä¿å­˜çš„æ¨¡å‹
    :param model_dir: ä¿å­˜æ¨¡å‹çš„è·¯å¾„
    """
    os.makedirs(model_dir, exist_ok=True)
    for model_name, model in models.items():
        if model is not None:
            joblib.dump(model, f'{model_dir}/{model_name}.pkl')
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³ {model_dir}")


# åŠ è½½æ¨¡å‹
def load_models(model_names: list, model_dir='data/models/'):
    """
    åŠ è½½ä¿å­˜çš„æ¨¡å‹
    :param model_names: æ¨¡å‹åç§°åˆ—è¡¨
    :param model_dir: ä¿å­˜æ¨¡å‹çš„è·¯å¾„
    :return: åŠ è½½çš„æ¨¡å‹å­—å…¸
    """
    models = {}
    for model_name in model_names:
        model_path = f'{model_dir}/{model_name}.pkl'
        if os.path.exists(model_path):
            models[model_name] = joblib.load(model_path)
    print(f"æ¨¡å‹å·²ä» {model_dir} åŠ è½½")
    return models


def load_datasets(path):
    samples = []
    with open(path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            data = json.loads(line.strip())
            samples.append(data)
    return samples


# @asynccontextmanager
async def get_httpx_client(time_out=100.0):
    limits = httpx.Limits(max_connections=100, max_keepalive_connections=50)
    timeout = httpx.Timeout(timeout=time_out, read=60.0, write=30.0, connect=5.0)
    async with httpx.AsyncClient(limits=limits, timeout=timeout) as cx:
        yield cx


async def call_http_request(url, headers=None, time_out=100.0, **kwargs):
    async with httpx.AsyncClient() as cx:
        try:
            response = await cx.get(url, headers=headers, timeout=time_out, **kwargs)
            response.raise_for_status()
            return response.json()
        except json.JSONDecodeError as e:
            return {'text': response.text}
        except Exception as e:
            print(e)
            return None


async def follow_http_html(url, time_out=100.0, **kwargs):
    async with httpx.AsyncClient(timeout=time_out, follow_redirects=True) as cx:
        try:
            response = await cx.get(url, **kwargs)
            response.raise_for_status()
            return response.text
        except httpx.HTTPError as e:
            print(f"[HTTP error] {e}")
            return None
        except Exception as e:
            print(e)
            return None


async def post_http_json(url, json=None, headers: dict = None, time_out=30, **kwargs):
    connector = aiohttp.TCPConnector(limit=100, limit_per_host=50)
    timeout = aiohttp.ClientTimeout(total=time_out, sock_read=60.0, sock_connect=5.0)
    async with aiohttp.ClientSession(connector=connector, timeout=timeout, headers=headers or {}) as session:
        try:
            async with session.post(url, json=json, **kwargs) as resp:
                resp.raise_for_status()  # æŠ›å‡º 4xx/5xx é”™è¯¯
                try:
                    return await resp.json()
                except aiohttp.ContentTypeError:
                    return {"status": resp.status, "error": "Non-JSON response", "body": await resp.text()}
        except aiohttp.ClientResponseError as e:
            print(f"[HTTP Error] Status: {e.status}, URL: {url}, Message: {e.message},body:{json}")
            return {"status": e.status, "error": e.message, "url": url}
        except (aiohttp.ClientConnectorError, asyncio.TimeoutError) as e:
            print(f"[Connection Error] URL: {url}, Message: {e}")
            return {"status": 503, "error": f"Connection failed: {str(e)}", "url": url}
        except Exception as e:
            print(f"[Unknown Error] URL: {url}, Exception: {e}")
            return {"status": 500, "error": str(e), "url": url}


async def post_http_form(url, data, headers=None, time_out=30, **kwargs):
    headers = headers or {}
    headers['Content-Type'] = 'application/x-www-form-urlencoded'
    async with aiohttp.ClientSession() as session:
        try:
            async with session.post(url, data=data, headers=headers, timeout=time_out, **kwargs) as resp:
                try:
                    resp.raise_for_status()
                    return await resp.json()
                except aiohttp.ContentTypeError:
                    return {"status": resp.status, "error": "Non-JSON response", "body": await resp.text()}

        except Exception as e:
            print(f"[Form POST Error] URL: {url}, Exception: {e}, Data: {data}")
            return {"status": 500, "error": str(e), "url": url}


async def get_http_query(url, params, headers=None, time_out=30, **kwargs):
    query_string = urlencode(params)
    url_with_params = f"{url}?{query_string}"
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url_with_params, headers=headers or {}, timeout=time_out, **kwargs) as resp:
                try:
                    resp.raise_for_status()
                    return await resp.json()
                except aiohttp.ContentTypeError:
                    return {"status": resp.status, "error": "Non-JSON response", "body": await resp.text()}
        except Exception as e:
            print(f"[GET Query Error] URL: {url_with_params}, Exception: {e}")
            return {"status": 500, "error": str(e), "url": url_with_params}


async def embed_images_as_base64(md_content, image_dir):
    """å¼‚æ­¥å°†Markdownä¸­çš„å›¾ç‰‡è½¬æ¢ä¸ºBase64å¹¶åµŒå…¥åˆ°Markdownä¸­"""
    lines = md_content.split('\n')
    new_lines = []

    for line in lines:
        if line.startswith("![") and "](" in line and ")" in line:
            start_idx = line.index("](") + 2
            end_idx = line.index(")", start_idx)
            img_rel_path = line[start_idx:end_idx]

            img_name = os.path.basename(img_rel_path)
            img_path = os.path.join(image_dir, img_name)

            if os.path.exists(img_path):
                # å¼‚æ­¥è¯»å–å¹¶è½¬æ¢å›¾ç‰‡ä¸ºBase64
                async with aiofiles.open(img_path, 'rb') as img_file:
                    img_data = await img_file.read()
                    img_base64 = base64.b64encode(img_data).decode('utf-8')

                img_extension = os.path.splitext(img_name)[-1].lower()
                # æ ¹æ®æ‰©å±•åç¡®å®š MIME ç±»å‹
                if img_extension in ['.jpg', '.jpeg']:
                    mime_type = 'image/jpeg'
                elif img_extension == '.gif':
                    mime_type = 'image/gif'
                else:
                    mime_type = 'image/png'
                # ä¿®æ”¹Markdownä¸­çš„å›¾ç‰‡è·¯å¾„ä¸ºBase64ç¼–ç 
                new_line = f'{line[:start_idx]}data:{mime_type};base64,{img_base64}{line[end_idx:]}'
                new_lines.append(new_line)
            else:  # å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¿ç•™åŸå§‹Markdownæ ¼å¼
                new_lines.append(line)
        else:  # ä¿ç•™éå›¾ç‰‡é“¾æ¥çš„åŸå§‹è¡Œ
            new_lines.append(line)

    return '\n'.join(new_lines)


# è°ƒæ•´ç¼©è¿›,ä¿®å¤ä»£ç ç¼©è¿›ï¼Œç¡®ä¿æœ€å°çš„ç¼©è¿›è¢«ç§»é™¤ï¼Œä»¥é¿å…ç¼©è¿›é”™è¯¯
def fix_indentation(code):
    lines = code.splitlines()

    min_indent = min(len(line) - len(line.lstrip()) for line in lines if line.strip())
    fixed_lines = [line[min_indent:] if len(line) >= min_indent else line for line in lines]
    return "\n".join(fixed_lines)


def fix_invalid_backslashes(match):
    char = match.group(1)
    if char in '"\\/bfnrtu':  # JSON é‡Œåˆæ³•çš„è½¬ä¹‰å­—ç¬¦åªæœ‰è¿™äº›ï¼š " \ bfnrtu
        return '\\' + char  # åˆæ³•ä¿ç•™
    else:
        return '\\\\' + char  # éæ³•çš„è¡¥æˆ \\ + å­—ç¬¦


def extract_python_code(text):
    """
    æå– Markdown ä»£ç å—ä¸­çš„ Python ä»£ç ï¼ŒåŒæ—¶æ”¯æŒç¼©è¿›ä»£ç å—
    """
    code_blocks = re.findall(r'```(?:python)?(.*?)```', text, re.DOTALL)
    if not code_blocks:
        # æŸ¥æ‰¾ç¼©è¿›ä»£ç å—ï¼Œå³æ¯è¡Œå‰ 4 ä¸ªç©ºæ ¼çš„ä»£ç ï¼Œ æ—  ``` åŒ…å›´çš„ä»£ç å—
        code_blocks = re.findall(r'((?: {4}.*\n)+)', text)

    return [fix_indentation(block) for block in code_blocks]  # [block.strip()]


def extract_any_code(markdown_string: str):
    # Regex pattern to match Python code blocks,åŒ¹é… Pythonä¸å…¶ä»–ä»£ç å—
    pattern = r"```[\w\s]*python\n([\s\S]*?)```|```([\s\S]*?)```"
    # Find all matches in the markdown string
    matches = re.findall(pattern, markdown_string, re.IGNORECASE)
    # Extract the Python code from the matches
    code_blocks = []
    for match in matches:
        code = match[0] or match[1]  # å¦‚æœæ˜¯ Python ä»£ç å—ï¼Œå– ```python ä¹‹åçš„ä»£ç ,å¦‚æœæ˜¯å…¶ä»–ä»£ç å—ï¼Œå–ä»£ç å†…å®¹
        code_blocks.append(code.strip())

    return code_blocks


def remove_function_decorators(func):
    """
    è·å–å‡½æ•°æºç ï¼Œå»æ‰æ‰€æœ‰è£…é¥°å™¨
    """
    source = inspect.getsource(func)  # è·å–åŸå§‹ä»£ç 
    tree = ast.parse(source)  # è§£æ AST

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):  # æ‰¾åˆ°å‡½æ•°å®šä¹‰
            node.decorator_list = []  # æ¸…ç©ºè£…é¥°å™¨åˆ—è¡¨

    return ast.unparse(tree)  # é‡æ–°è½¬æ¢æˆä»£ç 
    # import textwrap
    # textwrap.dedent(source)  # å¤„ç†ç¼©è¿›


def extract_function_metadata(code, r=None):
    # ç”¨äºè§£æPythonå‡½æ•°å¹¶æå–å…ƒæ•°æ®,ä»æºä»£ç çº§åˆ«æå–è¯¦ç»†çš„ä»£ç ä¿¡æ¯
    # ä½¿ç”¨ASTæ¥è§£æPythonä»£ç ,å°† Python æºä»£ç ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰è½¬æ¢ä¸ºæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰
    tree = ast.parse(code)
    functions = []

    # éå†ASTèŠ‚ç‚¹ï¼Œæ‰¾åˆ°å‡½æ•°å®šä¹‰
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            function_name = node.name
            arguments = [arg.arg for arg in node.args.args]
            docstring = ast.get_docstring(node) or ""
            function_code = ast.unparse(node) if hasattr(ast, 'unparse') else ast.dump(node)
            # if generate_docstring and not docstring:
            #     docstring = generate_docstring(function_name, function_code)

            metadata = {
                'name': function_name,
                'args': arguments,
                'docstring': docstring,
                'code': function_code
            }
            functions.append(metadata)
            if r:
                key = f"function:{function_name}"  # ç”¨å‡½æ•°åä½œä¸ºRedisçš„é”®
                r.set(key, str(metadata))  # å­˜å‚¨ä¸ºJSONæ ¼å¼

    return functions


def extract_sql_code(text):
    sql_blocks = re.findall(r'```(?:sql)?(.*?)```', text, re.DOTALL)
    if not sql_blocks:
        sql_blocks = re.findall(r'((?:SELECT|INSERT|UPDATE|DELETE).*?;)', text, re.DOTALL)
    return [block.strip() for block in sql_blocks]


def extract_html_code(text):
    html_blocks = re.findall(r'```(?:html)?(.*?)```', text, re.DOTALL)
    if not html_blocks:
        html_blocks = re.findall(r'(<html.*?</html>)', text, re.DOTALL | re.IGNORECASE)
    return [block.strip() for block in html_blocks]


def extract_cpp_code(text):
    cpp_blocks = re.findall(r'```(?:cpp|c\+\+)?(.*?)```', text, re.DOTALL)
    return [block.strip() for block in cpp_blocks]


def extract_java_code(text):
    java_blocks = re.findall(r'```(?:java)?(.*?)```', text, re.DOTALL)
    return [block.strip() for block in java_blocks]


def extract_bash_code(text):
    bash_blocks = re.findall(r'```(?:bash|sh)?(.*?)```', text, re.DOTALL)
    return [block.strip() for block in bash_blocks]


def extract_table_code(text):
    # æå–æ•´ä¸ªè¡¨æ ¼å—
    table_blocks = re.findall(r'```(?:table)?(.*?)```', text, re.DOTALL)
    if not table_blocks:
        table_blocks = re.findall(r'((?:\|.*?\|)+)', text)  # ç®€å•åŒ¹é… Markdown è¡¨æ ¼ï¼Œå¦‚ | A | B |
    return [block.strip() for block in table_blocks]


def extract_table_data(text) -> list[str]:
    """æå– Markdown æ ¼å¼çš„è¡¨æ ¼æ•°æ®,è¿”å›æŒ‰è¡¨æ ¼è¡Œåˆ†ç»„çš„è¡¨æ ¼åˆ—è¡¨"""
    table_pattern = re.findall(r'(\|.*\|)', text)
    tables = []
    current_table = []

    for line in text.split("\n"):
        if line.strip() in table_pattern:
            current_table.append(line.strip())
        elif current_table:
            tables.append("\n".join(current_table))
            current_table = []

    if current_table:
        tables.append("\n".join(current_table))

    return tables


def extract_table_blocks(text) -> tuple[list[str], str]:
    """
    ä»é•¿æ–‡æœ¬ä¸­æå–æ‰€æœ‰è¿ç»­çš„â€œ|...|â€è¡¨æ ¼å—ï¼Œä½œä¸ºä¸€ä¸ªæ•´ä½“æ®µè½è¿”å›ï¼Œ
    å¹¶è¿”å›å»é™¤äº†è¿™äº›è¡¨æ ¼å—åçš„çº¯æ­£æ–‡ã€‚
    :param text: åŸå§‹å¤šè¡ŒåˆåŒæ–‡æœ¬
    :return: (table_blocks, remaining_text)
      - table_blocks: List[str]ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€æ®µå®Œæ•´çš„è¡¨æ ¼ï¼ˆå«å¤šè¡Œï¼‰
      - remaining_text: strï¼Œæ²¡æœ‰è¡¨æ ¼å—çš„æ­£æ–‡
    """
    # åŒ¹é…æ¨¡å¼ï¼šè¿ç»­å¤šè¡Œã€æ¯è¡Œä»¥ '|' å¼€å¤´å¹¶è‡³å°‘ä¸€ä¸ª '|' ç»“å°¾
    pattern = re.compile(r'(?:^\|[^\n]*\|\s*$\n?)+', re.MULTILINE)

    # æå–æ‰€æœ‰è¡¨æ ¼å—
    table_blocks = pattern.findall(text)
    remaining_text = pattern.sub('', text)

    return table_blocks, remaining_text


def extract_table_segments(raw_text) -> list[tuple[str, str]]:
    # ordered
    # 1. æ­£åˆ™åŒ¹é…è¿ç»­å¤šè¡Œâ€œ|â€¦|â€è¡¨æ ¼å—
    table_pattern = re.compile(r'(?:^\|[^\n]*\|\s*$\n?)+', re.MULTILINE)

    segments = []
    last_end = 0
    # 2. éå†æ‰€æœ‰è¡¨æ ¼å—
    for m in table_pattern.finditer(raw_text):
        start, end = m.span()
        # 2a. å…ˆæŠŠè¡¨æ ¼å—å‰é¢çš„æ­£æ–‡ç‰‡æ®µæ”¶é›†ä¸‹æ¥
        if start > last_end:
            text_segment = raw_text[last_end:start]
            segments.append(('text', text_segment))
        # 2b. å†æŠŠè¿™ä¸ªè¡¨æ ¼å—æœ¬èº«æ”¶é›†ä¸‹æ¥
        table_block = m.group()
        segments.append(('table', table_block))
        last_end = end

    # 2c. æœ€åæ”¶é›†è¡¨æ ¼å—åå‰©ä½™çš„æ­£æ–‡
    if last_end < len(raw_text):
        segments.append(('text', raw_text[last_end:]))

    return segments


def parse_table_block(block: str) -> list[list[str]]:
    """
    å°†ä¸€ä¸ªè¿ç»­çš„è¡¨æ ¼å—ï¼ˆå¤šè¡Œä»¥ | å¼€å¤´å’Œç»“å°¾ï¼‰æ‹†æˆè¡Œå’Œå­—æ®µåˆ—è¡¨ã€‚
    :param block: str, å½¢å¦‚:
      "| åˆ—A | åˆ—B |\n| --- | --- |\n| 1  | x   |\n| 2  | y   |\n"
    :return: List[List[str]]ï¼Œå¦‚ [["åˆ—A","åˆ—B"], ["1","x"], ["2","y"]]
    """
    rows = []
    for line in block.strip().split('\n'):
        line = line.strip()
        if not line or not line.startswith('|') or line.count('|') < 2:
            continue
        # æŒ‰ | åˆ†ï¼Œä¸¢æ‰ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªç©ºå­—ç¬¦ä¸²
        cells = [cell.strip() for cell in line.split('|')[1:-1]]
        rows.append(cells)
    return rows


def extract_web_content(html):
    # æå–<title>å†…å®¹
    title_match = re.search(r"<title.*?>(.*?)</title>", html, re.IGNORECASE | re.DOTALL)
    title = title_match.group(1).strip() if title_match else ""

    # æå–<body>å†…å®¹ï¼Œå»é™¤è„šæœ¬ã€æ ·å¼ç­‰æ ‡ç­¾
    body_match = re.search(r"<body.*?>(.*?)</body>", html, re.IGNORECASE | re.DOTALL)
    body_content = body_match.group(1).strip() if body_match else ""

    # ç§»é™¤<script>å’Œ<style>æ ‡ç­¾åŠå…¶å†…å®¹
    body_content = re.sub(r"<(script|style).*?>.*?</\1>", "", body_content, flags=re.IGNORECASE | re.DOTALL)

    # ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾ï¼Œåªä¿ç•™æ–‡æœ¬
    text_content = re.sub(r"<[^>]+>", "", body_content)
    text_content = re.sub(r"\s+", " ", text_content).strip()

    return {"title": title, "content": text_content}


def extract_yaml_data(text):
    """æå– Markdown ä¸­çš„ YAML æ•°æ®"""
    yaml_blocks = re.findall(r'```yaml\n(.*?)\n```', text, re.DOTALL)
    parsed_data = []

    for block in yaml_blocks:
        try:
            parsed_data.append(yaml.safe_load(block))  # è§£æ YAML
        except yaml.YAMLError:
            parsed_data.append(None)  # è§£æå¤±è´¥åˆ™è¿”å› None

    return parsed_data


def extract_list_data(text):
    list_blocks = re.findall(r'```(?:list)?(.*?)```', text, re.DOTALL)
    if not list_blocks:
        list_blocks = re.findall(r'(\n\s*[-*].*?(\n\s{2,}.*?)*\n)', text)  # çº¯æ–‡æœ¬åˆ—è¡¨
    return [block.strip() for block in list_blocks]


def extract_json_data(text):
    # æå– JSON æ ¼å¼çš„ä»£ç å—
    json_blocks = re.findall(r'```(?:json)?(.*?)```', text, re.DOTALL)
    return [block.strip() for block in json_blocks]


def extract_code_blocks(text, lag='python', **kwargs):
    # ä»æ–‡æœ¬ä¸­æå–ç‰¹å®šæ ¼å¼çš„ä»£ç å—ï¼Œæ”¯æŒä¸åŒçš„ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ Pythonã€SQLã€HTML ç­‰ï¼‰ä»¥åŠè¡¨æ ¼ã€JSONã€åˆ—è¡¨ç­‰æ•°æ®ç±»å‹
    funcs = {
        "sql": extract_sql_code,
        "html": extract_html_code,
        "python": extract_python_code,
        "cpp": extract_cpp_code,
        "java": extract_java_code,
        "bash": extract_bash_code,
        "code": extract_any_code,
        "method": extract_method_calls,

        "table": extract_table_code,
        "yaml": extract_yaml_data,
        "list": extract_list_data,
        "json": extract_json_data,
    }
    if lag in funcs:
        return funcs[lag](text)

    # æå– ``` åŒ…è£¹çš„ä»£ç å—
    code_blocks = re.findall(r'```(\w+)?\n(.*?)```', text, re.DOTALL)  # r'```(.*?)```'
    if lag:
        code_blocks = [block for block in code_blocks if block.lstrip().startswith(lag)]
        return code_blocks  # è¿‡æ»¤å‡ºæŒ‡å®šè¯­è¨€çš„ä»£ç å—

    # try:
    #     for k, f in funcs.items():
    #         print(k,f(text))
    # except Exception as e:
    #     print(k,e)

    return {k: f(text) for k, f in funcs.items()}


def clean_json_string(json_str):
    # 1. å»é™¤ // æ³¨é‡Š
    json_str = re.sub(r'//.*', '', json_str)
    # 2. ä¿®å¤éæ³•åæ–œæ ï¼šæŠŠéæ³•çš„ \x è½¬ä¸º x
    json_str = re.sub(r'\\(.)', fix_invalid_backslashes, json_str)
    # 3. æ›¿æ¢ HTML æ ‡ç­¾ã€ä¼ªæ ‡ç­¾ã€éæ³•æ¢è¡Œç¬¦
    json_str = json_str.replace('<br>', '\n')
    json_str = json_str.replace('\\"', '"')
    json_str = json_str.replace('<', 'ã€Š').replace('>', 'ã€‹')  # ä¿®å¤ <ucam.xxx> é€ æˆçš„é”™è¯¯
    return json_str


def extract_json_from_string(input_str):
    # ä»ä¸€ä¸ªæ™®é€šå­—ç¬¦ä¸²ä¸­æå– JSON ç»“æ„ï¼Œä½†å¯èƒ½ä¸å¤„ç†åµŒå¥—çš„ JSON
    match = re.search(r'\{.*}', input_str, re.DOTALL)
    if match:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e1:
            json_str = clean_json_string(json_str)
            try:
                return json.loads(json_str)
            except json.JSONDecodeError as e2:
                print(f"Error decoding JSON: {e2},{input_str}")
    return None


def extract_jsons(input_str):
    """
    å¤„ç†åŒ…å«å¤šä¸ª JSON å¯¹è±¡çš„æ–‡æœ¬æ•°æ®,æˆåŠŸè§£æäº† JSON å¯¹è±¡ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰è§£æç»“æœçš„åˆ—è¡¨
    :param input_str:
    :return: list[dict]
    """
    # 1,None,-1
    matches = re.findall(r'\{.*?\}', input_str, re.DOTALL)  # regex.findall(r'\{(?:[^{}]|(?R))*\}', input_str)
    if not matches:
        return None
    json_objects = []  # [var.strip() for var in matches if '{' not in var]
    for match in matches:
        try:
            json_objects.append(json.loads(match))
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e} - Skipping this fragment: {match}")

    return json_objects


def extract_headers(text):
    # æå– ## æˆ– ### ç­‰æ ‡é¢˜
    headers = re.findall(r'^(#{1,6})\s+(.*)', text, re.MULTILINE)
    return [{'level': len(header[0]), 'text': header[1]} for header in headers]


def extract_links(text):
    # æå– Markdown æ ¼å¼çš„é“¾æ¥ [é“¾æ¥æ–‡å­—](é“¾æ¥åœ°å€)
    links = re.findall(r'\[([^\]]+)\]\((https?:\/\/[^\s]+)\)', text)
    return [{'text': link[0], 'url': link[1]} for link in links]


def extract_bold(text):
    # æå– Markdown æ ¼å¼çš„ **ç²—ä½“**
    bold_texts = re.findall(r'\*\*(.*?)\*\*', text)
    return bold_texts


def extract_italic(text):
    # æå– Markdown æ ¼å¼çš„ __æ–œä½“__ æˆ– *æ–œä½“*
    italic_texts = re.findall(r'__(.*?)__|\*(.*?)\*', text)
    return [italic[0] or italic[1] for italic in italic_texts]  # å¤„ç†ä¸¤ä¸ªæ•è·ç»„


def extract_date_strings(text):
    """
    ä»æ–‡æœ¬ä¸­æå–ç¬¦åˆæ ¼å¼çš„æ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²ã€‚

    Args:
        text (str): è¾“å…¥æ–‡æœ¬

    Returns:
        list: æå–å‡ºçš„æ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²åˆ—è¡¨
    """
    date_regex = r'\b\d{4}[-/]\d{2}[-/]\d{2}(?:\s\d{2}:\d{2}:\d{2})?\b'
    return re.findall(date_regex, text)


def extract_tagged_content(text, tag="answer"):
    """
    æå–æŒ‡å®šæ ‡ç­¾æœ€åä¸€ä¸ªåŒ¹é…
    Extracts the value from the last occurrence of a specified tag in the text.

    Args:
        text (str): The input text containing the tagged content.
        tag (str): The tag to extract content from (default is 'answer').

    Returns:
        str or None: The extracted content, or None if no valid content is found.
    """
    pattern = rf"<{tag}>(.*?)</{tag}>"  # æ­£åˆ™åŒ¹é… <tag>...</tag>
    matches = re.findall(pattern, text, re.DOTALL)  # è·å–æ‰€æœ‰åŒ¹é…é¡¹"<answer> </answer>""

    if matches:
        last_match = matches[-1].strip()  # è·å–æœ€åä¸€ä¸ªåŒ¹é…çš„å†…å®¹å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
        return None if last_match == "..." else last_match
    return None


# reasoning
def extract_tagged_split(text, tag="think"):
    """
    Splits the text into two parts: the content inside the specified tag
    and the remaining text outside the tag.

    Args:
        text (str): The input text containing the tagged content.
        tag (str): The tag to extract content from (default is 'think',reasoning).

    Returns:
        list: A list containing [tag_content, remaining_text].
    """
    pattern = re.compile(rf"<{tag}>(.*?)</{tag}>\s*(.*)", re.DOTALL)
    match = pattern.search(text)

    if match:
        think_content = match.group(1).strip()  # æå– <think> å†…çš„å†…å®¹,
        output_content = match.group(2).strip()  # æå–æœ€ç»ˆè¾“å‡ºå†…å®¹
        return [think_content, output_content]

    return [None, text]


def process_assistant_think(content):
    if '<think>' in content and '</think>' in content:
        content = re.sub(r'(<think>)(.*?)(</think>)',
                         r'<details style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†å†…å®¹ï¼ˆå±•å¼€ï¼‰</summary>\2</details>',
                         content,
                         flags=re.DOTALL)

    if '<think>' in content and '</think>' not in content:
        content = re.sub(r'<think>(.*?)$',
                         r'<details open style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†ä¸­...</summary>\1</details>',
                         content,
                         flags=re.DOTALL)

    if '<think>' not in content and '</think>' in content:
        content = re.sub(r'(.*?)</think>',
                         r'<details style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†å†…å®¹ï¼ˆå±•å¼€ï¼‰</summary>\1</details>',
                         content,
                         flags=re.DOTALL)

    return content


def ordinal_generator():
    ordinals = ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©']
    for ordinal in ordinals:
        yield ordinal


def remove_markdown(text):
    # å»é™¤ Markdown çš„å¸¸è§æ ‡è®°
    """
    **ç²—ä½“æ–‡æœ¬**
    _æ–œä½“æ–‡æœ¬_
    ![å›¾ç‰‡æè¿°](image_url)
    [é“¾æ¥æ–‡æœ¬](url)
    ### æ ‡é¢˜æ–‡æœ¬
    > å¼•ç”¨å—
    * æ— åºåˆ—è¡¨é¡¹
    1. æœ‰åºåˆ—è¡¨é¡¹
    ~~åˆ é™¤çº¿æ–‡æœ¬~~
    __ä¸‹åˆ’çº¿æ–‡æœ¬__
    """
    text = re.sub(r'(`{1,3})(.*?)\1', r'\2', text)  # å»é™¤åå¼•å·ä»£ç å—
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # å»é™¤ç²—ä½“
    text = re.sub(r'\*(.*?)\*', r'\1', text)  # å»é™¤æ–œä½“
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)  # å»é™¤å›¾ç‰‡
    text = re.sub(r'\[.*?\]\((.*?)\)', r'\1', text)  # å»é™¤é“¾æ¥ï¼Œä½†ä¿ç•™ URL
    text = re.sub(r'#{1,6}\s*(.*)', r'\1', text)  # å»é™¤æ ‡é¢˜
    text = re.sub(r'>\s*(.*)', r'\1', text)  # å»é™¤å¼•ç”¨å—
    text = re.sub(r'(\*|-|\+)\s+(.*)', r'\2', text)  # å»é™¤æ— åºåˆ—è¡¨ç¬¦å·
    text = re.sub(r'\d+\.\s+(.*)', r'\1', text)  # å»é™¤æœ‰åºåˆ—è¡¨ç¬¦å·
    text = re.sub(r'~~(.*?)~~', r'\1', text)  # å»é™¤åˆ é™¤çº¿
    text = re.sub(r'_{2}(.*?)_{2}', r'\1', text)  # å»é™¤ä¸‹åˆ’çº¿æ ‡è®°
    text = re.sub(r'\[(.*?)\]\((.*?)\)', r'\1', text)  # å»é™¤é“¾æ¥å’Œ URL
    text = re.sub(r'\n{2,}', '\n', text)  # å°†å¤šä½™çš„ç©ºè¡Œæ›¿æ¢ä¸ºå•ä¸ªæ¢è¡Œç¬¦
    return text.strip()


def format_for_wechat(text):
    formatted_text = extract_tagged_split(text, tag="think")[1]  # text
    formatted_text = re.sub(r'\*\*(.*?)\*\*', r'âœ¦\1âœ¦', formatted_text)  # **ç²—ä½“** è½¬æ¢ä¸º âœ¦ç²—ä½“âœ¦æ ·å¼
    formatted_text = re.sub(r'!!(.*?)!!', r'â—\1â—', formatted_text)  # !!é«˜äº®!! è½¬æ¢ä¸º â—ç¬¦å·åŒ…å›´
    # formatted_text = re.sub(r'__(.*?)__', r'â€»\1â€»', formatted_text)  # __æ–œä½“__ è½¬æ¢ä¸ºæ˜Ÿå·åŒ…å›´çš„æ ·å¼
    formatted_text = re.sub(r'~~(.*?)~~', r'_\1_', formatted_text)  # ~~ä¸‹åˆ’çº¿~~ è½¬æ¢ä¸ºä¸‹åˆ’çº¿åŒ…å›´
    formatted_text = re.sub(r'\^\^(.*?)\^\^', r'||\1||', formatted_text)  # ^^é‡è¦^^ è½¬æ¢ä¸º ||é‡è¦|| åŒ…å›´
    formatted_text = re.sub(r'######\s+(.*?)(\n|$)', r'[\1]\n', formatted_text)  # ###### å…­çº§æ ‡é¢˜
    formatted_text = re.sub(r'#####\s+(.*?)(\n|$)', r'ã€Š\1ã€‹\n', formatted_text)  # ##### äº”çº§æ ‡é¢˜
    formatted_text = re.sub(r'####\s+(.*?)(\n|$)', r'ã€\1ã€‘\n', formatted_text)  # #### æ ‡é¢˜è½¬æ¢
    formatted_text = re.sub(r'###\s+(.*?)(\n|$)', r'â€” \1 â€”\n', formatted_text)  # ### ä¸‰çº§æ ‡é¢˜
    formatted_text = re.sub(r'##\s+(.*?)(\n|$)', r'â€”â€” \1 â€”â€”\n', formatted_text)  # ## äºŒçº§æ ‡é¢˜
    formatted_text = re.sub(r'#\s+(.*?)(\n|$)', r'â€» \1 â€»\n', formatted_text)  # # ä¸€çº§æ ‡é¢˜
    # formatted_text = re.sub(r'```([^`]+)```',
    #                         lambda m: '\n'.join([f'ï½œ {line}' for line in m.group(1).splitlines()]) + '\n',
    #                         formatted_text)
    # formatted_text = re.sub(r'`([^`]+)`', r'ã€Œ\1ã€', formatted_text)  # `ä»£ç ` è½¬æ¢ä¸ºã€Œä»£ç ã€æ ·å¼
    # formatted_text = re.sub(r'>\s?(.*)', r'ğŸ’¬ \1', formatted_text)  # > å¼•ç”¨æ–‡æœ¬ï¼Œè½¬æ¢ä¸ºèŠå¤©ç¬¦å·åŒ…å›´
    # formatted_text = re.sub(r'^\s*[-*+]\s+', 'â€¢ ', formatted_text, flags=re.MULTILINE)  # æ— åºåˆ—è¡¨é¡¹
    # formatted_text = re.sub(r'^\s*\d+\.\s+',f"{next(ordinal_iter)} ", formatted_text, flags=re.MULTILINE)  # æœ‰åºåˆ—è¡¨é¡¹
    formatted_text = re.sub(r'\n{2,}', '\n\n', formatted_text)  # è½¬æ¢æ¢è¡Œä»¥é¿å…å¤šä½™ç©ºè¡Œ

    return formatted_text.strip()


def df_to_markdown(df, index=False):
    # df.fillna('N/A').to_markdown()
    headers = df.columns.tolist()
    md = "| " + " | ".join(headers) + " |\n"
    md += "| " + " | ".join(["---"] * len(headers)) + " |\n"
    for _, row in df.iterrows():
        md += "| " + " | ".join(row.astype(str)) + " |\n"

    return md if index else md.replace("| Index |", "|")  # å¯é€‰ç§»é™¤ç´¢å¼•åˆ—


def format_for_html(text):
    # Markdown æ ¼å¼çš„æ–‡æœ¬è½¬æ¢ä¸º HTML çš„å­—ç¬¦ä¸²,æ¸²æŸ“ Markdown æ–‡ç« 
    import markdown
    return markdown.markdown(text)
    # from IPython.display import Markdown, display
    # display(Markdown(f"`{export_command}`"))


def extract_string(text, extract, **kwargs):
    if not extract:
        return None
    funcs = {
        "json": extract_json_from_string,
        "jsons": extract_jsons,
        "header": extract_headers,
        "links": extract_links,
        "bold": extract_bold,
        "italic": extract_italic,
        "tables": extract_table_data,
        "table_segments": extract_table_segments,
        "date": extract_date_strings,
        "answer": extract_tagged_content,
        "think": partial(extract_tagged_split, tag="think"),
        "reasoning": partial(extract_tagged_split, tag="reasoning"),
        'sentence': split_sentences,
        'sentences_clean': split_sentences_clean,
        "wechat": format_for_wechat,
        'remark': remove_markdown,
        "html": format_for_html,
        "web": extract_web_content,
    }
    try:
        if extract in funcs:
            return funcs[extract](text, **kwargs)

        extract_type = extract.split('.')
        if extract_type[0] == 'code':
            transform = extract_code_blocks(text, lag=extract_type[1] if len(extract_type) > 1 else '', **kwargs)
            return transform

        return {k: f(text, **kwargs) for k, f in funcs.items()}  # "type": "all"
    except Exception as e:
        print(e)

    return None


# class Partial:
#     def __init__(self, func, *args, **kwargs):
#         self.func = func
#         self.args = args
#         self.kwargs = kwargs
#
#     def __call__(self, *more_args, **more_kwargs):
#         # åˆå¹¶å›ºå®šå‚æ•°å’Œæ–°å‚æ•°
#         all_args = self.args + more_args
#         all_kwargs = {**self.kwargs, **more_kwargs}
#         return self.func(*all_args, **all_kwargs)

def dict_to_xml(tag, d):
    """å°†å­—å…¸è½¬æ¢ä¸º XML å­—ç¬¦ä¸²"""
    elem = ET.Element(tag)
    for key, val in d.items():
        child = ET.SubElement(elem, key)
        if isinstance(val, list):
            for item in val:
                item_elem = ET.SubElement(child, "item")
                item_elem.text = str(item)
        else:
            child.text = str(val)
    return ET.tostring(elem, encoding='unicode')


def list_to_xml(tag, lst):
    """å°†åˆ—è¡¨è½¬æ¢ä¸º XML å­—ç¬¦ä¸²"""
    elem = ET.Element(tag)
    for item in lst:
        item_elem = ET.SubElement(elem, "item")
        item_elem.text = str(item)
    return ET.tostring(elem, encoding='unicode')


def df2doc(data, use_index=True):
    """
    å°† DataFrame ä¸­æ¯ä¸€è¡Œè½¬æ¢ä¸ºä¸€æ®µæ–‡æœ¬
    :param data: è¾“å…¥ DataFrame
    :param use_index: æ˜¯å¦åœ¨æ–‡æœ¬å‰å¢åŠ è¡Œç´¢å¼•
    :return: æ–‡æœ¬è®°å½•åˆ—è¡¨
    """
    docs = []
    try:
        import pandas as pd
        if use_index:
            for item in zip(data.index, data.to_dict(orient='records')):
                docs.append(f'{item[0]}\t' + '|'.join(
                    f'{k}#{v.strip() if isinstance(v, str) else v}' for k, v in item[1].items() if pd.notna(v)).strip())
        else:
            for item in data.to_dict(orient='records'):
                docs.append('|'.join(
                    f'{k}#{v.strip() if isinstance(v, str) else v}' for k, v in item.items() if pd.notna(v)).strip())
    except ImportError:
        if not isinstance(data, list) or not all(isinstance(d, dict) for d in data):
            raise ValueError("è¾“å…¥æ•°æ®åº”ä¸ºåˆ—è¡¨çš„å­—å…¸æ ¼å¼ï¼Œä¾‹å¦‚ [{'key1': 'value1', 'key2': 'value2'}, ...]")

        for idx, record in enumerate(data):  # data.iterrows()
            # æ‹¼æ¥æ¯ä¸ªå­—æ®µï¼Œè·³è¿‡ None å€¼ï¼Œå¹¶å¯¹å­—ç¬¦ä¸²åš strip å¤„ç†
            doc_line = '|'.join(
                f"{k}#{v.strip() if isinstance(v, str) else v}"
                for k, v in record.items() if v is not None
            )
            # å¦‚æœ use_index=Trueï¼Œåˆ™åœ¨å‰é¢åŠ ä¸Šç´¢å¼•
            if use_index:
                doc_line = f"{idx}\t" + doc_line

            docs.append(doc_line)
    except Exception as e:
        print(e)

    return docs


def df2doc_batch(data, batch_size=5):
    """
    å°† DataFrame æˆ–åˆ—è¡¨æ•°æ®æŒ‰ batch_size åˆ†æ‰¹ï¼Œyield æ¯ä¸ªæ‰¹æ¬¡çš„è®°å½•ï¼ˆåˆ—è¡¨ of dictsï¼‰ã€‚
    """
    try:
        import pandas as pd
        if isinstance(data, pd.DataFrame):
            data = data.to_dict(orient='records')
    except ImportError:
        if not isinstance(data, list) or not all(isinstance(d, dict) for d in data):
            raise ValueError("è¾“å…¥æ•°æ®åº”ä¸ºåˆ—è¡¨çš„å­—å…¸æ ¼å¼ï¼Œä¾‹å¦‚ [{'key1': 'value1', 'key2': 'value2'}, ...]")
    except Exception as e:
        print(e)

    batch = []
    for i, item in enumerate(data):
        batch.append(item)
        # æ¯ batch_size ç»„ä¸€ä¸ª batch
        if (i + 1) % batch_size == 0 or i == len(data) - 1:
            yield batch
            batch = []


def get_last_entries_records(records: list[dict], fields, use_index=False, max_tokens: int = 8000, tokenizer=None):
    texts = []
    total_chars = 0
    # ä»æœ€æ–°è®°å½•å¼€å§‹æ‹¼æ¥ï¼Œç›´åˆ°æ€»å­—ç¬¦æ•°è¶…è¿‡ max_tokens æ—¶åœæ­¢æ·»åŠ ï¼ˆè¿”å›æœ€åä¸è¶³ max_chars å­—ç¬¦çš„éƒ¨åˆ†ï¼‰
    for idx, record in enumerate(records):
        use_fields = fields or list(record.keys())
        prefix = f"{idx}\t" if use_index else ""
        item_str = prefix + '|'.join(
            f"{k}#{(str(record[k]).strip() if isinstance(record[k], str) else record[k])}"
            for k in use_fields if record.get(k) is not None
        )
        entry_length = lang_token_size(item_str, tokenizer)  # len(item_str)
        if total_chars + entry_length > max_tokens:
            break
        texts.append(item_str)
        total_chars += entry_length

    # å¦‚æœæœ‰å¤šä¸ªè®°å½•ï¼Œå€’åºæ‹¼æ¥ï¼ˆä¿è¯æœ€æ—©çš„è®°å½•åœ¨æœ€å‰é¢ï¼‰
    return list(reversed(texts))  # "\n\n".join(reversed(texts))


def get_max_items_from_list(data: list, max_tokens: int = 4000, tokenizer=None):
    """
        Get max items from list of items based on defined max tokens (based on openai compute)
        æ ¹æ®ç»™å®šçš„æœ€å¤§ token æ•°ï¼Œä»ä¸€ç»„å­—å…¸æ•°æ®ä¸­é€‰å–é€‚åˆçš„é¡¹ç›®ï¼Œç›´åˆ°è¾¾åˆ° token é™åˆ¶ä¸ºæ­¢
        :param data: åŒ…å«å­—å…¸çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸è¡¨ç¤ºä¸€ä¸ªé¡¹ç›®
        :param max_tokens: å…è®¸çš„æœ€å¤§ token æ•°
        :param tokenizer: å¯é€‰çš„ tokenizerï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼Œåˆ™æ ¹æ®è¯­è¨€è‡ªåŠ¨å¤„ç†ï¼‰
        :return: é€‚åˆçš„é¡¹ç›®åˆ—è¡¨
        List[Dict[str, str]]
    """
    result = []
    current_tokens = 0
    # encoding = tiktoken.encoding_for_model(encoding_name)
    # tiktoken.get_encoding("cl100k_base")
    for item in data:
        item_str = json.dumps(item)
        item_tokens = lang_token_size(item_str, tokenizer)
        if current_tokens + item_tokens > max_tokens:
            break

        result.append(item)
        current_tokens += item_tokens

    return result


def find_similar_word(target_keyword, tokens):
    max_ratio = 0
    similar_word_index = -1
    for i, token in enumerate(tokens):
        ratio = SequenceMatcher(None, target_keyword, token).ratio()
        if ratio > max_ratio:
            max_ratio = ratio
            similar_word_index = i
    return similar_word_index


def find_similar_words(query, tokens, top_n=3):
    matches = get_close_matches(query, tokens, n=top_n)
    # è®¡ç®—æ¯ä¸ªåŒ¹é…é¡¹ä¸æŸ¥è¯¢è¯çš„ç›¸ä¼¼åº¦
    results = []
    for match in matches:
        matcher = SequenceMatcher(None, query, match)
        results.append((match, matcher.ratio(), tokens.index(match)))

    return results


def contains_chinese(text):
    # æ£€æµ‹å­—ç¬¦ä¸²ä¸­æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
    chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
    return bool(chinese_pattern.search(text))
    # return detect(text)=='zh-cn'


def contains_hebrew_arabic(text):
    return bool(re.search(r'[\u0590-\u05FF\u0600-\u06FF]', text))


def contains_cjk(text):
    """æ£€æµ‹æ˜¯å¦åŒ…å« CJKï¼ˆä¸­æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ï¼‰å­—ç¬¦"""
    return bool(re.search(r'[\u4e00-\u9fff\u3040-\u30ff\uac00-\ud7af]', text))


def convert_to_pinyin(text):
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºä¸­å›½åŸå¸‚åç§°ï¼ˆä»…ä¸­æ–‡ï¼‰ï¼Œç„¶åè½¬æ¢ä¸ºæ‹¼éŸ³
    if all('\u4e00' <= char <= '\u9fff' for char in text):
        return ''.join(lazy_pinyin(text))
    return text


def lang_detect_to_trans(text):
    t = detect(text)
    if t == 'zh-cn':
        t = 'zh'
    if t == 'no':
        t = 'zh' if contains_chinese(text) else 'auto'
    return t


def lang_token_size(text, tokenizer=None):
    """è®¡ç®—æ¯æ®µæ–‡æœ¬çš„tokené•¿åº¦ï¼Œå¦‚æœæ²¡æœ‰æä¾›tokenizeråˆ™è¿”å›å­—ç¬¦é•¿åº¦"""
    if tokenizer:
        return len(tokenizer.encode(text))
    # ä¸­æ–‡å¹³å‡1å­—â‰ˆ1 tokenï¼Œè‹±æ–‡â‰ˆ4å­—=1 tokenï¼Œç²—ç•¥ä¼°ç®—
    lang = detect(text)
    if lang in ('en', 'fr', 'es', 'de'):
        # å¯¹äºè‹±æ–‡ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­ã€å¾·è¯­ç­‰ï¼Œä½¿ç”¨ç©ºæ ¼åˆ†è¯
        return len(text.split())
        # len(text) // 3

    # 'zh-cn','zh-hk','zh-tw','ja','ar',ç®€ä½“ä¸­æ–‡,æ—¥è¯­,é˜¿æ‹‰ä¼¯è¯­ï¼Œè¿”å›å­—ç¬¦æ•°
    return len(text)


def cut_text(text, tokenizer=None):
    # å»é™¤æ ‡ç‚¹/æ•°å­—/ç©ºæ ¼
    text = re.sub(r'[^\u4e00-\u9fa5]', '', str(text))
    if tokenizer:
        token_ids = tokenizer.encode(text)
        tokens = [tokenizer.decode([tid]) for tid in token_ids]  # tokenizer.tokenize
    else:
        tokens = jieba.lcut(text, cut_all=False)
    return tokens  # ' '.join(tokens)


def get_max_tokens_from_string(text: str, max_tokens: int, tokenizer) -> str:
    """
        Extract max tokens from string using the specified encoding (based on openai compute)
        ä»ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­æå–å‡ºç¬¦åˆæœ€å¤§ token æ•°é™åˆ¶çš„éƒ¨åˆ†
    """
    # from transformers import AutoTokenizer
    # encoding = AutoTokenizer.from_pretrained(model_id)
    # encoding = tiktoken.encoding_for_model(encoding_name)  # tiktoken.model.MODEL_TO_ENCODING
    tokens = tokenizer.encode(text)
    token_bytes = [tokenizer.decode_single_token_bytes(token) for token in tokens[:max_tokens]]
    return b"".join(token_bytes).decode()


def build_prompt(messages: list, use_role=False) -> str:
    """
    Build a single prompt string from a list of messages.
    Each message is expected to be a dictionary with 'role' and 'content' keys.
    This function concatenates all message contents, preserving the training format.
    """
    if use_role:
        # OpenAI-style messages are transformed to a structured conversation format for Ollama.
        return "\n".join(
            f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content'].strip()}"
            for msg in messages)
    return "\n".join([msg["content"].strip() for msg in messages])


def alternate_chat_history(messages: list):
    # ç¡®ä¿ user å’Œ assistant æ¶ˆæ¯äº¤æ›¿å‡ºç°ï¼Œæ’å…¥é»˜è®¤æ¶ˆæ¯æˆ–åˆ é™¤å¤šä½™æ¶ˆæ¯
    i = 0
    while i < len(messages) - 1:
        # if (
        #     isinstance(message, dict) and
        #     message.get("role") in ["user", "assistant"] and
        #     isinstance(message.get("content"), str) and
        #     message["content"].strip()  # ç¡®ä¿ content éç©º
        # ):
        message = messages[i]
        next_message = messages[i + 1]
        # å¤„ç†è¿ç»­ç›¸åŒè§’è‰²çš„æƒ…å†µ
        if message['role'] == next_message['role']:  # messages.insert(0, messages.pop(i))
            if i % 2 == 0:
                if message['role'] == 'user':
                    messages.insert(i + 1, {'role': 'assistant', 'content': 'è¿™æ˜¯ä¸€ä¸ªé»˜è®¤çš„å›ç­”ã€‚'})
                else:
                    messages.insert(i + 1, {'role': 'user', 'content': 'è¯·é—®æ‚¨æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ'})
            else:
                del messages[i + 1]
                i -= 1
        i += 1
    return messages


def split_whitespace_nonwhitespace(s, max_len=5):
    # æŒ‰ç…§ ç©ºç™½/éç©ºç™½ äº¤æ›¿æ‹†åˆ†å­—ç¬¦ä¸²ï¼Œæ§åˆ¶æ¯æ®µçš„æœ€å¤§é•¿åº¦ï¼Œé¢„åˆ‡å‰²
    for k, g in groupby(s, key=str.isspace):
        chunk = list(g)
        for i in range(0, len(chunk), max_len):
            yield ''.join(chunk[i:i + max_len])


LINE_STOP_FLAG = (
    '.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', ')', 'ï¼‰', '"', 'â€', ':', 'ï¼š', ';', 'ï¼›', ']', 'ã€‘', '}', '}', '>', 'ã€‹', 'ã€', ',', 'ï¼Œ',
    '-',
    'â€”', 'â€“',)
LINE_START_FLAG = ('(', 'ï¼ˆ', '"', 'â€œ', 'ã€', '{', 'ã€Š', '<', 'ã€Œ', 'ã€', 'ã€', '[',)


def find_last_punctuation(text, punctuations=("ã€‚", "ï¼Ÿ", "ï¼", "ï¼›", "ï¼š")):
    """æ‰¾åˆ°æ–‡æœ¬ä¸­æœ€åä¸€ä¸ªæœ‰æ•ˆçš„æ ‡ç‚¹ç¬¦å·ä½ç½®"""
    return max(text.rfind(p) for p in punctuations)


def is_punctuation_or_emoji(char):
    """æ£€æŸ¥å­—ç¬¦æ˜¯å¦ä¸ºç©ºæ ¼ã€æŒ‡å®šæ ‡ç‚¹æˆ–è¡¨æƒ…ç¬¦å·"""
    # å®šä¹‰éœ€è¦å»é™¤çš„ä¸­è‹±æ–‡æ ‡ç‚¹ï¼ˆåŒ…æ‹¬å…¨è§’/åŠè§’ï¼‰
    punctuation_set = {
        'ï¼Œ', ',',  # ä¸­æ–‡é€—å· + è‹±æ–‡é€—å·
        'ã€‚', '.',  # ä¸­æ–‡å¥å· + è‹±æ–‡å¥å·
        'ï¼', '!',  # ä¸­æ–‡æ„Ÿå¹å· + è‹±æ–‡æ„Ÿå¹å·
        '-', 'ï¼',  # è‹±æ–‡è¿å­—ç¬¦ + ä¸­æ–‡å…¨è§’æ¨ªçº¿
        'ã€'  # ä¸­æ–‡é¡¿å·
    }
    if char.isspace() or char in punctuation_set:
        return True
    # æ£€æŸ¥è¡¨æƒ…ç¬¦å·ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
    code_point = ord(char)
    emoji_ranges = [
        (0x1F600, 0x1F64F), (0x1F300, 0x1F5FF),
        (0x1F680, 0x1F6FF), (0x1F900, 0x1F9FF),
        (0x1FA70, 0x1FAFF), (0x2600, 0x26FF),
        (0x2700, 0x27BF)
    ]
    return any(start <= code_point <= end for start, end in emoji_ranges)


def get_string_no_punctuation_or_emoji(s):
    """å»é™¤å­—ç¬¦ä¸²é¦–å°¾çš„ç©ºæ ¼ã€æ ‡ç‚¹ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·,åªæ¸…ç†é¦–å°¾ï¼Œä¸å½±å“ä¸­é—´çš„å†…å®¹"""
    chars = list(s)
    # å¤„ç†å¼€å¤´çš„å­—ç¬¦
    start = 0
    while start < len(chars) and is_punctuation_or_emoji(chars[start]):
        start += 1
    # å¤„ç†ç»“å°¾çš„å­—ç¬¦
    end = len(chars) - 1
    while end >= start and is_punctuation_or_emoji(chars[end]):
        end -= 1
    return ''.join(chars[start:end + 1])


LLM_Abort_Event = asyncio.Event()  # threading.Event() çº¿ç¨‹å®‰å…¨


async def llm_abort_stop():
    LLM_Abort_Event.set()  # è§¦å‘ç»ˆæ­¢,æ˜¯å¦æå‰ç»ˆæ­¢


async def process_llm_stream(llm_responses_stream, token_size=20):
    """
    å¤„ç†å¤§æ¨¡å‹è¿”å›çš„æ–‡æœ¬æµï¼Œå¹¶æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²äº¤ç»™ TTS æœ—è¯»ã€‚
    :param llm_responses_stream: å¤§æ¨¡å‹è¿”å›çš„æ–‡æœ¬æµ
    :param token_size: æ ‡ç‚¹ä¸è¶³æ—¶ï¼Œå…è®¸çš„æœ€å°ç¼“å†²åŒºé•¿åº¦
    """
    response_message = []
    text_index = 0
    processed_chars = 0
    async for content in llm_responses_stream:
        response_message.append(content)
        if LLM_Abort_Event.is_set():  # å®æ—¶æ£€æŸ¥æ˜¯å¦ç»ˆæ­¢
            break

        # è·å–å½“å‰æœªå¤„ç†çš„æ–‡æœ¬
        full_text = "".join(response_message)
        current_text = full_text[processed_chars:]

        # æŸ¥æ‰¾æœ€åä¸€ä¸ªæœ‰æ•ˆæ ‡ç‚¹
        last_punct_pos = find_last_punctuation(current_text)
        if last_punct_pos != -1 or lang_token_size(current_text) > token_size:
            split_pos = last_punct_pos if last_punct_pos != -1 else token_size  # é€‰å–æœ€åˆé€‚çš„åˆ‡å‰²ç‚¹
            segment_text_raw = current_text[:split_pos + 1]
            segment_text = get_string_no_punctuation_or_emoji(segment_text_raw)  # å¤„ç†æ— æ•ˆå­—ç¬¦
            if segment_text:
                text_index += 1
                yield segment_text, text_index
                processed_chars += len(segment_text_raw)  # æ›´æ–°å·²å¤„ç†å­—ç¬¦ä½ç½®

    # å¤„ç†å‰©ä½™æœªåˆ†å‰²çš„æ–‡æœ¬
    remaining_text = "".join(response_message)[processed_chars:]
    if remaining_text:
        segment_text = get_string_no_punctuation_or_emoji(remaining_text)
        if segment_text:
            text_index += 1
            yield segment_text, text_index

    yield response_message, -1  # finish_task


async def start_llm_stream(new_llm_stream):
    """å¤ä½ç»ˆæ­¢ä¿¡å·ï¼Œå¹¶é‡æ–°å¯åŠ¨å¤§æ¨¡å‹æµ"""
    LLM_Abort_Event.clear()  # é‡æ–°å¯åŠ¨å‰å¤ä½
    async for text, idx in process_llm_stream(new_llm_stream):
        if idx > 0:
            print(f"ğŸ”Š æœ—è¯»: {text}")


def split_text_into_sentences(raw_text):
    # ä½¿ç”¨å¸¸è§çš„æ ‡ç‚¹ç¬¦å·åˆ†å‰²æ–‡æœ¬ï¼Œç”Ÿæˆå¥å­åˆ—è¡¨
    sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', '.', '!', '?', ';']  # å¸¸è§ä¸­æ–‡/è‹±æ–‡æ ‡ç‚¹
    sentences = []
    current_sentence = ""

    for char in raw_text:
        current_sentence += char
        if current_sentence[-1] in sentence_endings:
            sentences.append(current_sentence.strip())
            current_sentence = ""

    # å¦‚æœæœ‰æ®‹ç•™çš„æ–‡æœ¬ï¼ŒåŠ å…¥å¥å­åˆ—è¡¨
    if current_sentence.strip():
        sentences.append(current_sentence.strip())

    return sentences


def split_sentences(text,
                    pattern=(r'[^ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d\r\n]*\b[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\ã€'  # ä¸­æ–‡åºå· "ä¸€ã€äºŒã€"
                             r'|[^ï¼ˆ(ï¼‰)]*\b[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ï¼‰)]'  # æ‹¬å·å†…çš„ä¸­æ–‡åºå· "(ä¸€)(äºŒ)"
                             r'|[^\d\r\n]*\b\d+\ã€'  # æ•°å­—åºå· "1ã€2ã€"
                             r'|[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]'  # å¥å·ã€æ„Ÿå¹å·ã€é—®å·
                             r'|[^\r\n]*\r?\n'  # æ¢è¡Œç¬¦ï¼ˆæ”¯æŒ Windows çš„ \r\n å’Œ Unix çš„ \nï¼‰
                             )
                    ):
    """
    åˆ†å¥å‡½æ•°ï¼Œæ”¯æŒæŒ‰æ ‡ç‚¹ç¬¦å·å’Œç»“æ„åŒ–åºå·è¿›è¡Œåˆ†å¥ï¼Œåˆ†éš”ç¬¦ä¼šä¿ç•™åœ¨å‰ä¸€å¥ç»“å°¾ã€‚ç»“æ„åŒ–æ¯”è¾ƒæ¸…æ™°çš„åˆåŒã€åˆ¶åº¦æ–‡ä»¶ã€‚ç²—ç²’åº¦åˆ†å¥ï¼ˆä»¥è‡ªç„¶è¯­è¨€çš„æ ‡ç‚¹/åºå·ä¸ºä¸»ï¼‰
    :param text: è¾“å…¥çš„æ–‡æœ¬
    :param pattern: æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…åˆ†éš”ç¬¦
    :return: åˆ†å‰²åçš„å¥å­åˆ—è¡¨
    """
    if not pattern:
        pattern = r'(?=[ã€‚ï¼ï¼Ÿ])'
    sentences = re.findall(pattern, text)
    # re.findall re.split(pattern, text)
    return [s.strip() for s in sentences if s.strip()]


def split_sentences_clean(text, h_symbols=True, h_tables=True):
    """
    åˆåŒã€è§„ç« ã€å¸¦å¤§é‡ç¼–å·ã€æ¡æ¬¾ã€è¡¨æ ¼çš„æ–‡æœ¬,åˆ†å¥å»ºæ¨¡ã€æ‘˜è¦ã€åˆ‡å—å¤„ç†
    ç¯‡ç« åˆ†å¥ï¼Œé¢å¤–æ”¯æŒï¼š
      - ç¬¬Xæ¡ï¼ˆä¸­å›½å¼æ¡æ¬¾ï¼‰
      - (ä¸€)ã€(1)ã€(a) ç­‰æ‹¬å·ç¼–å·
      - 1.1ã€2.3.4 ç­‰å¤šçº§å°æ•°ç¼–å·
    :param text: str, æ•´æ®µåŸå§‹æ–‡æœ¬
    :param h_symbols: bool, æ˜¯å¦å¤„ç†è¿ç»­ç¬¦å·å’Œæ¢è¡Œç¬¦æ ‡å‡†åŒ–
    :param h_tables: bool, æ˜¯å¦å¤„ç†è¡¨æ ¼ç¬¦å·â€œ|â€
    :return: list of sentences
    """
    # 1. ç»Ÿä¸€æ¢è¡Œç¬¦
    text = text.replace('\r\n', '\n').replace('\r', '\n')

    if h_symbols:
        # 2. åœ¨å„ç§åºå·åé¢åŠ ç©ºæ ¼ï¼Œé¿å…ä¸æ­£æ–‡ç²˜è¿
        # ï¼ˆ1ï¼‰ä¸­å›½å¼æ¡æ¬¾ï¼šç¬¬Xæ¡
        text = re.sub(r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ¡)', r'\1 ', text)
        # ï¼ˆ2ï¼‰æ‹¬å·ç¼–å·ï¼š(ä¸€)ã€(1)ã€(a)â€¦â€¦
        text = re.sub(r'(\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\dA-Za-z]+\))', r'\1 ', text)
        # ï¼ˆ3ï¼‰å¤šçº§å°æ•°ç¼–å·ï¼š1.1ã€2.3.4â€¦â€¦
        text = re.sub(r'(\d+(?:\.\d+)+)', r'\1 ', text)

        # 3. ç‰¹æ®Šå¤„ç†è¡¨æ ¼â€œ|åºå·.â€ã€â€œ|åºå·ã€â€
        text = re.sub(r'(\|\s*\d+[\.ã€])', r'\1 ', text)
        text = re.sub(r'(^|\n)\s*(\d+[\.ã€])', r'\1\2 ', text)

    if h_tables:
        # 4. æŠŠè¡¨æ ¼åˆ†éš”ç¬¦ â€˜|â€™ çœ‹ä½œå¥å·
        text = text.replace('|', 'ã€‚')

    # 5. åˆå¹¶è¿ç»­ä¸­æ–‡æ ‡ç‚¹
    text = re.sub(r'[ã€‚ï¼ï¼Ÿï¼›]{2,}', 'ã€‚', text)

    # 6. æŒ‰ä¸­æ–‡å¥å·ã€é—®å·ã€å¹å·ã€åˆ†å·åˆ‡å¥
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›])', text)

    # 7. å»ç©ºç™½ï¼Œè¿‡æ»¤å¤ªçŸ­çš„
    return [s.strip() for s in sentences if s.strip() and len(s.strip()) > 1]


def cross_sentence_chunk(sentences: list[str], chunk_size=5, overlap_size=2, max_length=512, tokenizer=None):
    """
    æ»‘åŠ¨çª—å£åˆ†å— + æœ€å¤§é•¿åº¦æˆªæ–­ï¼ˆåªæµ‹é•¿åº¦ï¼Œä¸ç”¨ tokenizer.decodeï¼‰
    :param sentences: åˆ†å¥åçš„å¥å­åˆ—è¡¨
    :param chunk_size: æ¯å—åŒ…å«å‡ ä¸ªå¥å­
    :param overlap_size: ç›¸é‚»å—é‡å å‡ ä¸ªå¥å­
    :param max_length: æœ€å¤§é•¿åº¦ï¼ˆtokenæ•°æˆ–å­—ç¬¦æ•°ï¼‰
    :param tokenizer: ç”¨äºè®¡ç®— token é•¿åº¦çš„åˆ†è¯å™¨ï¼ˆå¯é€‰ï¼‰
    :return: List[str] æ¯å—ä¸€ä¸ªå­—ç¬¦ä¸²
    """
    chunks = []
    step = max(chunk_size - overlap_size, 1)

    for i in range(0, len(sentences), step):
        window = sentences[i: i + chunk_size]
        text = " ".join(window)

        # ç”¨ tokenizer åªæµ‹é•¿åº¦ï¼Œä¸ decode
        if tokenizer:
            token_count = len(tokenizer.encode(text))
            if token_count > max_length:
                # ç›´æ¥æŒ‰å­—ç¬¦æˆªæ–­
                text = text[: max_length]
        else:
            # ç”¨å­—ç¬¦é•¿åº¦ä½œä¸º fallback
            if len(text) > max_length:
                text = text[: max_length]

        chunks.append(text)

    return chunks


def organize_segments_chunk(sentences: list[str], chunk_size=5, overlap_size=2, max_length=512, tokenizer=None) -> list[
    list[str]]:
    """
    äº¤å‰åˆ†å—å‡½æ•°ï¼Œå°†å¥å­åˆ—è¡¨æŒ‰å—åˆ’åˆ†ï¼Œå¹¶åœ¨å—ä¹‹é—´ä¿æŒä¸€å®šé‡å ï¼Œå¹¶æ ¹æ®max_lengthæ§åˆ¶æ¯ä¸ªæ®µè½çš„æœ€å¤§é•¿åº¦ã€‚
    :param sentences: åˆ†å¥åçš„å¥å­åˆ—è¡¨
    :param chunk_size: æ¯ä¸ªå—çš„å¥å­æ•°é‡
    :param overlap_size: å—ä¹‹é—´çš„é‡å å¥å­æ•°
    :param max_length: æ¯ä¸ªå—çš„æœ€å¤§é•¿åº¦ï¼ˆtokenæ•°ï¼‰
    :param tokenizer: ç”¨äºè®¡ç®—tokené•¿åº¦çš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰
    :return: äº¤å‰åˆ†å—åçš„å¥å­å—åˆ—è¡¨
    """
    chunks = []
    total_sentences = len(sentences)
    current_chunk = []

    # æ ¹æ® max_length å’Œå¥å­æ•°é‡åˆ†å—
    for i in range(total_sentences):
        sentence = sentences[i]
        current_chunk.append(sentence)
        current_chunk_text = ' '.join(current_chunk)

        # å¦‚æœå½“å‰å—é•¿åº¦è¶…è¿‡ max_lengthï¼Œåˆ™åˆ†å‰²å—å¹¶é‡ç½®
        if lang_token_size(current_chunk_text, tokenizer=tokenizer) > max_length:
            current_chunk.pop()  # åˆ é™¤æœ€åä¸€ä¸ªå¥å­
            chunks.append(current_chunk)  # æ·»åŠ å½“å‰å—
            current_chunk = [sentence]  # ä»å½“å‰å¥å­å¼€å§‹æ–°çš„å—

        # å¦‚æœæ˜¯æœ€åä¸€ä¸ªå¥å­ï¼Œå°†å½“å‰å—æ·»åŠ åˆ°ç»“æœä¸­
        if i == total_sentences - 1:
            chunks.append(current_chunk)

    # å¤„ç†æ»‘åŠ¨çª—å£é‡å 
    overlapped_chunks = []
    for i in range(0, len(chunks), chunk_size - overlap_size):
        chunk = []
        for j in range(i, min(i + chunk_size, len(chunks))):
            chunk.extend(chunks[j])

        while lang_token_size(' '.join(chunk), tokenizer=tokenizer) > max_length:
            overlapped_chunks.append(chunk[:chunk_size])  # åˆ†å‰²ä¿ç•™å‰é¢çš„å—
            chunk = chunk[chunk_size:]  # å‰©ä¸‹çš„éƒ¨åˆ†ç»§ç»­å¤„ç†

        overlapped_chunks.append(chunk[:max_length])  # ç¡®ä¿å—çš„é•¿åº¦åœ¨ max_length ä¹‹å†…,æ·»åŠ ä¸è¶…é•¿çš„éƒ¨åˆ†

    return overlapped_chunks


# å®ç°å°åˆ°å¤§åˆ†å—é€»è¾‘
def organize_segments(tokens, small_chunk_size: int = 175, large_chunk_size: int = 512, overlap: int = 20):
    '''
    å°å—é€‚åˆç”¨äºæŸ¥è¯¢åŒ¹é…ï¼Œæé«˜æŸ¥è¯¢çš„ç²¾å‡†åº¦ã€‚
    å¤§å—åˆ’åˆ†ï¼Œå°†åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å¤šä¸ªå°å—åˆå¹¶ä¸ºè¾ƒå¤§çš„ç‰‡æ®µã€‚
    æ»‘åŠ¨çª—å£ï¼šä¸ºäº†ä¿æŒä¸Šä¸‹æ–‡å…³ç³»ï¼Œåœ¨å°å—å’Œå¤§å—ä¹‹é—´æ·»åŠ ä¸€å®šçš„é‡å åŒºåŸŸï¼Œç¡®ä¿è¾¹ç¼˜ä¿¡æ¯ä¸ä¸¢å¤±ã€‚è¿™æ ·ï¼ŒæŸ¥è¯¢ç»“æœèƒ½ä¿æŒæ›´é«˜çš„è¿è´¯æ€§ã€‚
    '''

    # å°å—åˆ†å‰²
    small_chunks = []
    for i in range(0, len(tokens), small_chunk_size - overlap):
        small_chunks.append(tokens[i:i + small_chunk_size])  # ''.join()

    # ç»„ç»‡å¤§ç‰‡æ®µ
    large_chunks = []
    for i in range(0, len(small_chunks), large_chunk_size // small_chunk_size):
        large_chunk = []
        for j in range(i, min(i + large_chunk_size // small_chunk_size, len(small_chunks))):
            large_chunk.extend(small_chunks[j])
        large_chunks.append(large_chunk[:large_chunk_size])

    return small_chunks, large_chunks


# æ”¯æŒçš„æ‰©å±•å
def get_local_suffix(folder_path, supported_suffix=None, recursive=False):
    supported_extensions = (ext.lower() for ext in supported_suffix or [".jpg", ".jpeg", ".png", ".bmp"])
    folder = Path(folder_path)
    if not folder.is_dir():
        raise ValueError(f"The path '{folder_path}' is not a valid directory.")
    pattern = "**/*" if recursive else "*"
    return [str(f_path) for f_path in folder.glob(pattern) if f_path.suffix.lower() in supported_extensions]


def get_file_type(object_name: str) -> str:
    """
    æ ¹æ®æ–‡ä»¶åæˆ–è·¯å¾„åˆ¤æ–­æ–‡ä»¶ç±»å‹ã€‚

    :param object_name: æ–‡ä»¶åæˆ–è·¯å¾„
    :return: æ–‡ä»¶ç±»å‹ï¼ˆå¦‚ 'image', 'audio', 'video', 'text', 'compressed', '*'ï¼‰
    .pdf .txt .csv .doc .docx .xls .xlsx .ppt .pptx .md .jpeg .png .bmp .gif .svg .svgz .webp .ico .xbm .dib .pjp .tif .pjpeg .avif .dot .apng .epub .tiff .jfif .html .json .mobi .log .go .h .c .cpp .cxx .cc .cs .java .js .css .jsp .php .py .py3 .asp .yaml .yml .ini .conf .ts .tsx
    """
    if not object_name:
        return ""

    _, file_extension = os.path.splitext(object_name.lower())

    # å®šä¹‰æ–‡ä»¶ç±»å‹åˆ†ç±»
    file_types = {
        "image": [".png", ".jpg", ".jpeg", ".gif", ".bmp", ".svg", ".webp", ".tiff", ".heic", ".heif"],
        "audio": [".mp3", ".wav", ".ogg", ".aac", ".flac", ".m4a"],
        "video": [".mp4", ".avi", ".mkv", ".mov", ".wmv", ".flv", ".webm", ".3gp"],
        "text": [".txt", ".csv", ".md", ".html", ".json", ".xml"],
        "document": [".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".numbers"],
        "compressed": [".zip", ".rar", ".7z", ".tar", ".gz", ".bz2"],
        "code": [".py", ".java", ".c", ".cpp", ".js", ".ts", ".html", ".css", ".sql"]
    }

    for file_type, extensions in file_types.items():
        if file_extension in extensions:
            return file_type

    return "*"


def get_file_type_wx(object_name: str) -> str:
    if not object_name:  # object_name.endswith()
        return ""
    '''
    æ–‡æ¡£ï¼šDOCã€DOCXã€XLSã€XLSXã€PPTã€PPTXã€PDFã€Numbersã€CSV
    å›¾ç‰‡ï¼šJPGã€JPG2ã€PNGã€GIFã€WEBPã€HEICã€HEIFã€BMPã€PCDã€TIFF
    æ–‡ä»¶ä¸Šä¼ å¤§å°é™åˆ¶ï¼šæ¯ä¸ªæ–‡ä»¶æœ€å¤§512MBã€‚
    '''
    _, file_extension = os.path.splitext(object_name.lower())
    # æ ¹æ®æ–‡ä»¶åç¼€åˆ¤æ–­ç±»å‹
    if file_extension in [".png", ".jpg", ".jpeg", ".gif", ".bmp", ".svg", ".webp"]:
        return "image"
    elif file_extension in [".mp3", ".wav", ".ogg", ".aac", ".flac"]:
        return "audio"
    elif file_extension in [".mp4", ".avi", ".mkv", ".mov", ".wmv", ".flv", ".webm"]:
        return "video"
    elif file_extension in [".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".txt", ".csv",
                            '.zip', '.rar', '.html']:
        return "*"
    return ""


def parse_tool_text(text):
    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ¥åŒ¹é… <tags>, <tool_call>, <content> åŠå…¶å†…å®¹
    tags_pattern = r'<tags>(.*?)</tags>'
    tool_call_pattern = r'<tool_call>(.*?)</tool_call>'
    content_pattern = r'<content>(.*?)</content>'
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾åŒ¹é…çš„å†…å®¹
    tags_match = re.search(tags_pattern, text, re.DOTALL)
    tool_call_match = re.search(tool_call_pattern, text, re.DOTALL)
    content_match = re.search(content_pattern, text, re.DOTALL)
    # æå–åŒ¹é…çš„å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    tags = tags_match.group(1).strip() if tags_match else ""
    tool_call = tool_call_match.group(1).strip() if tool_call_match else ""
    content = content_match.group(1).strip() if content_match else ""
    # å°†æå–çš„å†…å®¹å­˜å‚¨åœ¨å­—å…¸ä¸­
    result = {
        "tags": tags,
        "tool_call": tool_call,
        "content": content
    }
    return result


def get_clock(t, speed=10):
    return "ğŸ•ğŸ•‘ğŸ•’ğŸ•“ğŸ•”ğŸ••ğŸ•–ğŸ•—ğŸ•˜ğŸ•™ğŸ•šğŸ•›"[int(t * speed) % 12]


def parse_time(val):
    # è§£ææ—¶é—´å­—ç¬¦ä¸²ï¼Œå¹¶è½¬æ¢ä¸º datetime å¯¹è±¡ã€‚å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤å€¼ datetime.minï¼ˆ0001-01-01 00:00:00ï¼‰ã€‚
    if isinstance(val, datetime):
        return val
    try:
        return datetime.strptime(val, "%Y-%m-%d %H:%M:%S")
    except Exception:
        return datetime.min


def parse_time_format(val):
    # å°† datetime å¯¹è±¡è½¬æ¢ä¸º ISO 8601 æ ¼å¼çš„å­—ç¬¦ä¸²
    if isinstance(val, datetime):
        return val.isoformat()  # è½¬æ¢ä¸ºISO 8601æ ¼å¼
    return val


def format_date(date_str: str):
    # ç›´æ¥ä½¿ç”¨ strptime æ¥è§£ææ—¥æœŸå¹¶æ ¼å¼åŒ–ä¸ºç›®æ ‡æ ¼å¼ï¼šYYYY-MM-DD
    return datetime.strptime(date_str, "%Y/%m/%d %H:%M:%S").date().strftime("%Y-%m-%d")


def format_date_type(date=None):
    """
    :param date:å¯ä»¥æ˜¯ä¸€ä¸ªæ—¥æœŸå­—ç¬¦ä¸²æˆ– Noneï¼ˆå¦‚æœä¼ å…¥ Noneï¼Œåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸï¼‰ã€‚
    :return:è¿”å›ä¸€ä¸ª datetime å¯¹è±¡ï¼Œå¦‚æœ date æ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²ï¼Œæˆ–è€…å½“å‰æ—¶é—´æˆ–datetime
    """
    # å¦‚æœæ²¡æœ‰ä¼ å…¥æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
    if not date:
        date = datetime.now()
    elif isinstance(date, str):
        supported_formats = [
            "%Y-%m-%d",
            "%Y/%m/%d",
            "%Y-%m-%d %H:%M:%S",
            "%Y/%m/%d %H:%M:%S",
        ]
        for fmt in supported_formats:
            try:
                date = datetime.strptime(date, fmt)
                break
            except ValueError:
                continue
        else:
            raise ValueError(f"Invalid date format: {date}. Supported formats are {supported_formats}.")

    return date  # isinstance(date, datetime)


def get_times_shift(days_shift: int = 0, hours_shift: int = 0):
    """
    :param days_shift: åç§»çš„å¤©æ•°ï¼Œ>0 è¡¨ç¤ºæœªæ¥ï¼Œ<0 è¡¨ç¤ºè¿‡å»ï¼Œ0 è¡¨ç¤ºå½“å‰æ—¥æœŸã€‚
    :param hours_shift: åç§»çš„å°æ—¶æ•°ï¼Œ>0 è¡¨ç¤ºæœªæ¥ï¼Œ<0 è¡¨ç¤ºè¿‡å»ï¼Œ0 è¡¨ç¤ºå½“å‰æ—¶é—´ã€‚
    :return: æ ¼å¼åŒ–åçš„æ—¶é—´ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD HH:MM:SS'ã€‚
    """
    current_datetime = datetime.now()
    adjusted_time = current_datetime + timedelta(days=days_shift, hours=hours_shift)
    return adjusted_time.strftime('%Y-%m-%d %H:%M:%S')


def get_day_range(date=None, shift: int = 0, count: int = 1):
    date = format_date_type(date)
    start_date = date - timedelta(days=shift)
    end_date = start_date + timedelta(days=count)
    return start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")


def get_week_range(date=None, shift: int = 0, count: int = 1):
    """
    è·å–æŒ‡å®šæ—¥æœŸæ‰€åœ¨å‘¨çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸã€‚
    æ”¯æŒé€šè¿‡ shift å‚æ•°åç§»å‘¨ã€‚

    :param date: æŒ‡å®šçš„æ—¥æœŸï¼ˆé»˜è®¤ä¸ºå½“å‰æ—¥æœŸï¼‰ã€‚
    :param shift: åç§»å‘¨æ•°ï¼Œ>0 è¡¨ç¤ºæœªæ¥çš„å‘¨ï¼Œ<0 è¡¨ç¤ºè¿‡å»çš„å‘¨ï¼Œ0 è¡¨ç¤ºå½“å‰å‘¨ã€‚
    :param count: æ§åˆ¶è¿”å›çš„å‘¨æ•°èŒƒå›´ï¼Œé»˜è®¤ä¸º 1ï¼Œè¡¨ç¤ºè¿”å›ä¸€ä¸ªå‘¨çš„æ—¥æœŸèŒƒå›´ã€‚
    :return: è¿”å›æŒ‡å®šå‘¨çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º ('YYYY-MM-DD', 'YYYY-MM-DD')
    """
    date = format_date_type(date)
    # æ ¹æ® shift å‚æ•°è°ƒæ•´æ—¥æœŸ
    date = date + relativedelta(weeks=shift)
    # è·å–ä»Šå¤©æ˜¯å‘¨å‡  (0 æ˜¯å‘¨ä¸€, 6 æ˜¯å‘¨æ—¥)
    weekday = date.weekday()
    # è®¡ç®—å‘¨ä¸€çš„æ—¥æœŸ (å¼€å§‹æ—¥æœŸ)
    start_of_week = date - timedelta(days=weekday)
    # è®¡ç®—å‘¨æ—¥çš„æ—¥æœŸ (ç»“æŸæ—¥æœŸ)
    # end_of_week = start_of_week + timedelta(days=6)

    end_date = start_of_week + timedelta(weeks=count) - timedelta(days=1)  # start_of_week + timedelta(days=6)

    return start_of_week.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")


def get_month_range(date=None, shift: int = 0, count: int = 1):
    """
    è·å–æŒ‡å®šæ—¥æœŸæ‰€åœ¨æœˆçš„å¼€å§‹å’Œç»“æŸæ—¥æœŸã€‚
    æ”¯æŒé€šè¿‡ shift å‚æ•°åç§»æœˆæ•°ï¼Œå’Œé€šè¿‡ count æ§åˆ¶è¿”å›çš„æœˆä»½èŒƒå›´ã€‚

    :param date: æŒ‡å®šçš„æ—¥æœŸï¼ˆé»˜è®¤ä¸ºå½“å‰æ—¥æœŸï¼‰ã€‚
    :param shift: åç§»çš„æœˆæ•°ï¼Œ>0 è¡¨ç¤ºæœªæ¥çš„æœˆï¼Œ<0 è¡¨ç¤ºè¿‡å»çš„æœˆï¼Œ0 è¡¨ç¤ºå½“å‰æœˆã€‚
    :param count: æ§åˆ¶è¿”å›çš„æœˆä»½èŒƒå›´ï¼Œé»˜è®¤ä¸º 1ï¼Œè¡¨ç¤ºè¿”å›ä¸€ä¸ªæœˆçš„å¼€å§‹å’Œç»“æŸæ—¥æœŸã€‚
    :return: è¿”å›æŒ‡å®šæœˆä»½çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º ('YYYY-MM-DD', 'YYYY-MM-DD')
      """
    date = format_date_type(date)
    # æ ¹æ® shift å‚æ•°è°ƒæ•´æ—¥æœŸ
    start_date = (date + relativedelta(months=shift)).replace(day=1)
    # è®¡ç®—ä¸‹ä¸ªæœˆçš„ç¬¬ä¸€å¤©ï¼Œç„¶åå‡å»ä¸€å¤©
    end_date = (start_date + relativedelta(months=count)).replace(day=1) - timedelta(days=1)  # + timedelta(days=32)
    return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')  # '%Y-%m-01'


def get_quarter_range(date=None, shift: int = 0, count: int = 1):
    """
    è·å–æŒ‡å®šæ—¥æœŸæ‰€åœ¨å­£åº¦çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸã€‚
    æ”¯æŒé€šè¿‡ shift å‚æ•°åç§»å­£åº¦æ•°ï¼Œå’Œé€šè¿‡ count æ§åˆ¶è¿”å›çš„å­£åº¦èŒƒå›´ã€‚

    :param date: æŒ‡å®šçš„æ—¥æœŸï¼ˆé»˜è®¤ä¸ºå½“å‰æ—¥æœŸï¼‰ã€‚
    :param shift: åç§»çš„å­£åº¦æ•°ï¼Œ>0 è¡¨ç¤ºæœªæ¥çš„å­£åº¦ï¼Œ<0 è¡¨ç¤ºè¿‡å»çš„å­£åº¦ï¼Œ0 è¡¨ç¤ºå½“å‰å­£åº¦ã€‚
    :param count: æ§åˆ¶è¿”å›çš„å­£åº¦èŒƒå›´ï¼Œé»˜è®¤ä¸º 1ï¼Œè¡¨ç¤ºè¿”å›ä¸€ä¸ªå­£åº¦çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸã€‚
    :return: è¿”å›æŒ‡å®šå­£åº¦çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º ('YYYY-MM-DD', 'YYYY-MM-DD')
    """
    date = format_date_type(date)

    # ç¡®å®šå½“å‰æ—¥æœŸæ‰€åœ¨å­£åº¦çš„èµ·å§‹æœˆä»½
    start_month = 3 * ((date.month - 1) // 3) + 1
    start_date = (date.replace(month=start_month, day=1)
                  + relativedelta(months=3 * shift))

    # è®¡ç®—å­£åº¦ç»“æŸæ—¥æœŸ
    end_date = (start_date + relativedelta(months=3 * count)).replace(day=1) - timedelta(days=1)

    return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')


def get_quarter_month_range(date=None, shift: int = 0, count: int = 1):
    """
    è·å–æŒ‡å®šæ—¥æœŸæ‰€åœ¨å­£åº¦çš„æœˆä»½èŒƒå›´ã€‚
    æ”¯æŒé€šè¿‡ shift å‚æ•°åç§»å­£åº¦æ•°ï¼Œå’Œé€šè¿‡ count æ§åˆ¶è¿”å›çš„å­£åº¦èŒƒå›´ã€‚

    :param date: æŒ‡å®šçš„æ—¥æœŸï¼ˆé»˜è®¤ä¸ºå½“å‰æ—¥æœŸï¼‰ã€‚
    :param shift: åç§»å­£åº¦æ•°ï¼Œ>0 è¡¨ç¤ºæœªæ¥çš„å­£åº¦ï¼Œ<0 è¡¨ç¤ºè¿‡å»çš„å­£åº¦ï¼Œ0 è¡¨ç¤ºå½“å‰å­£åº¦ã€‚
    :param count: æ§åˆ¶è¿”å›çš„å­£åº¦èŒƒå›´ï¼Œé»˜è®¤ä¸º 1ï¼Œè¡¨ç¤ºè¿”å›ä¸€ä¸ªå­£åº¦çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸã€‚
    :return: è¿”å›å­£åº¦çš„å¼€å§‹æœˆå’Œç»“æŸæœˆä»¥åŠèµ·å§‹å¹´ä»½ï¼Œæ ¼å¼ä¸º ('YYYY','MM', 'MM')
    """
    date = format_date_type(date)

    current_year = date.year

    # è®¡ç®—å½“å‰å­£åº¦çš„èµ·å§‹æœˆä»½
    quarter_start = (date.month - 1) // 3 * 3 + 1

    # æ ¹æ® shift åç§»å­£åº¦
    quarter_start += shift * 3

    # å¤„ç†è·¨å¹´æƒ…å†µï¼šå¦‚æœèµ·å§‹æœˆä»½è¶…å‡ºäº†12æœˆï¼Œéœ€è¦è°ƒæ•´å¹´ä»½
    if quarter_start > 12:
        quarter_start -= 12
        current_year += 1
    elif quarter_start < 1:
        quarter_start += 12
        current_year -= 1

    # è®¡ç®—å­£åº¦çš„ç»“æŸæœˆä»½
    quarter_end = quarter_start + 3 * count - 1

    # å¤„ç†ç»“æŸæœˆä»½è·¨å¹´æƒ…å†µï¼šå¦‚æœç»“æŸæœˆä»½è¶…è¿‡12æœˆï¼Œéœ€è¦è°ƒæ•´å¹´ä»½
    if quarter_end > 12:
        quarter_end -= 12

    return current_year, quarter_start, quarter_end


def get_year_range(date=None, shift: int = 0, count: int = 1):
    """
    è·å–æŒ‡å®šæ—¥æœŸæ‰€åœ¨å¹´çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸã€‚
    æ”¯æŒé€šè¿‡ shift å‚æ•°åç§»å¹´æ•°ï¼Œå’Œé€šè¿‡ count æ§åˆ¶è¿”å›çš„å¹´åº¦èŒƒå›´ã€‚

    :param date: æŒ‡å®šçš„æ—¥æœŸï¼ˆé»˜è®¤ä¸ºå½“å‰æ—¥æœŸï¼‰ã€‚
    :param shift: åç§»çš„å¹´æ•°ï¼Œ>0 è¡¨ç¤ºæœªæ¥çš„å¹´ï¼Œ<0 è¡¨ç¤ºè¿‡å»çš„å¹´ï¼Œ0 è¡¨ç¤ºå½“å‰å¹´ã€‚
    :param count: æ§åˆ¶è¿”å›çš„å¹´åº¦èŒƒå›´ï¼Œé»˜è®¤ä¸º 1ï¼Œè¡¨ç¤ºè¿”å›ä¸€å¹´çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸã€‚
    :return: è¿”å›æŒ‡å®šå¹´çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º ('YYYY-MM-DD', 'YYYY-MM-DD')
    """
    date = format_date_type(date)

    # è®¡ç®—å¹´ä»½çš„å¼€å§‹æ—¥æœŸ
    start_date = date.replace(month=1, day=1) + relativedelta(years=shift)

    # è®¡ç®—å¹´ä»½çš„ç»“æŸæ—¥æœŸ
    end_date = (start_date + relativedelta(years=count)).replace(day=1, month=1) - timedelta(days=1)

    return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')


def get_half_year_range(date=None, shift: int = 0, count: int = 1):
    """
    è·å–æŒ‡å®šæ—¥æœŸæ‰€åœ¨çš„åŠå¹´ï¼ˆå‰åŠå¹´æˆ–ååŠå¹´ï¼‰èŒƒå›´ã€‚
    æ”¯æŒé€šè¿‡ shift å‚æ•°åç§»åŠå¹´æ•°ï¼Œå’Œé€šè¿‡ count æ§åˆ¶è¿”å›çš„åŠå¹´æ•°èŒƒå›´ã€‚

    :param date: æŒ‡å®šçš„æ—¥æœŸï¼ˆé»˜è®¤ä¸ºå½“å‰æ—¥æœŸï¼‰ã€‚
    :param shift: åŠå¹´åç§»é‡ï¼Œ0 è¡¨ç¤ºå½“å‰åŠå¹´ï¼Œ-1 è¡¨ç¤ºå‰ä¸€åŠå¹´ï¼Œ1 è¡¨ç¤ºä¸‹ä¸€åŠå¹´ã€‚
    :param count: è¿”å›çš„åŠå¹´èŒƒå›´ï¼Œé»˜è®¤ä¸º 1ï¼Œè¡¨ç¤ºè¿”å›ä¸€ä¸ªåŠå¹´çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸã€‚
    :return: è¿”å›æŒ‡å®šåŠå¹´çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º ('YYYY-MM-DD', 'YYYY-MM-DD')
    """
    date = format_date_type(date)

    # åˆ¤æ–­å½“å‰æ˜¯å‰åŠå¹´è¿˜æ˜¯ååŠå¹´
    if date.month <= 6:
        start_date = date.replace(month=1, day=1)
    else:
        start_date = date.replace(month=7, day=1)

    # è°ƒæ•´æ—¥æœŸåˆ°æŒ‡å®šçš„åŠå¹´
    start_date += relativedelta(months=6 * shift)
    # è®¡ç®—åŠå¹´ç»“æŸæ—¥æœŸ
    end_date = (start_date + relativedelta(months=6 * count)).replace(day=1) - timedelta(days=1)

    return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')


def date_range_calculator(period_type: str, date=None, shift: int = 0, count: int = 1) -> dict:
    """
    è®¡ç®—åŸºäºå‚è€ƒæ—¥æœŸçš„æ—¶é—´èŒƒå›´ã€‚

    :param period_type: æ—¶é—´å‘¨æœŸç±»å‹ï¼Œ'days'ã€'weeks'ã€'months' ç­‰
    :param date: åŸºå‡†æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'
    :param shift: åŠå¹´åç§»é‡ï¼Œ0 è¡¨ç¤ºå½“å‰åŠå¹´ï¼Œ-1 è¡¨ç¤ºå‰ä¸€åŠå¹´ï¼Œ1 è¡¨ç¤ºä¸‹ä¸€åŠå¹´ã€‚
    :param count: æ—¶é—´å‘¨æœŸæ•°é‡ï¼Œè¡¨ç¤ºä»å‚è€ƒæ—¥æœŸå‘å‰æˆ–å‘åçš„æ—¶é•¿
    :return: è¿”å›è®¡ç®—å‡ºçš„æ—¥æœŸèŒƒå›´ï¼ŒåŒ…å« 'start_date' å’Œ 'end_date'
    """
    period_map = {'days': get_day_range,
                  'weeks': get_week_range,
                  'month': get_month_range,
                  'quarters': get_quarter_range,
                  'half_year': get_half_year_range,
                  'year': get_year_range,
                  }

    handler = period_map.get(period_type)
    if handler:
        start_date, end_date = handler(date, shift, count)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ—¶é—´å•ä½: {period_type}")

    # è¿”å›ç»“æœå­—å…¸ï¼ŒåŒ…å«å¼€å§‹å’Œç»“æŸæ—¥æœŸ
    return {'start_date': start_date, 'end_date': end_date}


def is_finite(value) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰é™æ•°å­—"""
    try:
        float_val = float(value)
        return not (float_val == float('inf') or float_val == float('-inf') or float_val != float_val)
    except (TypeError, ValueError):
        return False


def cosine_sim(vecs1, vecs2):
    # ä¸¤ä¸ªå‘é‡ï¼ˆ1D æ•°ç»„ï¼‰ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
    dot_product = np.dot(vecs1, vecs2)
    similarity = dot_product / (np.linalg.norm(vecs1) * np.linalg.norm(vecs2))
    return similarity


def fast_dot_np(vecs1, vecs2):
    # ç”¨ NumPy æ‰¹é‡è®¡ç®—ç‚¹ç§¯,å½¢çŠ¶ç›¸åŒçš„ 2D æ•°ç»„é€è¡Œç‚¹ç§¯,çŸ©é˜µé€å…ƒç´ ç›¸ä¹˜åæŒ‰è¡Œæ±‚å’Œ
    return np.einsum('ij,ij->i', vecs1, vecs2)  # np.sum(A * B, axis=1)


# from sklearn.preprocessing import normalize
def normalize_np(vecs) -> list[float]:
    # æ‰‹åŠ¨å½’ä¸€åŒ–
    # norms = np.sqrt(np.einsum('ij,ij->i', vecs, vecs)) #æ¨¡é•¿,L2 èŒƒæ•° ||ndarr1|| for each row
    return vecs / np.linalg.norm(vecs, axis=1, keepdims=True)


def normalize_embeddings(vectors: list[list[float]], to_list=False):
    normalized = [vec / np.linalg.norm(vec) if np.linalg.norm(vec) != 0 else vec for vec in np.array(vectors)]
    return [vec.tolist() if isinstance(vec, np.ndarray) else vec for vec in normalized] if to_list else normalized


def calc_diff(x, y):
    x, y = x.double(), y.double()
    denominator = (x * x + y * y).sum()
    sim = 2 * (x * y).sum() / denominator
    return 1 - sim


# from sklearn.metrics.pairwise import cosine_similarity
def cosine_similarity_np(ndarr1, ndarr2):
    denominator = np.outer(np.linalg.norm(ndarr1, axis=1), np.linalg.norm(ndarr2, axis=1))
    dot_product = np.dot(ndarr1, ndarr2.T)  # np.einsum('ik,jk->ij', ndarr1, ndarr2)
    with np.errstate(divide='ignore', invalid='ignore'):
        similarity = np.where(denominator != 0, dot_product / denominator, 0)
    return similarity


# from scipy.special import softmax
def softmax_np(x):
    e_x = np.exp(x - np.max(x))  # Subtract max for numerical stability
    return e_x / e_x.sum()


def generate_loss_mask(input_ids, bos_id, eos_id, max_length):
    loss_mask = [0] * len(input_ids)
    i = 0
    while i < len(input_ids):
        if input_ids[i:i + len(bos_id)] == bos_id:
            start = i + len(bos_id)
            end = start
            while end < len(input_ids):
                if input_ids[end:end + len(eos_id)] == eos_id:
                    break
                end += 1
            for j in range(start + 1, min(end + len(eos_id) + 1, max_length)):
                loss_mask[j] = 1
            i = end + len(eos_id) if end < len(input_ids) else len(input_ids)
        else:
            i += 1
    return loss_mask


def get_similar_nodes(embeddings, base_nodes, top_k=3):
    """
    è®¡ç®— base_nodesï¼ˆåˆæ­¥å¬å›çš„è®°å½•ï¼‰ä¸æ‰€æœ‰è®°å½•ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„ top_k è®°å½•
    :param embeddings: æ‰€æœ‰èŠ‚ç‚¹çš„åµŒå…¥çŸ©é˜µ (Tensor)
    :param base_nodes: éœ€è¦æŸ¥è¯¢ç›¸ä¼¼è®°å½•çš„èŠ‚ç‚¹ç´¢å¼•åˆ—è¡¨
    :param top_k: æ¯ä¸ªèŠ‚ç‚¹è¦æ‰¾çš„ç›¸ä¼¼è®°å½•æ•°
    :return: å¬å›çš„ç›¸ä¼¼è®°å½•ç´¢å¼•åˆ—è¡¨
    """
    # æå– base_nodes å¯¹åº”çš„å‘é‡
    base_embeddings = embeddings[base_nodes]
    # all_embeddings = embeddings.cpu().detach().numpy()

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (sklearn)
    similarity_matrix = cosine_similarity_np(base_embeddings, embeddings)
    # å¯¹æ¯ä¸ª base_node å–æœ€ç›¸ä¼¼çš„ top_k è®°å½•ï¼ˆæ’é™¤è‡ªèº«ï¼‰
    similar_nodes = set()
    for i, node in enumerate(base_nodes):
        sorted_indices = np.argsort(-similarity_matrix[i])  # è·å–è¯¥è®°å½•çš„ç›¸ä¼¼åº¦æ’åº,é™åºæ’åº
        for idx in sorted_indices:
            if idx != node:  # æ’é™¤è‡ªèº«
                similar_nodes.add(idx)
            if len(similar_nodes) >= top_k:
                break

    return list(similar_nodes)


def float16_to_bin(num):
    # å°†float16æ•°æ‰“åŒ…ä¸º2å­—èŠ‚16ä½ï¼Œä½¿ç”¨struct.pack å¤„ç†äºŒè¿›åˆ¶æ•°æ®çš„æ¨¡å—
    packed_num = struct.pack('e', num)  # e åŠç²¾åº¦æµ®ç‚¹æ•°ï¼ˆfloat16,16-bit) b'\x00<'
    # è§£åŒ…æ‰“åŒ…åçš„å­—èŠ‚ä»¥è·å–æ•´æ•°è¡¨ç¤º
    int_value = struct.unpack('H', packed_num)[0]
    # å°†æ•´æ•°è¡¨ç¤ºè½¬æ¢ä¸ºäºŒè¿›åˆ¶
    binary_representation = bin(int_value)[2:].zfill(16)
    return binary_representation


def get_memory_info():
    """ä» /proc/self/status è·å–å†…å­˜ä½¿ç”¨ï¼ˆLinux Onlyï¼‰"""
    try:
        with open("/proc/self/status") as f:
            for line in f:
                if line.startswith("VmRSS:"):
                    mem_kb = int(line.split()[1])
                    return mem_kb / 1024  # è½¬ä¸º MB
    except Exception:
        return -1


def get_cpu_time():
    """ä» /proc/self/stat è·å– CPU æ—¶é—´ï¼ˆå•ä½ï¼šç§’ï¼‰"""
    try:
        with open("/proc/self/stat") as f:
            values = f.read().split()
            utime = int(values[13])
            stime = int(values[14])
            ticks_per_sec = os.sysconf(os.sysconf_names["SC_CLK_TCK"])
            return (utime + stime) / ticks_per_sec
    except Exception:
        return -1


def get_open_fds_count() -> int:
    try:
        base = f"/proc/{os.getpid()}/fd"
        return len(os.listdir(base))
    except Exception:
        return -1


def count_http_connections(port=8000):
    import platform
    if platform.system() != "Linux":
        print("Warning: count_http_connections is only supported on Linux.")
        return -1
    count = 0
    with open("/proc/net/tcp", "r") as f:
        lines = f.readlines()[1:]
        for line in lines:
            parts = line.strip().split()
            local_address = parts[1]
            local_port_hex = local_address.split(":")[1]
            if int(local_port_hex, 16) == port:
                count += 1
    return count


def named_partial(name, func, *args, **kwargs):
    p = partial(func, *args, **kwargs)
    p.__name__ = name
    return p


def is_empty_lambda(func):
    try:
        if callable(func) and func.__name__ == '<lambda>' and len(inspect.signature(func).parameters) == 0:
            return func() == []
    except:
        return False
    return False


def print_functions(module_name: str = None):
    module = importlib.import_module(module_name) if module_name else inspect.getmodule(inspect.currentframe())
    funcs = inspect.getmembers(module, inspect.isfunction)
    for name, _func in funcs:
        print(f"Function: {name}")


def functions_registry(functions_list: list, safe_path=True, module_name: str = None) -> dict:
    """
    æ ¹æ®å‡½æ•°åç§°åˆ—è¡¨,åˆ›å»ºå…¨å±€å‡½æ•°æ³¨å†Œè¡¨,æˆ–è€…æŒ‡å®šæ¨¡å—ä¸­åŠ¨æ€åŠ è½½
    1. ä»å½“å‰å…¨å±€ä½œç”¨åŸŸæŸ¥æ‰¾å‡½æ•°åï¼›
    2. æŒ‡å®š module_nameï¼Œæ‰¹é‡ä»è¯¥æ¨¡å—åŠ è½½ï¼›
    3. ä½¿ç”¨ 'module.path:func' æ ¼å¼ï¼Œå•ä¸ªåŠ¨æ€åŠ è½½ã€‚

    :param functions_list: éœ€è¦æ³¨å†Œçš„å‡½æ•°ååˆ—è¡¨
    :param safe_path: å–æ¶ˆä¸æ£€æŸ¥æ˜¯å¦å¯è°ƒç”¨ã€‚
    :param module_name: æ¨¡å—åç§°ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰ï¼Œé€‚åˆä»ä¸€ä¸ªæ¨¡å—ä¸­åŠ è½½å¤šä¸ªå‡½æ•°ã€‚
    :return: Dict[str, Callable[..., Any]]
    """
    module = importlib.import_module(module_name) if module_name else None
    if not safe_path:
        return {name: getattr(module, name) if module else globals().get(name) for name in functions_list}

    registry = {}
    for name in functions_list:
        try:
            if ":" in name:
                module_path, func_name = name.rsplit(":", 1)
                _module = importlib.import_module(module_path)
                func_obj = getattr(_module, func_name, None)
            else:
                func_obj = getattr(module, name) if module else globals().get(name)

            if not callable(func_obj):
                raise ValueError(f"å‡½æ•° {name} ä¸å­˜åœ¨æˆ–ä¸æ˜¯å¯è°ƒç”¨å¯¹è±¡,æœªåœ¨å½“å‰ä½œç”¨åŸŸä¸­æ‰¾åˆ°,å¯èƒ½æœªå¯¼å…¥æˆ–æ¨¡å—æœªæŒ‡å®šã€‚")

            registry[name] = func_obj
        except Exception as e:
            registry[name] = None
            print(f"[âš ï¸] åŠ è½½å‡½æ•°å¤±è´¥: {name} â†’ {type(e).__name__}: {e}")

    return registry
    # get_function_parameters


def function_registry_dynamic(functions_list: list, module_names: list):
    """
    åŠ¨æ€åŠ è½½æ¨¡å—å¹¶æ³¨å†Œå‡½æ•°
    :param functions_list: éœ€è¦æ³¨å†Œçš„å‡½æ•°ååˆ—è¡¨
    :param module_names: æ¨¡å—åç§°åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰
    :return: å‡½æ•°æ³¨å†Œè¡¨
    """
    registry = {}
    for module_name in module_names:
        try:
            module = importlib.import_module(module_name)  # åŠ¨æ€åŠ è½½æ¨¡å—
            for name in functions_list:
                if name not in registry:  # é¿å…é‡å¤è¦†ç›–
                    func = getattr(module, name, None)
                    if func is not None:
                        registry[name] = func
        except ModuleNotFoundError:
            print(f"Module '{module_name}' not found.")
    return registry


def openai_tools_to_mcp(tools):
    def decorator(mcp_server):
        for tool in tools:
            if tool["type"] == "function":
                func_info = tool["function"]

                def make_handler(func_name, func_desc):
                    async def handler(params):
                        # è¿™é‡Œå¯ä»¥æ·»åŠ é€šç”¨å¤„ç†é€»è¾‘
                        print(f"Handling {func_name} with params: {params}")
                        # å®é™…å¤„ç†å‡½æ•°åº”è¯¥åœ¨åˆ«å¤„å®šä¹‰
                        return await globals()[f"handle_{func_name}"](params)

                    handler.__name__ = func_name
                    handler.__doc__ = func_desc
                    return handler

                mcp_server.add_handler(
                    action=func_info["name"],
                    handler=make_handler(func_info["name"], func_info["description"])
                )
        return mcp_server

    return decorator


def deduplicate_tools_by_name(tools: list[dict]) -> list[dict]:
    seen = set()
    tools_metadata = []
    for tool in tools:
        name = tool.get("function", {}).get("name") or tool.get("name")
        if name and name not in seen:
            seen.add(name)
            tools_metadata.append(tool)
    return tools_metadata


# import psutil,signal,contextlib
# def kill_process_tree(pid: int):
#     """
#     Kills all descendant processes of the given pid by sending SIGKILL.
#
#     Args:
#         pid (int): Process ID of the parent process
#     """
#     try:
#         parent = psutil.Process(pid)
#     except psutil.NoSuchProcess:
#         return
#
#     # Get all children recursively
#     children = parent.children(recursive=True)
#
#     # Send SIGKILL to all children first
#     for child in children:
#         with contextlib.suppress(ProcessLookupError):
#             os.kill(child.pid, signal.SIGKILL)
#
#     # Finally kill the parent
#     with contextlib.suppress(ProcessLookupError):
#         os.kill(pid, signal.SIGKILL)


class BM25:
    def __init__(self, corpus, k1=1.5, b=0.75):
        self.k1 = k1
        self.b = b
        self.corpus = [jieba.lcut(doc) for doc in corpus]  # ä½¿ç”¨ jieba å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯, cut_all=False
        self.doc_lengths = [len(doc) for doc in self.corpus]
        self.avg_doc_length = sum(self.doc_lengths) / len(self.doc_lengths)
        self.doc_count = len(self.corpus)
        self.doc_term_freqs = [Counter(doc) for doc in self.corpus]
        self.inverted_index = self.build_inverted_index()
        self.idf_cache = {}  # å¢åŠ ä¸€ä¸ª IDF ç¼“å­˜ï¼Œæé«˜æ•ˆç‡

    def build_inverted_index(self):
        inverted_index = {}
        for doc_id, doc_term_freq in enumerate(self.doc_term_freqs):
            for term, freq in doc_term_freq.items():
                if term not in inverted_index:
                    inverted_index[term] = []
                inverted_index[term].append((doc_id, freq))
        return inverted_index

    def idf(self, term):
        if term in self.idf_cache:
            return self.idf_cache[term]
        doc_freq = len(self.inverted_index.get(term, []))
        if doc_freq == 0:
            self.idf_cache[term] = 0
        else:
            self.idf_cache[term] = math.log((self.doc_count - doc_freq + 0.5) / (doc_freq + 0.5) + 1.0)
        return self.idf_cache[term]

    def bm25_score(self, query_terms, doc_id):
        score = 0
        doc_length = self.doc_lengths[doc_id]
        for term in query_terms:
            tf = self.doc_term_freqs[doc_id].get(term, 0)
            idf = self.idf(term)
            numerator = tf * (self.k1 + 1)
            denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avg_doc_length))
            score += idf * (numerator / denominator)
        return score

    def rank_documents(self, query, sort=True, normalize=False):
        query_terms = list(jieba.cut(query))  # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        scores = [(doc_id, self.bm25_score(query_terms, doc_id)) for doc_id in range(self.doc_count)]
        if normalize:
            max_score = max(scores, key=lambda x: x[1])[1]
            min_score = min(scores, key=lambda x: x[1])[1]
            if max_score != min_score:
                scores = [(doc_id, (score - min_score) / (max_score - min_score)) for doc_id, score in scores]

        return sorted(scores, key=lambda x: x[1], reverse=True) if sort else scores


class LRUCache:
    def __init__(self, capacity: int):
        self.stack = OrderedDict()
        self.capacity = capacity

    def get(self, key):
        if key in self.stack:
            self.stack.move_to_end(key)
            return self.stack[key]
        else:
            return None

    def put(self, key, value) -> None:
        if key in self.stack:
            self.stack[key] = value
            self.stack.move_to_end(key)
        else:
            self.stack[key] = value
        if len(self.stack) > self.capacity:
            self.stack.popitem(last=False)

    def change_capacity(self, capacity):
        self.capacity = capacity
        for i in range(len(self.stack) - capacity):
            self.stack.popitem(last=False)

    def delete(self, key):
        if key in self.stack:
            del self.stack[key]

    def keys(self):
        return self.stack.keys()

    def __len__(self):
        return len(self.stack)

    def __contains__(self, key):
        return key in self.stack


# class BM25:
#     def __init__(self, corpus, k1=1.5, b=0.75):
#         self.k1 = k1
#         self.b = b
#         self.corpus = [list(jieba.cut(doc)) for doc in corpus]  # doc.split()
#         self.N = len(self.corpus)  # è¯­æ–™åº“ä¸­æ–‡æ¡£æ€»æ•°
#         self.avgdl = sum(len(doc) for doc in self.corpus) / self.N  # æ–‡æ¡£çš„å¹³å‡é•¿åº¦
#         self.df = self._calculate_df()  # æ¯ä¸ªè¯é¡¹çš„æ–‡æ¡£é¢‘ç‡
#         self.idf = self._calculate_idf()  # æ¯ä¸ªè¯é¡¹çš„é€†æ–‡æ¡£é¢‘ç‡
#
#     def _calculate_df(self):
#         """è®¡ç®—è¯é¡¹çš„æ–‡æ¡£é¢‘ç‡"""
#         df = {}
#         for doc in self.corpus:
#             unique_words = set(doc)
#             for word in unique_words:
#                 df[word] = df.get(word, 0) + 1
#         return df
#
#     def _calculate_idf(self):
#         """è®¡ç®—è¯é¡¹çš„é€†æ–‡æ¡£é¢‘ç‡"""
#         idf = {}
#         for word, freq in self.df.items():
#             idf[word] = math.log((self.N - freq + 0.5) / (freq + 0.5) + 1)
#         return idf
#
#     def _score(self, query, doc):
#         """è®¡ç®—å•ä¸ªæ–‡æ¡£å¯¹æŸ¥è¯¢çš„ BM25 å¾—åˆ†"""
#         score = 0.0
#         doc_len = len(doc)
#         term_frequencies = Counter(doc)
#         for word in query:
#             if word in term_frequencies:
#                 freq = term_frequencies[word]
#                 numerator = self.idf.get(word, 0) * freq * (self.k1 + 1)
#                 denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)
#                 score += numerator / denominator
#         return score
#
#     def get_scores(self, query):
#         """è®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªæ–‡æ¡£å¯¹æŸ¥è¯¢çš„ BM25 å¾—åˆ†"""
#         query = list(jieba.cut(query))  # ä½¿ç”¨ jieba å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
#         scores = []
#         for doc in self.corpus:
#             scores.append(self._score(query, doc))
#         return scores

if __name__ == "__main__":
    # from rank_bm25 import BM25Okapi
    jieba.initialize()
    # jieba.load_userdict('data/patent_thesaurus.txt')
    corpus = [
        "å¿«é€Ÿçš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†æ‡’ç‹—",
        "æ‡’ç‹—èººä¸‹äº†",
        "ç‹ç‹¸å¾ˆå¿«é€Ÿå¹¶ä¸”è·³å¾—å¾ˆé«˜",
        "å¿«é€Ÿçš„æ£•è‰²ç‹ç‹¸",
        "çŒ«è·³è¿‡äº†ç‹—"
    ]
    query = "å¿«é€Ÿçš„ç‹ç‹¸"
    bm25 = BM25(corpus)
    scores = bm25.rank_documents(query)
    print(scores, bm25.corpus)
    # print(BM25(corpus).rank_documents(query))

    with open("utils.py", 'r', encoding='utf-8') as file:
        code = file.read()
    functions = extract_function_metadata(code, r=None)
    for func in functions:
        print(func)
