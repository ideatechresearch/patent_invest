import re, json
import inspect
import xml.etree.ElementTree as ET
from difflib import get_close_matches, SequenceMatcher
from collections import OrderedDict, Counter
import math
import jieba


class LRUCache:
    def __init__(self, capacity: int):
        self.stack = OrderedDict()
        self.capacity = capacity

    def get(self, key):
        if key in self.stack:
            self.stack.move_to_end(key)
            return self.stack[key]
        else:
            return None

    def put(self, key, value) -> None:
        if key in self.stack:
            self.stack[key] = value
            self.stack.move_to_end(key)
        else:
            self.stack[key] = value
        if len(self.stack) > self.capacity:
            self.stack.popitem(last=False)

    def change_capacity(self, capacity):
        self.capacity = capacity
        for i in range(len(self.stack) - capacity):
            self.stack.popitem(last=False)

    def delete(self, key):
        if key in self.stack:
            del self.stack[key]

    def keys(self):
        return self.stack.keys()

    def __len__(self):
        return len(self.stack)

    def __contains__(self, key):
        return key in self.stack


def get_function_parameters(func):
    signature = inspect.signature(func)
    parameters = signature.parameters
    return [param for param in parameters]


def get_function_string(func):
    try:
        function_code = inspect.getsource(func)
        return function_code
    except Exception as e:
        return f"{e}"


# é€’å½’åœ°å°†å­—å…¸ä¸­çš„æ‰€æœ‰é”®åè½¬æ¢ä¸ºé¦–å­—æ¯å¤§å†™çš„é©¼å³°å‘½å
def convert_keys_to_pascal_case(d):
    if isinstance(d, dict):
        new_dict = {}
        for k, v in d.items():
            new_key = ''.join(x.title() for x in k.split('_'))
            new_dict[new_key] = convert_keys_to_pascal_case(v)
        return new_dict
    elif isinstance(d, list):
        return [convert_keys_to_pascal_case(item) for item in d]
    else:
        return d


def extract_json_from_string(input_str):
    match = re.search(r'\{.*}', input_str, re.DOTALL)
    if match:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e}")
    return None


def execute_code_blocks(text):
    code_blocks = extract_python_code(text)
    for code in code_blocks:
        try:
            exec(code)
        except Exception as e:
            print(f"Error executing code block: {e}")


# è°ƒæ•´ç¼©è¿›,ä¿®å¤ä»£ç ç¼©è¿›ï¼Œç¡®ä¿æœ€å°çš„ç¼©è¿›è¢«ç§»é™¤ï¼Œä»¥é¿å…ç¼©è¿›é”™è¯¯
def fix_indentation(code):
    lines = code.splitlines()

    min_indent = min(len(line) - len(line.lstrip()) for line in lines if line.strip())
    fixed_lines = [line[min_indent:] if len(line) >= min_indent else line for line in lines]
    return "\n".join(fixed_lines)


def extract_python_code(text):
    code_blocks = re.findall(r'```(?:python)?(.*?)```', text, re.DOTALL)
    if not code_blocks:
        code_blocks = re.findall(r'((?: {4}.*\n)+)', text)

    return [fix_indentation(block) for block in code_blocks]  # [block.strip()]


def extract_sql_code(text):
    sql_blocks = re.findall(r'```(?:sql)?(.*?)```', text, re.DOTALL)
    if not sql_blocks:
        sql_blocks = re.findall(r'((?:SELECT|INSERT|UPDATE|DELETE).*?;)', text, re.DOTALL)
    return [block.strip() for block in sql_blocks]


def extract_html_code(text):
    html_blocks = re.findall(r'```(?:html)?(.*?)```', text, re.DOTALL)
    if not html_blocks:
        html_blocks = re.findall(r'(<html.*?</html>)', text, re.DOTALL | re.IGNORECASE)
    return [block.strip() for block in html_blocks]


def extract_cpp_code(text):
    cpp_blocks = re.findall(r'```(?:cpp|c\+\+)?(.*?)```', text, re.DOTALL)
    return [block.strip() for block in cpp_blocks]


def extract_java_code(text):
    java_blocks = re.findall(r'```(?:java)?(.*?)```', text, re.DOTALL)
    return [block.strip() for block in java_blocks]


def extract_bash_code(text):
    bash_blocks = re.findall(r'```(?:bash|sh)?(.*?)```', text, re.DOTALL)
    return [block.strip() for block in bash_blocks]


def extract_table_data(text):
    table_blocks = re.findall(r'```(?:table)?(.*?)```', text, re.DOTALL)
    if not table_blocks:
        table_blocks = re.findall(r'((?:\|.*?\|)+)', text)  # ç®€å•åŒ¹é… Markdown è¡¨æ ¼ï¼Œå¦‚ | A | B |
    return [block.strip() for block in table_blocks]


def extract_list_data(text):
    list_blocks = re.findall(r'```(?:list)?(.*?)```', text, re.DOTALL)
    if not list_blocks:
        list_blocks = re.findall(r'(\n\s*[-*].*?(\n\s{2,}.*?)*\n)', text)  # çº¯æ–‡æœ¬åˆ—è¡¨
    return [block.strip() for block in list_blocks]


def extract_json_data(text):
    # æå– JSON æ ¼å¼çš„ä»£ç å—
    json_blocks = re.findall(r'```(?:json)?(.*?)```', text, re.DOTALL)
    return [block.strip() for block in json_blocks]


def extract_code_blocks(text, lag=''):
    funcs = {
        "sql": extract_sql_code,
        "html": extract_html_code,
        "python": extract_python_code,
        "cpp": extract_cpp_code,
        "java": extract_java_code,
        "bash": extract_bash_code,

        "table": extract_table_data,
        "list": extract_list_data,
        "json": extract_json_data,
    }
    if lag in funcs:
        return funcs[lag](text)

    # æå– ``` åŒ…è£¹çš„ä»£ç å—
    code_blocks = re.findall(r'```(.*?)```', text, re.DOTALL)
    if lag:
        code_blocks = [block for block in code_blocks if block.startswith(lag)]
        return code_blocks  # è¿‡æ»¤å‡ºæŒ‡å®šè¯­è¨€çš„ä»£ç å—

    return {k: f(text) for k, f in funcs.items()}


def extract_jsons(input_str, n=None):
    # 1,None,-1
    matches = re.findall(r'\{.*?\}', input_str, re.DOTALL)
    if not matches:
        return None
    json_objects = []
    for match in matches:
        try:
            json_objects.append(json.loads(match))
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e} - Skipping this fragment: {match}")

    if not json_objects:
        return None

    return json_objects if n is None else json_objects[:n]


def extract_headers(text):
    # æå– ## æˆ– ### ç­‰æ ‡é¢˜
    headers = re.findall(r'^(#{1,6})\s+(.*)', text, re.MULTILINE)
    return [{'level': len(header[0]), 'text': header[1]} for header in headers]


def extract_links(text):
    # æå– Markdown æ ¼å¼çš„é“¾æ¥ [é“¾æ¥æ–‡å­—](é“¾æ¥åœ°å€)
    links = re.findall(r'\[([^\]]+)\]\((https?:\/\/[^\s]+)\)', text)
    return [{'text': link[0], 'url': link[1]} for link in links]


def extract_bold(text):
    # æå– Markdown æ ¼å¼çš„ **ç²—ä½“**
    bold_texts = re.findall(r'\*\*(.*?)\*\*', text)
    return bold_texts


def extract_italic(text):
    # æå– Markdown æ ¼å¼çš„ __æ–œä½“__ æˆ– *æ–œä½“*
    italic_texts = re.findall(r'__(.*?)__|\*(.*?)\*', text)
    return [italic[0] or italic[1] for italic in italic_texts]  # å¤„ç†ä¸¤ä¸ªæ•è·ç»„


def ordinal_generator():
    ordinals = ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©']
    for ordinal in ordinals:
        yield ordinal


def remove_markdown(text):
    # å»é™¤ Markdown çš„å¸¸è§æ ‡è®°
    """
    **ç²—ä½“æ–‡æœ¬**
    _æ–œä½“æ–‡æœ¬_
    ![å›¾ç‰‡æè¿°](image_url)
    [é“¾æ¥æ–‡æœ¬](url)
    ### æ ‡é¢˜æ–‡æœ¬
    > å¼•ç”¨å—
    * æ— åºåˆ—è¡¨é¡¹
    1. æœ‰åºåˆ—è¡¨é¡¹
    ~~åˆ é™¤çº¿æ–‡æœ¬~~
    __ä¸‹åˆ’çº¿æ–‡æœ¬__
    """
    text = re.sub(r'(`{1,3})(.*?)\1', r'\2', text)  # å»é™¤åå¼•å·ä»£ç å—
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # å»é™¤ç²—ä½“
    text = re.sub(r'\*(.*?)\*', r'\1', text)  # å»é™¤æ–œä½“
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)  # å»é™¤å›¾ç‰‡
    text = re.sub(r'\[.*?\]\((.*?)\)', r'\1', text)  # å»é™¤é“¾æ¥ï¼Œä½†ä¿ç•™ URL
    text = re.sub(r'#{1,6}\s*(.*)', r'\1', text)  # å»é™¤æ ‡é¢˜
    text = re.sub(r'>\s*(.*)', r'\1', text)  # å»é™¤å¼•ç”¨å—
    text = re.sub(r'(\*|-|\+)\s+(.*)', r'\2', text)  # å»é™¤æ— åºåˆ—è¡¨ç¬¦å·
    text = re.sub(r'\d+\.\s+(.*)', r'\1', text)  # å»é™¤æœ‰åºåˆ—è¡¨ç¬¦å·
    text = re.sub(r'~~(.*?)~~', r'\1', text)  # å»é™¤åˆ é™¤çº¿
    text = re.sub(r'_{2}(.*?)_{2}', r'\1', text)  # å»é™¤ä¸‹åˆ’çº¿æ ‡è®°
    text = re.sub(r'\[(.*?)\]\((.*?)\)', r'\1', text)  # å»é™¤é“¾æ¥å’Œ URL
    text = re.sub(r'\n{2,}', '\n', text)  # å°†å¤šä½™çš„ç©ºè¡Œæ›¿æ¢ä¸ºå•ä¸ªæ¢è¡Œç¬¦
    return text.strip()


def format_for_wechat(text):
    formatted_text = text
    formatted_text = re.sub(r'\*\*(.*?)\*\*', r'âœ¦\1âœ¦', formatted_text)  # **ç²—ä½“** è½¬æ¢ä¸º âœ¦ç²—ä½“âœ¦æ ·å¼
    formatted_text = re.sub(r'__(.*?)__', r'â€»\1â€»', formatted_text)  # __æ–œä½“__ è½¬æ¢ä¸ºæ˜Ÿå·åŒ…å›´çš„æ ·å¼
    formatted_text = re.sub(r'!!(.*?)!!', r'â—\1â—', formatted_text)  # !!é«˜äº®!! è½¬æ¢ä¸º â—ç¬¦å·åŒ…å›´
    formatted_text = re.sub(r'~~(.*?)~~', r'_\1_', formatted_text)  # ~~ä¸‹åˆ’çº¿~~ è½¬æ¢ä¸ºä¸‹åˆ’çº¿åŒ…å›´
    formatted_text = re.sub(r'\^\^(.*?)\^\^', r'||\1||', formatted_text)  # ^^é‡è¦^^ è½¬æ¢ä¸º ||é‡è¦|| åŒ…å›´
    formatted_text = re.sub(r'######\s+(.*?)(\n|$)', r'[\1]\n', formatted_text)  # ###### å…­çº§æ ‡é¢˜
    formatted_text = re.sub(r'#####\s+(.*?)(\n|$)', r'ã€Š\1ã€‹\n', formatted_text)  # ##### äº”çº§æ ‡é¢˜
    formatted_text = re.sub(r'####\s+(.*?)(\n|$)', r'ã€\1ã€‘\n', formatted_text)  # #### æ ‡é¢˜è½¬æ¢
    formatted_text = re.sub(r'###\s+(.*?)(\n|$)', r'â€” \1 â€”\n', formatted_text)  # ### ä¸‰çº§æ ‡é¢˜
    formatted_text = re.sub(r'##\s+(.*?)(\n|$)', r'â€”â€” \1 â€”â€”\n', formatted_text)  # ## äºŒçº§æ ‡é¢˜
    formatted_text = re.sub(r'#\s+(.*?)(\n|$)', r'â€”â€”â€” \1 â€”â€”â€”\n', formatted_text)  # # ä¸€çº§æ ‡é¢˜
    # formatted_text = re.sub(r'```([^`]+)```',
    #                         lambda m: '\n'.join([f'ï½œ {line}' for line in m.group(1).splitlines()]) + '\n',
    #                         formatted_text)
    # formatted_text = re.sub(r'`([^`]+)`', r'ã€Œ\1ã€', formatted_text)  # `ä»£ç ` è½¬æ¢ä¸ºã€Œä»£ç ã€æ ·å¼
    # formatted_text = re.sub(r'>\s?(.*)', r'ğŸ’¬ \1', formatted_text)  # > å¼•ç”¨æ–‡æœ¬ï¼Œè½¬æ¢ä¸ºèŠå¤©ç¬¦å·åŒ…å›´
    # formatted_text = re.sub(r'^\s*[-*+]\s+', 'â€¢ ', formatted_text, flags=re.MULTILINE)  # æ— åºåˆ—è¡¨é¡¹
    # formatted_text = re.sub(r'^\s*\d+\.\s+',f"{next(ordinal_iter)} ", formatted_text, flags=re.MULTILINE)  # æœ‰åºåˆ—è¡¨é¡¹
    formatted_text = re.sub(r'\n{2,}', '\n\n', formatted_text)  # è½¬æ¢æ¢è¡Œä»¥é¿å…å¤šä½™ç©ºè¡Œ

    return formatted_text.strip()


def extract_string(text, extract, **kwargs):
    if not extract:
        return None
    funcs = {
        "json": extract_json_from_string,
        "jsons": extract_jsons,
        "header": extract_headers,
        "links": extract_links,
        "bold": extract_bold,
        "italic": extract_italic,
        "wechat": format_for_wechat,
    }
    try:
        if extract in funcs:
            return funcs[extract](text, **kwargs)

        extract_type = extract.split('.')
        if extract_type[0] == 'code':
            transform = extract_code_blocks(text, lag=extract_type[1], **kwargs)
            return transform

        return {k: f(text, **kwargs) for k, f in funcs.items()}  # "type": "all"
    except Exception as e:
        print(e)

    return None


def dict_to_xml(tag, d):
    """å°†å­—å…¸è½¬æ¢ä¸º XML å­—ç¬¦ä¸²"""
    elem = ET.Element(tag)
    for key, val in d.items():
        child = ET.SubElement(elem, key)
        if isinstance(val, list):
            for item in val:
                item_elem = ET.SubElement(child, "item")
                item_elem.text = str(item)
        else:
            child.text = str(val)
    return ET.tostring(elem, encoding='unicode')


def list_to_xml(tag, lst):
    """å°†åˆ—è¡¨è½¬æ¢ä¸º XML å­—ç¬¦ä¸²"""
    elem = ET.Element(tag)
    for item in lst:
        item_elem = ET.SubElement(elem, "item")
        item_elem.text = str(item)
    return ET.tostring(elem, encoding='unicode')


def find_similar_word(target_keyword, tokens):
    max_ratio = 0
    similar_word_index = -1
    for i, token in enumerate(tokens):
        ratio = SequenceMatcher(None, target_keyword, token).ratio()
        if ratio > max_ratio:
            max_ratio = ratio
            similar_word_index = i
    return similar_word_index


def find_similar_words(query, tokens, top_n=3):
    matches = get_close_matches(query, tokens, n=top_n)
    # è®¡ç®—æ¯ä¸ªåŒ¹é…é¡¹ä¸æŸ¥è¯¢è¯çš„ç›¸ä¼¼åº¦
    results = []
    for match in matches:
        matcher = SequenceMatcher(None, query, match)
        results.append((match, matcher.ratio(), tokens.index(match)))

    return results


def contains_chinese(text):
    # æ£€æµ‹å­—ç¬¦ä¸²ä¸­æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
    chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
    return bool(chinese_pattern.search(text))


class BM25:
    def __init__(self, corpus, k1=1.5, b=0.75):
        self.k1 = k1
        self.b = b
        self.corpus = [jieba.lcut(doc) for doc in corpus]  # ä½¿ç”¨ jieba å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯
        self.doc_lengths = [len(doc) for doc in self.corpus]
        self.avg_doc_length = sum(self.doc_lengths) / len(self.doc_lengths)
        self.doc_count = len(self.corpus)
        self.doc_term_freqs = [Counter(doc) for doc in self.corpus]
        self.inverted_index = self.build_inverted_index()
        self.idf_cache = {}  # å¢åŠ ä¸€ä¸ª IDF ç¼“å­˜ï¼Œæé«˜æ•ˆç‡

    def build_inverted_index(self):
        inverted_index = {}
        for doc_id, doc_term_freq in enumerate(self.doc_term_freqs):
            for term, freq in doc_term_freq.items():
                if term not in inverted_index:
                    inverted_index[term] = []
                inverted_index[term].append((doc_id, freq))
        return inverted_index

    def idf(self, term):
        if term in self.idf_cache:
            return self.idf_cache[term]
        doc_freq = len(self.inverted_index.get(term, []))
        if doc_freq == 0:
            self.idf_cache[term] = 0
        else:
            self.idf_cache[term] = math.log((self.doc_count - doc_freq + 0.5) / (doc_freq + 0.5) + 1.0)
        return self.idf_cache[term]

    def bm25_score(self, query_terms, doc_id):
        score = 0
        doc_length = self.doc_lengths[doc_id]
        for term in query_terms:
            tf = self.doc_term_freqs[doc_id].get(term, 0)
            idf = self.idf(term)
            numerator = tf * (self.k1 + 1)
            denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avg_doc_length))
            score += idf * (numerator / denominator)
        return score

    def rank_documents(self, query):
        query_terms = list(jieba.cut(query))  # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        scores = [(doc_id, self.bm25_score(query_terms, doc_id)) for doc_id in range(self.doc_count)]
        return sorted(scores, key=lambda x: x[1], reverse=True)


# class BM25:
#     def __init__(self, corpus, k1=1.5, b=0.75):
#         self.k1 = k1
#         self.b = b
#         self.corpus = [list(jieba.cut(doc)) for doc in corpus]  # doc.split()
#         self.N = len(self.corpus)  # è¯­æ–™åº“ä¸­æ–‡æ¡£æ€»æ•°
#         self.avgdl = sum(len(doc) for doc in self.corpus) / self.N  # æ–‡æ¡£çš„å¹³å‡é•¿åº¦
#         self.df = self._calculate_df()  # æ¯ä¸ªè¯é¡¹çš„æ–‡æ¡£é¢‘ç‡
#         self.idf = self._calculate_idf()  # æ¯ä¸ªè¯é¡¹çš„é€†æ–‡æ¡£é¢‘ç‡
#
#     def _calculate_df(self):
#         """è®¡ç®—è¯é¡¹çš„æ–‡æ¡£é¢‘ç‡"""
#         df = {}
#         for doc in self.corpus:
#             unique_words = set(doc)
#             for word in unique_words:
#                 df[word] = df.get(word, 0) + 1
#         return df
#
#     def _calculate_idf(self):
#         """è®¡ç®—è¯é¡¹çš„é€†æ–‡æ¡£é¢‘ç‡"""
#         idf = {}
#         for word, freq in self.df.items():
#             idf[word] = math.log((self.N - freq + 0.5) / (freq + 0.5) + 1)
#         return idf
#
#     def _score(self, query, doc):
#         """è®¡ç®—å•ä¸ªæ–‡æ¡£å¯¹æŸ¥è¯¢çš„ BM25 å¾—åˆ†"""
#         score = 0.0
#         doc_len = len(doc)
#         term_frequencies = Counter(doc)
#         for word in query:
#             if word in term_frequencies:
#                 freq = term_frequencies[word]
#                 numerator = self.idf.get(word, 0) * freq * (self.k1 + 1)
#                 denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)
#                 score += numerator / denominator
#         return score
#
#     def get_scores(self, query):
#         """è®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªæ–‡æ¡£å¯¹æŸ¥è¯¢çš„ BM25 å¾—åˆ†"""
#         query = list(jieba.cut(query))  # ä½¿ç”¨ jieba å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
#         scores = []
#         for doc in self.corpus:
#             scores.append(self._score(query, doc))
#         return scores

if __name__ == "__main__":
    # from rank_bm25 import BM25Okapi
    jieba.initialize()
    # jieba.load_userdict('data/patent_thesaurus.txt')
    corpus = [
        "å¿«é€Ÿçš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†æ‡’ç‹—",
        "æ‡’ç‹—èººä¸‹äº†",
        "ç‹ç‹¸å¾ˆå¿«é€Ÿå¹¶ä¸”è·³å¾—å¾ˆé«˜",
        "å¿«é€Ÿçš„æ£•è‰²ç‹ç‹¸",
        "çŒ«è·³è¿‡äº†ç‹—"
    ]
    query = "å¿«é€Ÿçš„ç‹ç‹¸"
    bm25 = BM25(corpus)
    scores = bm25.rank_documents(query)
    print(scores, bm25.corpus)
    # print(BM25(corpus).rank_documents(query))
