import httpx, aiohttp, aiofiles, asyncio
from http import HTTPStatus
import oss2
from pathlib import PurePosixPath
from typing import List, Dict, Tuple, Any, Union, Callable, Awaitable, Generator, Optional
import random
from PIL import Image
from openai import OpenAI, AsyncOpenAI
# import qianfan
import dashscope
from dashscope.audio.tts import ResultCallback
from dashscope.audio.asr import Recognition, Transcription
# from lagent import tool_api
from functools import partial  # cache, lru_cache, partial, wraps, wraps
from config import *
from utils import *
from agents.ai_tools import *
from agents.ai_prompt import *

AI_Client = {}  # Dict[str, OpenAI]


async def get_data_for_model(model: dict):
    """è·å–æ¯ä¸ªæ¨¡å‹çš„æ•°æ®"""
    model_name = model.get('name')
    client = AI_Client.get(model_name)
    if client:
        try:
            models = await client.models.list()
            model['data'] = [m.model_dump() for m in models.data]
        except Exception as e:
            print(f"OpenAI {model_name} error occurred: {e}")


async def init_ai_clients(ai_models=AI_Models, api_keys=API_KEYS, get_data=False):
    tasks = []
    for model in ai_models:
        model_name = model.get('name')
        api_key = api_keys.get(model_name)
        if api_key:
            model['api_key'] = api_key
            if model_name not in AI_Client and model.get('supported_openai'):  # model_name in SUPPORTED_OPENAI_MODELS
                AI_Client[model_name]: AsyncOpenAI = AsyncOpenAI(api_key=api_key, base_url=model['base_url'])  # OpenAI
                if get_data:
                    tasks.append(get_data_for_model(model))

    if get_data:
        await asyncio.gather(*tasks)

    ModelListExtract.set()
    # print(len(ModelListExtract.get()))


# client = AI_Client['deepseek']
# print(dir(client.chat.completions))# 'create', 'with_raw_response', 'with_streaming_response'
# print(dir(client.completions))
# print(dir(client.embeddings))
# print(dir(client.files)) #'content', 'create', 'delete', 'list', 'retrieve', 'retrieve_content', 'wait_for_processing'


def find_ai_model(name, model_id: int = 0, search_field: str = 'model'):
    """
    åœ¨ AI_Models ä¸­æŸ¥æ‰¾æ¨¡å‹ã€‚å¦‚æœæ‰¾åˆ°åç§°åŒ¹é…çš„æ¨¡å‹ï¼Œè¿”å›æ¨¡å‹åŠå…¶ç±»å‹æˆ–å…·ä½“çš„å­æ¨¡å‹åç§°ã€‚

    å‚æ•°:
    - name: è¦æŸ¥æ‰¾çš„æ¨¡å‹åç§°
    - model_id: å¯é€‰å‚æ•°ï¼ŒæŒ‡å®šè¿”å›çš„å­æ¨¡å‹ç´¢å¼•ï¼Œé»˜è®¤ä¸º 0
    - search_field: è¦åœ¨å…¶ä¸­æŸ¥æ‰¾åç§°çš„å­—æ®µï¼ˆé»˜è®¤ä¸º 'model'ï¼‰
     è¿”å›:
    - Tuple[Dict[str, Any], Union[str, None]]: æ¨¡å‹åŠå…¶å¯¹åº”çš„å­æ¨¡å‹åç§°ï¼ˆæˆ– Noneï¼‰

    å¼‚å¸¸:
    - ValueError: å¦‚æœæœªæ‰¾åˆ°æ¨¡å‹
    """
    model = next(
        (item for item in AI_Models if item['name'] == name or name in item.get(search_field, [])),
        None
    )
    if model:
        model_items = model.get(search_field, [])
        if isinstance(model_items, list):
            if name in model_items:
                return model, name
            if model_items:
                model_i = model_id if abs(model_id) < len(model_items) else 0
                return model, model_items[model_i]
        elif isinstance(model_items, dict):
            if name in model_items:
                return model, model_items[name]
            # å¦‚æœæä¾›äº†åºå·ï¼Œè¿”å›åºå·å¯¹åº”çš„å€¼
            keys = list(model_items.keys())
            model_i = model_id if abs(model_id) < len(keys) else 0
            return model, model_items[keys[model_i]]

        return model, None

    raise ValueError(f"Model with name {name} not found.")
    # HTTPException(status_code=400, detail=f"Model with name {name} not found.")


async def ai_tool_response(messages, tools=None, model_name=Config.DEFAULT_MODEL, model_id=1,
                           top_p=0.95, temperature=0.01, **kwargs):
    """
      è°ƒç”¨ AI æ¨¡å‹æ¥å£ï¼Œä½¿ç”¨æä¾›çš„å·¥å…·é›†å’Œå¯¹è¯æ¶ˆæ¯ï¼Œè¿”å›æ¨¡å‹çš„å“åº”ã€‚qwen
        :return: æ¨¡å‹å“åº”çš„æ¶ˆæ¯å¯¹è±¡
    """
    model_info, name = find_ai_model(model_name, model_id)
    client = AI_Client.get(model_info['name'], None)
    # tools = [{"type": "web_search",}]
    if client:
        try:
            completion = await client.chat.completions.create(
                model=name,
                messages=messages,
                tools=tools,
                # tool_choice="auto",
                temperature=temperature,
                top_p=top_p,
                **kwargs,
            )
            return completion.choices[0].message
            # return completion.model_dump()
            # response['choices'][0]['message']
        except Exception as e:
            print(f"OpenAI error occurred: {e}")

    return None  # await ai_chat_post(model_info, payload)


def functions_registry(functions_list: list):
    # åˆ›å»ºå…¨å±€å‡½æ•°æ³¨å†Œè¡¨
    return {name: globals().get(name) for name in functions_list}


Function_Registry = {}


async def ai_tools_messages(response_message):
    """
    è§£ææ¨¡å‹å“åº”ï¼ŒåŠ¨æ€è°ƒç”¨å·¥å…·å¹¶ç”Ÿæˆåç»­æ¶ˆæ¯åˆ—è¡¨ã€‚

    :param response_message: æ¨¡å‹å“åº”çš„æ¶ˆæ¯å¯¹è±¡
    :return: åŒ…å«åŸå§‹å“åº”å’Œå·¥å…·è°ƒç”¨ç»“æœçš„æ¶ˆæ¯åˆ—è¡¨
    """
    messages = [response_message.to_dict()]  # ChatCompletionMessage
    tool_calls = response_message.tool_calls
    if not tool_calls:
        results = execute_code_blocks(response_message.content)
        # print(results)
        for res in results:
            messages.append({
                'role': 'tool',
                'content': res['error'] if res['error'] else res['output'],
                'tool_call_id': 'output.getvalue',
                'name': 'exec'
            })
        return messages

    global Function_Registry
    if not Function_Registry:  # åŠ¨æ€æ€§å»¶è¿ŸåŠ è½½,å…¨å±€æ³¨å†Œè¡¨åˆå§‹åŒ–
        Function_Registry = functions_registry(functions_list=[
            "get_times_shift", "get_weather", "web_search", "date_range_calculator",
            "get_day_range", "get_week_range", "get_month_range",
            "get_quarter_range", "get_year_range", "get_half_year_range",
            "search_bmap_location", "auto_translate"
            # æ·»åŠ æ›´å¤šå¯è°ƒç”¨å‡½æ•°
        ])

    for tool in tool_calls:
        func_name = tool.function.name  # function_name
        func_args = tool.function.arguments  # function_args = json.loads(tool_call.function.arguments)
        func_reg = Function_Registry.get(func_name, None)  # ä»æ³¨å†Œè¡¨ä¸­è·å–å‡½æ•°
        # print(func_args)
        # æ£€æŸ¥å¹¶è§£æ func_args ç¡®ä¿æ˜¯å­—å…¸
        if isinstance(func_args, str):
            try:
                func_args = json.loads(func_args)  # å°è¯•å°†å­—ç¬¦ä¸²è§£æä¸ºå­—å…¸
            except json.JSONDecodeError as e:
                messages.append({
                    "role": "tool",
                    "content": f"Error in {func_name}: Invalid arguments format ({str(e)}).",
                    "tool_call_id": tool.id,
                    'name': func_name
                })
                continue

        if not isinstance(func_args, dict):
            messages.append({
                "role": "tool",
                "content": f"Error in {func_name}: Arguments must be a mapping type, got {type(func_args).__name__}.",
                "tool_call_id": tool.id,
                'name': func_name
            })
            continue

        try:
            if func_reg:
                if inspect.iscoroutinefunction(func_reg):
                    func_out = await func_reg(**func_args)
                else:
                    func_out = func_reg(**func_args)
            else:
                func_name = extract_method_calls(func_name)
                # compile(code, '<string>', 'eval')
                # eval(expression, globals=None, locals=None)æ‰§è¡Œå•ä¸ªè¡¨è¾¾å¼,åŠ¨æ€è®¡ç®—ç®€å•çš„è¡¨è¾¾å¼æˆ–ä»å­—ç¬¦ä¸²ä¸­è§£æå€¼
                func_out = eval(f'{func_name}(**{func_args})')

            messages.append({
                'role': 'tool',
                'content': f'{func_out}',
                'tool_call_id': tool.id,
                'name': func_name
            })

        except Exception as e:
            error = f"Error in {func_name}: {str(e)}" if func_reg else f"Error: Function '{func_name}' not found."
            messages.append({
                'role': 'tool',
                'content': error,
                'tool_call_id': tool.id,
                'name': func_name
            })

            # print( f"Error in {func_name}: {str(e)}")
    return messages  # [*tool_mmessages,]


async def ai_auto_calls(question, **kwargs):
    messages = [{"role": "system", "content": System_content.get('31')},
                {"role": "user", "content": question}]
    response = await ai_tool_response(messages=messages, tools=AI_Tools, **kwargs)
    if response:
        final_messages = await ai_tools_messages(response)
        return [{msg['name']: msg['content']} for msg in final_messages if msg['role'] == "tool"]
    return []


async def ai_files_messages(files: List[str], question: str = None, model_name: str = 'qwen-long', model_id=-1,
                            **kwargs):
    """
    å¤„ç†æ–‡ä»¶å¹¶ç”Ÿæˆ AI æ¨¡å‹çš„å¯¹è¯ç»“æœã€‚

    :param files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    :param model_name: æ¨¡å‹åç§°
    :param model_id: æ¨¡å‹ ID
    :return: æ¨¡å‹ç”Ÿæˆçš„å¯¹è¯ç»“æœå’Œæ–‡ä»¶å¯¹è±¡åˆ—è¡¨
    """
    model_info, name = find_ai_model(model_name, model_id)
    client = AI_Client.get(model_info['name'], None)
    messages = []
    for file_path in files:
        file_path_obj = Path(file_path)
        if not file_path_obj.exists():  # .is_file()
            continue

        if client:  # client.files.create
            file_object = await client.files.create(file=file_path_obj, purpose="file-extract")
            if model_info['name'] == 'qwen':
                messages.append({"role": "system", "content": f"fileid://{file_object.id}", })
                # client.files.list()
                # æ–‡ä»¶ä¿¡æ¯client.files.retrieve(file_object.id)
                # æ–‡ä»¶å†…å®¹client.files.content(file_object.id)
            elif model_info['name'] == 'moonshot':
                file_content = await client.files.content(file_id=file_object.id)
                messages.append({"role": "system", "content": file_content.text, })
        else:
            dashscope_file_upload(messages, file_path=str(file_path_obj))

    if question:
        messages.append({"role": "user", "content": question})
        # print(messages)
        completion = await client.chat.completions.create(model=name, messages=messages, **kwargs)
        bot_response = completion.choices[0].message.content
        messages.append({"role": "assistant", "content": bot_response})
        return messages

    return messages


Embedding_Cache = {}


# https://www.openaidoc.com.cn/docs/guides/embeddings
async def ai_embeddings(inputs, model_name: str = 'qwen', model_id: int = 0, **kwargs) -> List[List[float]]:
    """
    text = text.replace("\n", " ")
    ä»è¿œç¨‹æœåŠ¡è·å–åµŒå…¥ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†å’Œç¼“å­˜å’Œå¤šæ¨¡å‹å¤„ç†ã€‚
    :param inputs: è¾“å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
    :param model_name: æ¨¡å‹åç§°
    :return: åµŒå…¥åˆ—è¡¨
    ï¼ˆ1ï¼‰æ–‡æœ¬æ•°é‡ä¸è¶…è¿‡ 16ã€‚
    ï¼ˆ2ï¼‰æ¯ä¸ªæ–‡æœ¬é•¿åº¦ä¸è¶…è¿‡ 512 ä¸ª tokenï¼Œè¶…å‡ºè‡ªåŠ¨æˆªæ–­ï¼Œtoken ç»Ÿè®¡ä¿¡æ¯ï¼Œtoken æ•° = æ±‰å­—æ•°+å•è¯æ•°*1.3 ï¼ˆä»…ä¸ºä¼°ç®—é€»è¾‘ï¼Œä»¥å®é™…è¿”å›ä¸ºå‡†)ã€‚
    ï¼ˆ3ï¼‰ æ‰¹é‡æœ€å¤š 16 ä¸ªï¼Œè¶…è¿‡ 16 åé»˜è®¤æˆªæ–­ã€‚
    """
    if not model_name:
        return []
    if not inputs:
        return []

    global Embedding_Cache
    cache_key = generate_hash_key(inputs, model_name, model_id)
    if cache_key in Embedding_Cache:
        # print(cache_key)
        return Embedding_Cache[cache_key]

    try:
        model_info, name = find_ai_model(model_name, model_id, 'embedding')
    except:
        return []

    batch_size = 16  # DASHSCOPE_MAX_BATCH_SIZE = 25
    has_error = False
    client = AI_Client.get(model_info['name'], None)
    if client:  # openai.Embedding.create
        if isinstance(inputs, list) and len(inputs) > batch_size:
            tasks = [client.embeddings.create(
                model=name, input=inputs[i:i + batch_size],
                encoding_format="float",
                **kwargs
            ) for i in range(0, len(inputs), batch_size)]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            embeddings = [None] * len(inputs)
            input_idx = 0
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"Error encountered: {result}")
                    has_error = True
                    continue
                for idx, item in enumerate(result.data):
                    embeddings[input_idx] = item.embedding
                    input_idx += 1

            if not has_error:  # not  any(embedding is None for embedding in embeddings):
                Embedding_Cache[cache_key] = embeddings

        else:
            # await asyncio.to_thread(client.embeddings.create
            completion = await client.embeddings.create(
                model=name, input=inputs, encoding_format="float")

            embeddings = [item.embedding for item in completion.data]
            # data = json.loads(completion.model_dump_json()

        return embeddings

    # dashscope.TextEmbedding.call
    url = model_info['embedding_url']
    api_key = model_info['api_key']
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    payload = {
        "input": inputs,
        "model": name,
        "encoding_format": "float"
    }
    payload.update(kwargs)
    embeddings = []
    try:
        async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
            if isinstance(inputs, list) and len(inputs) > batch_size:
                for i in range(0, len(inputs), batch_size):
                    batch = inputs[i:i + batch_size]
                    payload["input"] = batch  # {"texts":batch}
                    response = await cx.post(url, headers=headers, json=payload)
                    response.raise_for_status()
                    data = response.json().get('data')  # "output"
                    if data and len(data) == len(batch):
                        embeddings += [emb.get('embedding') for emb in data]
                    else:
                        print(f"Error: Batch {i // batch_size + 1} response size mismatch.")
                        has_error = True
            else:
                response = await cx.post(url, headers=headers, json=payload)
                response.raise_for_status()
                data = response.json().get('data')
                embeddings = [emb.get('embedding') for emb in data]

    except (httpx.HTTPStatusError, httpx.RequestError) as exc:
        print(exc)

    if not has_error:
        Embedding_Cache[cache_key] = embeddings

    return embeddings


async def ai_reranker(query: str, documents: List[str], top_n: int, model_name="BAAI/bge-reranker-v2-m3", model_id=0,
                      **kwargs):
    if not model_name:
        return []
    try:
        model_info, name = find_ai_model(model_name, model_id, 'reranker')
    except:
        return []
    url = model_info['reranker_url']
    api_key = model_info['api_key']
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    payload = {
        "query": query,
        "model": name,
        "documents": documents,
        "top_n": top_n,
        "return_documents": False,
        # 'max_chunks_per_doc': 1024,  # æœ€å¤§å—æ•°
        # 'overlap_tokens': 80,  # é‡å æ•°é‡
    }
    payload.update(kwargs)
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        response = await cx.post(url, headers=headers, json=payload)
        if response.status_code == 200:
            results = response.json().get('results')
            matches = [(match["document"]["text"] if match.get("document") else documents[match["index"]],
                        match["relevance_score"], match["index"]) for match in results]
            return matches
        else:
            print(response.text)
    return []


# ç”Ÿæˆ:conversation or summary,Fill-In-the-Middle
async def ai_generate(prompt: str, user_request: str = '', suffix: str = None, stream=False, temperature=0.7,
                      max_tokens=4096, model_name='silicon', model_id=0, get_content=True, **kwargs):
    '''
    Completionsè¶³ä»¥è§£å†³å‡ ä¹ä»»ä½•è¯­è¨€å¤„ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬å†…å®¹ç”Ÿæˆã€æ‘˜è¦ã€è¯­ä¹‰æœç´¢ã€ä¸»é¢˜æ ‡è®°ã€æƒ…æ„Ÿåˆ†æç­‰ç­‰ã€‚
    éœ€è¦æ³¨æ„çš„ä¸€ç‚¹é™åˆ¶æ˜¯ï¼Œå¯¹äºå¤§å¤šæ•°æ¨¡å‹ï¼Œå•ä¸ªAPIè¯·æ±‚åªèƒ½åœ¨æç¤ºå’Œå®Œæˆä¹‹é—´å¤„ç†æœ€å¤š4096ä¸ªæ ‡è®°ã€‚
    '''
    model_info, name = find_ai_model(model_name, model_id, "generation")
    if not name:
        return await ai_chat(model_info=None, messages=None, user_request=user_request, system=prompt,
                             temperature=temperature, max_tokens=max_tokens, top_p=0.8, model_name=model_name,
                             model_id=model_id, get_content=get_content, **kwargs)

    if user_request:
        prompt += '\n\n' + user_request

    if model_info['name'] == 'qwen':
        response = dashscope.Generation.call(model=name, prompt=prompt)
        return response.output.text

    client = AI_Client.get(model_info['name'], None)
    # client.completions.create
    response = await client.completions.create(
        # engine=name,
        model=name,
        prompt=prompt,
        suffix=suffix,
        max_tokens=max_tokens,
        temperature=temperature,
        stream=stream,
        stop=None,
        **kwargs,
        # n=1,
    )
    if stream:
        async def stream_data():
            async for chunk in response:
                yield chunk.choices[0].text if get_content else chunk.model_dump_json()

        return stream_data()

    return response.choices[0].text.strip() if get_content else response.model_dump()


async def request_ollama(prompt, model_name="mistral", stream=False):
    url = "http://localhost:11434/api/generate"
    payload = {"model": model_name, "prompt": prompt, "stream": stream}

    async with aiohttp.ClientSession() as session:
        response = await session.post(url, json=payload)
        return response  # ç›´æ¥è¿”å›å“åº”å¯¹è±¡


async def call_ollama(prompt, model_name="mistral", stream=True):
    response = await request_ollama(prompt, model_name=model_name, stream=stream)

    async def stream_data():
        async for line in response.content:
            line = line.strip()
            if line:
                try:
                    data = json.loads(line.decode('utf-8'))
                    yield data
                    # print(data.get("response", ""), end="", flush=True)  # å®æ—¶æ‰“å° AI ç”Ÿæˆçš„æ–‡æœ¬
                except json.JSONDecodeError:
                    pass  # å¿½ç•¥ JSON è§£æé”™è¯¯

    try:
        return stream_data() if stream else await response.json()
    finally:
        await response.release()


def messages_to_prompt(messages):
    """ å°† OpenAI-style messages è½¬æ¢ä¸º Ollama çš„ prompt """
    prompt = ""
    for msg in messages:
        role = "User" if msg["role"] == "user" else "Assistant"
        prompt += f"{role}: {msg['content']}\n"
    prompt += "Assistant:"
    return prompt


async def openai_llama_chat_completions(messages, stream=True, **kwargs):
    """
      æ¨¡æ‹Ÿ OpenAI çš„è¿”å›ç»“æ„
      - å¦‚æœ stream=Trueï¼Œæ¨¡æ‹Ÿæµå¼è¿”å›
      - å¦‚æœ stream=Falseï¼Œè¿”å›å®Œæ•´ JSON å“åº”
    """
    prompt = messages_to_prompt(messages)
    response = await call_ollama(prompt, model_name="mistral", stream=stream)
    if stream:
        async def stream_data():
            async for chunk in response:
                content = chunk.get("response", "")
                data = {"id": "chatcmpl-xyz", "object": "chat.completion.chunk", "created": int(time.time()),
                        "model": "gpt-4o",
                        "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]},
                yield f"data: {json.dumps(data)}\n\n"  # (f"data: {data}\n\n").encode("utf-8")

        return stream_data()

    completion = response.get("response", "")
    return {
        "id": f"chatcmpl-{int(time.time())}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": "gpt-4o",
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": completion,
                },
                "finish_reason": "stop",
            }
        ],
        "usage": {
            "prompt_tokens": len(prompt),
            "completion_tokens": len(completion),  # .split()
            "total_tokens": len(prompt) + len(completion),
        },
    }


#
# async def create_completion(
#         messages: List[Dict[str, str]],
#         refresh_token: str,
#         model: str = Config.DEFAULT_MODEL,
#         retry_count: int = 0
# ) -> Dict:
#     """åŒæ­¥å¯¹è¯è¡¥å…¨
#
#     Args:
#         messages: æ¶ˆæ¯åˆ—è¡¨
#         refresh_token: åˆ·æ–°token
#         model: æ¨¡å‹åç§°
#         retry_count: é‡è¯•æ¬¡æ•°
#
#     Returns:
#         Dict: è¡¥å…¨ç»“æœ
#
#     Raises:
#         API_REQUEST_PARAMS_INVALID: å‚æ•°æ— æ•ˆ
#     """
#     try:
#         if not messages:
#             raise  Exception("æ¶ˆæ¯ä¸èƒ½ä¸ºç©º")
#
#         # è§£ææ¨¡å‹å‚æ•°
#         model_info = parse_model(model)
#
#         # ç”Ÿæˆå›¾åƒ
#         image_urls = generate_images(
#             model=model_info['model'],
#             prompt=messages[-1]['content'],
#             width=model_info['width'],
#             height=model_info['height'],
#             refresh_token=refresh_token
#         )
#
#         # æ„é€ è¿”å›ç»“æœ
#         return {
#             'id': generate_uuid(),
#             'model': model or model_info['model'],
#             'object': 'chat.completion',
#             'choices': [{
#                 'index': 0,
#                 'message': {
#                     'role': 'assistant',
#                     'content': ''.join(f'![image_{i}]({url})\n' for i, url in enumerate(image_urls))
#                 },
#                 'finish_reason': 'stop'
#             }],
#             'usage': {
#                 'prompt_tokens': 1,
#                 'completion_tokens': 1,
#                 'total_tokens': 2
#             },
#             'created':  int(time.time())
#         }
#     except Exception as e:
#         if retry_count < Config.MAX_RETRY_COUNT:
#             print(f"Response error: {str(e)}")
#             print(f"Try again after {Config.RETRY_DELAY}s...")
#             await asyncio.sleep(Config.RETRY_DELAY)
#             return await create_completion(messages, refresh_token, model, retry_count + 1)
#         raise e

#
# async def create_completion_stream(
#         messages: List[Dict[str, str]],
#         refresh_token: str,
#         model: str = Config.DEFAULT_MODEL,
#         retry_count: int = 0
# ) -> Generator[Dict, None, None]:
#     """æµå¼å¯¹è¯è¡¥å…¨
#
#     Args:
#         messages: æ¶ˆæ¯åˆ—è¡¨
#         refresh_token: åˆ·æ–°token
#         model: æ¨¡å‹åç§°
#         retry_count: é‡è¯•æ¬¡æ•°
#
#     Yields:
#         Dict: è¡¥å…¨ç»“æœç‰‡æ®µ
#     """
#     try:
#         if not messages:
#             yield {
#                 'id': generate_uuid(),
#                 'model': model,
#                 'object': 'chat.completion.chunk',
#                 'choices': [{
#                     'index': 0,
#                     'delta': {'role': 'assistant', 'content': 'æ¶ˆæ¯ä¸ºç©º'},
#                     'finish_reason': 'stop'
#                 }]
#             }
#             return
#
#         # è§£ææ¨¡å‹å‚æ•°
#         model_info = parse_model(model)
#
#         # å‘é€å¼€å§‹ç”Ÿæˆæ¶ˆæ¯
#         yield {
#             'id': generate_uuid(),
#             'model': model or model_info['model'],
#             'object': 'chat.completion.chunk',
#             'choices': [{
#                 'index': 0,
#                 'delta': {'role': 'assistant', 'content': 'ğŸ¨ å›¾åƒç”Ÿæˆä¸­ï¼Œè¯·ç¨å€™...'},
#                 'finish_reason': None
#             }]
#         }
#
#         try:
#             # ç”Ÿæˆå›¾åƒ
#             image_urls = generate_images(
#                 model=model_info['model'],
#                 prompt=messages[-1]['content'],
#                 width=model_info['width'],
#                 height=model_info['height'],
#                 refresh_token=refresh_token
#             )
#
#             # å‘é€å›¾åƒURL
#             for i, url in enumerate(image_urls):
#                 yield {
#                     'id': generate_uuid(),
#                     'model': model or model_info['model'],
#                     'object': 'chat.completion.chunk',
#                     'choices': [{
#                         'index': i + 1,
#                         'delta': {
#                             'role': 'assistant',
#                             'content': f'![image_{i}]({url})\n'
#                         },
#                         'finish_reason': None if i < len(image_urls) - 1 else 'stop'
#                     }]
#                 }
#
#             # å‘é€å®Œæˆæ¶ˆæ¯
#             yield {
#                 'id': generate_uuid(),
#                 'model': model or model_info['model'],
#                 'object': 'chat.completion.chunk',
#                 'choices': [{
#                     'index': len(image_urls) + 1,
#                     'delta': {
#                         'role': 'assistant',
#                         'content': 'å›¾åƒç”Ÿæˆå®Œæˆï¼'
#                     },
#                     'finish_reason': 'stop'
#                 }]
#             }
#
#         except Exception as e:
#             # å‘é€é”™è¯¯æ¶ˆæ¯
#             yield {
#                 'id': generate_uuid(),
#                 'model': model or model_info['model'],
#                 'object': 'chat.completion.chunk',
#                 'choices': [{
#                     'index': 1,
#                     'delta': {
#                         'role': 'assistant',
#                         'content': f'ç”Ÿæˆå›¾ç‰‡å¤±è´¥: {str(e)}'
#                     },
#                     'finish_reason': 'stop'
#                 }]
#             }
#     except Exception as e:
#         if retry_count < Config.MAX_RETRY_COUNT:
#             print(f"Response error: {str(e)}")
#             print(f"Try again after {Config.RETRY_DELAY}s...")
#             await asyncio.sleep(Config.RETRY_DELAY)
#             async for chunk in create_completion_stream(messages, refresh_token, model, retry_count + 1):
#                 yield chunk
#             return
#         raise e


def agent_func_calls(agent):
    from knowledge import ideatech_knowledge
    function_agent = {
        'default': lambda *args, **kwargs: [],
        '2': web_search,  # web_search_async
        '30': ideatech_knowledge,
        '32': ai_auto_calls
    }
    return function_agent.get(agent, lambda *args, **kwargs: [])


def async_to_sync(func, *args, **kwargs):
    return asyncio.run(func(*args, **kwargs))


async def wrap_sync(func, *args, **kwargs):
    return await asyncio.to_thread(func, *args, **kwargs)


# retriever
async def retrieved_reference(user_request: str, keywords: List[Union[str, Tuple[str, Any]]] = None,
                              tool_calls: List[Callable[[...], Any]] = None, **kwargs):
    # docs = retriever(question)
    # Assume this is the document retrieved from RAG
    # function_call = Agent_Functions.get(agent, lambda *args, **kwargs: [])
    # refer = function_call(user_message, ...)
    tool_calls = tool_calls or []
    items_to_process = []
    tasks = []  # asyncio.create_task
    if not keywords:
        items_to_process = [user_request]  # ','.join(keywords)
    else:
        if all(not (callable(func) and func.__name__ == '<lambda>' and func()) for func in tool_calls):
            tool_calls.append(ai_auto_calls)  # not in agent_funcalls web_search_async

        function_registry = {'map_search': search_amap_location,
                             'web_search': web_search_async,
                             'translate': baidu_translate,
                             'auto_calls': ai_auto_calls}

        for item in keywords:
            if isinstance(item, tuple) and len(item) > 1:
                func = function_registry.get(item[0])  # user_calls å‡½æ•°
                if func:
                    func_args = item[1:]  # å‰©ä¸‹çš„å‚æ•°
                    if inspect.iscoroutinefunction(func):
                        tasks.append(func(*func_args))
                    else:
                        tasks.append(wrap_sync(func, *func_args))
            else:  # isinstance(keyword, str)
                items_to_process.append(item)  # keyword

    for func in filter(callable, tool_calls):
        if func.__name__ == '<lambda>' and func() == []:  # empty_lambda
            continue
        for item in items_to_process:
            if inspect.iscoroutinefunction(func):
                tasks.append(func(item, **kwargs))
            else:
                tasks.append(wrap_sync(func, item, **kwargs))

    refer = await asyncio.gather(*tasks, return_exceptions=True)  # gather æ”¶é›†æ‰€æœ‰å¼‚æ­¥è°ƒç”¨çš„ç»“æœ

    for f, r in zip(tasks, refer):
        if not r:
            print(f.__name__, r)
    return [item for result in refer if not isinstance(result, Exception)
            for item in (result.items() if isinstance(result, dict) else result)]  # å±•å¹³åµŒå¥—ç»“æœ


# Callable[[å‚æ•°ç±»å‹], è¿”å›ç±»å‹]
async def get_chat_payload(messages, user_request: str, system: str = '', temperature: float = 0.4, top_p: float = 0.8,
                           max_tokens: int = 1024, model_name=Config.DEFAULT_MODEL, model_id=0,
                           tool_calls: List[Callable[[...], Any]] = None,
                           keywords: List[Union[str, Tuple[str, Any]]] = None, images: List[str] = None, **kwargs):
    model_info, name = find_ai_model(model_name, model_id, 'model')
    model_type = model_info['type']

    if isinstance(messages, list) and messages:
        if model_type in ('baidu', 'tencent'):
            if messages[0].get('role') == 'system':  # ['system,assistant,user,tool,function']
                system = messages[0].get('content')
                del messages[0]

            # the role of first message must be user
            if messages[0].get('role') != 'user':  # userï¼ˆtoolï¼‰
                messages.insert(0, {'role': 'user', 'content': user_request or 'è¯·é—®æ‚¨æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ'})

            # ç¡®ä¿ user å’Œ assistant æ¶ˆæ¯äº¤æ›¿å‡ºç°
            for i, message in enumerate(messages[:-1]):
                # if (
                #     isinstance(message, dict) and
                #     message.get("role") in ["user", "assistant"] and
                #     isinstance(message.get("content"), str) and
                #     message["content"].strip()  # ç¡®ä¿ content éç©º
                # ):
                next_message = messages[i + 1]
                if message['role'] == next_message['role']:  # messages.insert(0, messages.pop(i))
                    if i % 2 == 0:
                        if message['role'] == 'user':
                            messages.insert(i + 1, {'role': 'assistant', 'content': 'è¿™æ˜¯ä¸€ä¸ªé»˜è®¤çš„å›ç­”ã€‚'})
                        else:
                            messages.insert(i + 1, {'role': 'user', 'content': 'è¯·é—®æ‚¨æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ'})
                    else:
                        del messages[i + 1]

        if model_type != 'baidu' and system:
            if messages[0].get('role') != 'system':
                messages.insert(0, {"role": "system", "content": system})
            # messages[-1]['content'] = messages[0]['content'] + '\n' + messages[-1]['content']

        if user_request:
            if messages[-1]["role"] != 'user':
                messages.append({'role': 'user', 'content': user_request})
            else:
                pass
                # if messages[-1]["role"] == 'user':
                #     messages[-1]['content'] = user_request
        else:
            if messages[-1]["role"] == 'user':
                user_request = messages[-1]["content"]
    else:
        if messages is None:
            messages = []
        if model_type != 'baidu' and system:  # system_message
            messages = [{"role": "system", "content": system}]
        messages.append({'role': 'user', 'content': user_request})

    refer = await retrieved_reference(user_request, keywords, tool_calls, **kwargs)
    if refer:
        formatted_refer = '\n'.join(map(str, refer))
        # """Answer the users question using only the provided information below:{docs}""".format(docs=formatted_refer)
        messages[-1]['content'] = (f'ä»¥ä¸‹æ˜¯ç›¸å…³å‚è€ƒèµ„æ–™:\n{formatted_refer}\n'
                                   f'è¯·ç»“åˆä»¥ä¸Šå†…å®¹æˆ–æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œé’ˆå¯¹ä¸‹é¢çš„é—®é¢˜è¿›è¡Œè§£ç­”ï¼š\n{user_request}')

    if images:  # å›¾ç‰‡å†…å®¹ç†è§£
        messages[-1]['content'] = [{"type": "text", "text": user_request}]  # text-prompt è¯·è¯¦ç»†æè¿°ä¸€ä¸‹è¿™å‡ å¼ å›¾ç‰‡ã€‚è¿™æ˜¯å“ªé‡Œï¼Ÿ
        messages[-1]['content'] += [{"type": "image_url", "image_url": {"url": image}} for image in images]

    payload = dict(
        model=name,  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡å‹
        messages=messages,
        temperature=temperature,
        top_p=top_p,
        # top_k=50,
        max_tokens=max_tokens,
        # extra_body = {"prefix": "```python\n", "suffix":"åç¼€å†…å®¹"} å¸Œæœ›çš„å‰ç¼€å†…å®¹,åŸºäºç”¨æˆ·æä¾›çš„å‰ç¼€ä¿¡æ¯æ¥è¡¥å…¨å…¶ä½™çš„å†…å®¹
        # response_format={"type": "json_object"}
        # "tools":retrievalã€web_searchã€function
    )
    if model_type == 'baidu':
        payload['system'] = system
    # if model_info['name']=='baichuan':
    #     extra_body = {
    #         "tools": [{
    #             "type": "retrieval",
    #             "retrieval": {
    #                 "kb_ids": [
    #                     "kb-123",
    #                      "kb-xxx"
    #                 ],
    #           "answer_mode": "knowledge-base-only"
    #             }
    #         }]
    #     }
    #     [{
    #         "type": "web_search",
    #         "web_search": {
    #             "enable": True,
    #             "search_mode": "performance_first"#"quality_first"
    #         }
    #     }]

    # print(payload)
    return model_info, payload, refer


def get_chat_payload_post(model_info, payload):
    # é€šè¿‡ requests åº“ç›´æ¥å‘èµ· HTTP POST è¯·æ±‚
    url = model_info['url']
    api_key = model_info['api_key']
    headers = {'Content-Type': 'application/json', }
    payload = payload.deepcopy()
    if api_key:
        if isinstance(api_key, list):
            idx = model_info['model'].index(payload["model"])
            api_key = model_info['api_key'][idx]

        headers = {'Content-Type': 'application/json',
                   "Authorization": f'Bearer {api_key}'
                   }
    if model_info['type'] == 'baidu':  # 'ernie'
        url = build_url(f'{url}{payload["model"]}',
                        get_baidu_access_token(Config.BAIDU_qianfan_API_Key,
                                               Config.BAIDU_qianfan_Secret_Key))  # ?access_token=" + get_access_token()
        # print(url)
        # payload.pop('model', None)
        payload['max_output_tokens'] = payload.pop('max_tokens', 1024)
        # payload['enable_system_memory'] = False
        # payload["enable_citation"]= False
        # payload["disable_search"] = False
        # payload["user_id"]=
        # payload["user_ip"]=
        # payload['system'] = system
    if model_info['type'] == 'tencent':
        service = 'hunyuan'
        host = url.split("//")[-1]
        payload = convert_keys_to_pascal_case(payload)
        payload.pop('MaxTokens', None)
        headers = get_tencent_signature(service, host, payload, action='ChatCompletions',
                                        secret_id=Config.TENCENT_SecretId,
                                        secret_key=Config.TENCENT_Secret_Key)
        headers['X-TC-Version'] = '2023-09-01'
        # headers["X-TC-Region"] = 'ap-shanghai'

    # if model_info['name'] == 'silicon':
    #     headers = {
    #         "accept": "application/json",
    #         "content-type": "application/json",
    #         "authorization": "Bearer sk-tokens"
    #     }
    return url, headers, payload


async def ai_chat(model_info, payload=None, get_content=True, **kwargs):
    if not payload:
        model_info, payload, _ = await get_chat_payload(**kwargs)
    else:
        payload.update(kwargs)  # {**payload, **kwargs}

    client = AI_Client.get(model_info['name'], None)
    if client:
        try:
            # await asyncio.to_thread(client.chat.completions.create, **payload)
            completion = await client.chat.completions.create(**payload)
            return completion.choices[0].message.content if get_content else completion.model_dump()  # è‡ªåŠ¨åºåˆ—åŒ–ä¸º JSON
            # json.loads(completion.model_dump_json())
        except Exception as e:
            return f"OpenAI error occurred: {e}"

    url, headers, payload = get_chat_payload_post(model_info, payload)
    # print(headers, payload)
    limits = httpx.Limits(max_connections=100, max_keepalive_connections=10)
    parse_rules = {
        'baidu': lambda d: d.get('result'),
        'tencent': lambda d: d.get('Response', {}).get('Choices', [{}])[0].get('Message', {}).get('Content'),
        # d.get('Choices')[0].get('Message').get('Content')
        'default': lambda d: d.get('choices', [{}])[0].get('message', {}).get('content')
    }

    try:
        async with httpx.AsyncClient(limits=limits, timeout=Config.HTTP_TIMEOUT_SEC) as cx:
            response = await cx.post(url, headers=headers, json=payload)
            response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
            data = response.json()
            if get_content:
                result = parse_rules.get(model_info['type'], parse_rules['default'])(data)
                if result:
                    return result
                print(response.text)
            return data

    except Exception as e:
        # print(response.text)
        return f"HTTP error occurred: {e}"


async def ai_chat_async(model_info, payload=None, get_content=True, **kwargs):
    if not payload:
        model_info, payload, _ = await get_chat_payload(**kwargs)
    else:
        # payload=copy(payload)
        payload.update(kwargs)

    payload["stream"] = True
    # payload["stream_options"]= {"include_usage": True}        # å¯é€‰ï¼Œé…ç½®ä»¥åä¼šåœ¨æµå¼è¾“å‡ºçš„æœ€åä¸€è¡Œå±•ç¤ºtokenä½¿ç”¨ä¿¡æ¯
    client = AI_Client.get(model_info['name'], None)
    # print(payload)
    if client:
        try:
            stream = await client.chat.completions.create(**payload)
            async for chunk in stream:  # for chunk in stream
                if get_content:
                    delta = chunk.choices[0].delta
                    if delta.content:  # ä»¥ä¸¤ä¸ªæ¢è¡Œç¬¦ \n\n ç»“æŸå½“å‰ä¼ è¾“çš„æ•°æ®å—
                        yield delta.content  # completion.append(delta.content)
                else:
                    yield chunk.model_dump_json()  # è·å–å­—èŠ‚æµæ•°æ®
        except Exception as e:
            yield f"OpenAI error occurred: {e}"
        # yield '[DONE]'
        return  # å¼‚æ­¥ç”Ÿæˆå™¨çš„ç»“æŸæ— éœ€è¿”å›å€¼

    url, headers, payload = get_chat_payload_post(model_info, payload)
    limits = httpx.Limits(max_connections=100, max_keepalive_connections=10)
    try:
        async with httpx.AsyncClient(limits=limits) as cx:
            async with cx.stream("POST", url, headers=headers, json=payload) as response:
                response.raise_for_status()
                async for content in process_line_stream(response, model_info['type']):
                    # print(chunk["choices"][0]["delta"].get("content", ""), end="", flush=True)
                    yield content
    except httpx.RequestError as e:
        yield str(e)

    # yield "[DONE]"


async def process_line_stream(response, model_type='default'):
    data = ""
    async for line in response.aiter_lines():
        # print(line)
        line = line.strip()
        if len(line) == 0:  # å¼€å¤´çš„è¡Œ not line
            if data:
                yield process_data_chunk(data, model_type)  # ä¸€ä¸ªæ•°æ®å—çš„ç»“æŸ yield from + '\n'
                data = ""
            continue
        if line.startswith("data: "):
            line_data = line.lstrip("data: ")
            if line_data == "[DONE]":
                # if data:
                #     yield process_data_chunk(data, model_type)
                # print(data)
                break
            if model_type == 'tencent':
                chunk = json.loads(line_data)
                reason = chunk.get('Choices', [{}])[0].get('FinishReason')
                if reason == "stop":
                    # raise StopIteration(2)
                    break
            elif model_type == 'baidu':
                chunk = json.loads(line_data)
                if chunk.get('is_end') is True:
                    break

            content = process_data_chunk(line_data, model_type)
            if content:
                yield content
        else:
            data += "\n" + line

    if data:
        yield process_data_chunk(data, model_type)


def process_data_chunk(data, model_type='default'):
    try:
        chunk = json.loads(data)

        if model_type == 'baidu':  # line.decode("UTF-8")
            return chunk.get("result") or chunk.get("error_msg")
        else:
            if model_type == 'tencent':
                chunk = convert_keys_to_lower_case(chunk)

            choices = chunk.get('choices', [])
            if choices:
                delta = choices[0].get('delta', {})
                return delta.get("content")

    except (json.JSONDecodeError, KeyError, IndexError) as e:
        return str(e)
    return None


async def forward_stream(response):
    async for line in response.iter_lines(decode_unicode=True):
        if not line:
            continue
        if line.startswith("data: "):
            line_data = line.lstrip("data: ")
            if line_data == "[DONE]":
                break
            try:
                parsed_content = json.loads(line_data)
                yield {"json": parsed_content}
            except json.JSONDecodeError:
                yield {"text": line_data}


def web_search(text: str, api_key: str = Config.GLM_Service_Key) -> list:
    """
    é€šè¿‡æä¾›çš„æŸ¥è¯¢æ–‡æœ¬æ‰§è¡Œç½‘ç»œæœç´¢ã€‚

    :param text: æŸ¥è¯¢æ–‡æœ¬ã€‚
    :param api_key: ç”¨äºè®¿é—®ç½‘ç»œæœç´¢å·¥å…·çš„APIå¯†é’¥ï¼ˆé»˜è®¤ä¸ºé…ç½®ä¸­çš„å¯†é’¥ï¼‰ã€‚
    :return: æœç´¢ç»“æœåˆ—è¡¨æˆ–é”™è¯¯ä¿¡æ¯ã€‚
    """
    msg = [{"role": "user", "content": text}]
    url = "https://open.bigmodel.cn/api/paas/v4/tools"
    data = {
        "request_id": str(uuid.uuid4()),
        "tool": "web-search-pro",
        "stream": False,
        "messages": msg
    }

    headers = {'Authorization': api_key}
    try:
        response = requests.post(url, json=data, headers=headers, timeout=Config.HTTP_TIMEOUT_SEC)
        response.raise_for_status()

        data = response.json()
        search_result = data.get('choices', [{}])[0].get('message', {}).get('tool_calls', [{}])[1].get('search_result')
        if search_result:
            return [{
                'title': result.get('title'),
                'content': result.get('content'),
                'link': result.get('link'),
                'media': result.get('media')
            } for result in search_result]
        return [{'content': response.content.decode()}]
    except (requests.exceptions.RequestException, KeyError, IndexError) as e:
        return [{'error': str(e)}]


async def web_search_async(text: str, api_key: str = Config.GLM_Service_Key) -> list:
    msg = [{"role": "user", "content": text}]
    tool = "web-search-pro"
    url = "https://open.bigmodel.cn/api/paas/v4/tools"
    data = {
        "request_id": str(uuid.uuid4()),
        "tool": tool,
        "stream": False,
        "messages": msg
    }

    headers = {'Authorization': api_key}

    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        try:
            response = await cx.post(url, json=data, headers=headers)
            response.raise_for_status()

            data = response.json()
            results = data['choices'][0]['message']['tool_calls'][1]['search_result']
            return [{
                'title': result.get('title'),
                'content': result.get('content'),
                'link': result.get('link'),
                'media': result.get('media')
            } for result in results]

            # return resp.text  # resp.content.decode()
        except (httpx.HTTPStatusError, httpx.RequestError) as exc:
            return [{'error': str(exc)}]
        # finally:
        #     await response.close()

        # https://portal.azure.com/#home


def bing_search(query, bing_api_key):
    url = f"https://api.bing.microsoft.com/v7.0/search?q={query}"
    headers = {"Ocp-Apim-Subscription-Key": bing_api_key}
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    search_results = response.json()

    results = []
    for result in search_results.get('webPages', {}).get('value', []):
        title = result.get('name')
        snippet = result.get('snippet')
        link = result.get('url')
        results.append((title, snippet, link))
    return results  # "\n".join([f"{i+1}. {title}: {snippet} ({link})" for i, (title, snippet, link) in enumerate(search_results[:5])])


# https://ziyuan.baidu.com/fastcrawl/index
def baidu_search(query, baidu_api_key, baidu_secret_key):
    access_token = get_baidu_access_token(baidu_api_key, baidu_secret_key)
    search_url = "https://aip.baidubce.com/rest/2.0/knowledge/v1/search"  # https://aip.baidubce.com/rpc/2.0/unit/service/v3/chat
    headers = {"Content-Type": "application/x-www-form-urlencoded"}
    search_params = {
        "access_token": access_token,
        "query": query,
        "scope": 1,  # æœç´¢èŒƒå›´
        "page_no": 1,
        "page_size": 5
    }
    search_response = requests.post(search_url, headers=headers, data=search_params)

    if search_response.status_code == 401:  # å¦‚æœtokenå¤±æ•ˆ
        global baidu_access_token
        baidu_access_token = None
        access_token = get_baidu_access_token(baidu_api_key, baidu_secret_key)

        search_params["access_token"] = access_token
        search_response = requests.post(search_url, headers=headers, data=search_params)

    search_response.raise_for_status()

    search_results = search_response.json()
    results = []
    for result in search_results.get('result', []):
        title = result.get('title')
        content = result.get('content')
        url = result.get('url')
        results.append((title, content, url))
    return results  # "\n".join([f"{i+1}. {title}: {content} ({url})" for i, (title, content, url) in enumerate(search_results[:5])])


def wikipedia_search(query):
    response = requests.get(f"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={query}&format=json",
                            timeout=Config.HTTP_TIMEOUT_SEC, proxies=Config.HTTP_Proxies)
    search_results = response.json().get('query', {}).get('search', [])
    if search_results:
        page_id = search_results[0]['pageid']
        page_response = requests.get(
            f"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&pageids={page_id}&format=json")
        page_data = page_response.json()['query']['pages'][str(page_id)]
        return page_data.get('extract', 'No extract found.')
    return "No information found."


Assistant_Cache = {}


# https://platform.baichuan-ai.com/docs/assistants
async def ai_assistant_create(instructions: str, user_name: str, tools_type="code_interpreter", model_id=4):
    # å¦‚æœæ‚¨åˆ¤æ–­æ˜¯ä¸€æ¬¡è¿ç»­çš„å¯¹è¯ï¼Œåˆ™æ— éœ€è‡ªå·±æ‹¼æ¥ä¸Šä¸‹æ–‡ï¼Œåªéœ€å°†æœ€æ–°çš„ message æ·»åŠ åˆ°å¯¹åº”çš„ thread id å³å¯å¾—åˆ°åŒ…å«äº† thread id å†å²ä¸Šä¸‹æ–‡çš„å›å¤ï¼Œå†å²ä¸Šä¸‹æ–‡è¶…è¿‡æˆ‘ä»¬ä¼šå¸®æ‚¨è‡ªåŠ¨æˆªæ–­ã€‚
    global Assistant_Cache
    cache_key = generate_hash_key(instructions, user_name, tools_type, model_id)
    if cache_key in Assistant_Cache:
        return Assistant_Cache[cache_key]

    model_info, model_name = find_ai_model('baichuan', model_id, 'model')
    assistants_url = f"{model_info['base_url']}assistants"
    threads_url = f"{model_info['base_url']}threads"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {model_info['api_key']}"
    }
    payload = {
        "instructions": instructions,
        "name": user_name,
        "tools": [{"type": tools_type}],  # web_search,code_interpreter,function
        "model": model_name
    }
    try:
        async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
            assistant_response = await cx.post(assistants_url, headers=headers, json=payload)
            assistant_response.raise_for_status()
            threads_response = await cx.post(threads_url, headers=headers, json={})
            threads_response.raise_for_status()
            results = {'headers': headers,
                       'assistant_data': assistant_response.json(),
                       'threads_url': threads_url,
                       'threads_id': threads_response.json()['id']}
            Assistant_Cache[cache_key] = results  # assistant_response.json()["id"],assistant_id
            return results
    except Exception as e:
        return {"error": f"Unexpected error: {str(e)}"}


async def ai_assistant_run(user_request: str, instructions: str, user_name: str, tools_type="code_interpreter",
                           model_id=4, max_retries=20, interval=5, backoff_factor=1.5):
    assistant = await ai_assistant_create(instructions, user_name, tools_type, model_id)
    if "error" in assistant:
        return assistant

    assistant_data = assistant['assistant_data']
    headers = assistant['headers']
    messages_url = f'{assistant["threads_url"]}/{assistant["threads_id"]}/messages'
    threads_url = f'{assistant["threads_url"]}/{assistant["threads_id"]}/runs'

    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        messages_response = await cx.post(messages_url, headers=headers, json={"role": "user", "content": user_request})
        messages_response.raise_for_status()
        run_response = await cx.post(threads_url, headers=headers, json={"assistant_id": assistant_data['id']})
        run_response.raise_for_status()
        run_id = run_response.json().get('id')
        run_url = f'{threads_url}/{run_id}'
        retries = 0
        while retries < max_retries:
            try:
                run_status_response = await cx.get(run_url, headers=headers)  # å®šæœŸæ£€ç´¢ run id çš„è¿è¡ŒçŠ¶æ€
                # print(retries,run_status_response.status_code, run_status_response.headers, run_status_response.text)

                run_status_response.raise_for_status()
                status_data = run_status_response.json()

                if status_data.get('status') == 'completed':
                    messages_status_response = await cx.get(messages_url, headers=headers)
                    run_status_response.raise_for_status()
                    status_data = messages_status_response.json()
                    # print(status_data)
                    return status_data.get('data')

                await asyncio.sleep(interval)
                retries += 1

            except httpx.HTTPStatusError as http_error:
                if run_status_response.status_code == 429:
                    await asyncio.sleep(interval)
                    retries += 1
                    interval *= backoff_factor
                    continue

                return {"error": f"HTTP error: {str(http_error)}"}  # status_data['error']

            except Exception as e:
                return {"error": f"Unexpected error: {str(e)}"}

        return {
            "error": f"Timeout: The task did not complete within the expected time. status:{status_data},retries:{retries}"}


# ï¼ˆ1ï¼‰æ–‡æœ¬æ•°é‡ä¸è¶…è¿‡ 16ã€‚ ï¼ˆ2ï¼‰æ¯ä¸ªæ–‡æœ¬é•¿åº¦ä¸è¶…è¿‡ 512 ä¸ª tokenï¼Œè¶…å‡ºè‡ªåŠ¨æˆªæ–­ï¼Œtoken ç»Ÿè®¡ä¿¡æ¯ï¼Œtoken æ•° = æ±‰å­—æ•°+å•è¯æ•°*1.3 ï¼ˆä»…ä¸ºä¼°ç®—é€»è¾‘ï¼Œä»¥å®é™…è¿”å›ä¸ºå‡†)ã€‚
async def get_baidu_embeddings(texts: List[str], access_token: str, model_name='bge_large_zh') -> List:
    if not isinstance(texts, list):
        texts = [texts]

    url = f"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/embeddings/{model_name}?access_token={access_token}"
    headers = {
        'Content-Type': 'application/json',
        # "Authorization: Bearer $BAICHUAN_API_KEY"
    }
    batch_size = 16
    embeddings = []
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        if len(texts) < batch_size:
            payload = json.dumps({"input": texts})  # "model":
            response = await cx.post(url, headers=headers, data=payload)
            data = response.json().get('data')
            # if len(texts) == 1:
            #     return data[0].get('embedding') if data else None
            embeddings = [d.get('embedding') for d in data]
        else:
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                payload = {"input": batch}
                try:
                    response = await cx.post(url, headers=headers, json=payload)
                    response.raise_for_status()
                    data = response.json().get('data')
                    if data and len(data) == len(batch):
                        embeddings += [d.get('embedding') for d in data]
                except (httpx.HTTPStatusError, httpx.RequestError) as exc:
                    print(exc)

    return embeddings


def get_hf_embeddings(texts, model_name='BAAI/bge-large-zh-v1.5', access_token=Config.HF_Service_Key):
    # "https://api-inference.huggingface.co/models/BAAI/bge-reranker-large"
    url = f"https://api-inference.huggingface.co/models/{model_name}"
    headers = {"Authorization": f'Bearer {access_token}'}
    payload = {"inputs": texts}
    response = requests.post(url, headers=headers, json=payload, proxies=Config.HTTP_Proxies,
                             timeout=Config.HTTP_TIMEOUT_SEC)
    data = response.json().get('data')
    return [emb.get('embedding') for emb in data]


async def similarity_embeddings(query, tokens: List[str], filter_idx: List[int] = None, tokens_vector=None,
                                embeddings_calls: Callable[[...], Any] = ai_embeddings, **kwargs):
    """
    è®¡ç®—æŸ¥è¯¢ä¸ä¸€ç»„æ ‡è®°ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚
    å‚æ•°:
        query (str): æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚
        tokens (List[str]): è¦æ¯”è¾ƒçš„æ ‡è®°åˆ—è¡¨ã€‚
        filter_idx (List[int], optional): è¦è¿‡æ»¤å‡ºçš„æ ‡è®°ç´¢å¼•ã€‚é»˜è®¤ä¸º `None`ï¼Œè¡¨ç¤ºä¸è¿›è¡Œè¿‡æ»¤ã€‚
        tokens_vector (np.ndarray, optional): é¢„è®¡ç®—çš„æ ‡è®°å‘é‡æ•°ç»„ã€‚é»˜è®¤ä¸º `None`ã€‚
        embeddings_calls (Callable, optional): ç”¨äºç”ŸæˆåµŒå…¥çš„å¼‚æ­¥å‡½æ•°ï¼Œé»˜è®¤ä¸º `None`ã€‚
        **kwargs: ä¼ é€’ç»™ `embeddings_calls` çš„é¢å¤–å‚æ•°ã€‚

    è¿”å›:
        np.ndarray: ç›¸ä¼¼åº¦å¾—åˆ†æ•°ç»„ï¼ˆä¸ `tokens` é•¿åº¦ä¸€è‡´ï¼‰ã€‚
    """
    if filter_idx is None:
        filter_idx = list(range(len(tokens)))
    else:
        filter_idx = [i for i in filter_idx if i < len(tokens)]
    similarity = np.full(len(tokens), np.nan)
    if not query:
        return similarity

    tokens_idx = [(i, token) for i, token in enumerate(tokens) if token]
    query_vector = await embeddings_calls(query, **kwargs)
    if tokens_vector is None:
        # list(np.array(idx_tokens)[:, 1])
        tokens_vector = await embeddings_calls([token for _, token in tokens_idx], **kwargs)

    matching_indices = np.isin([j[0] for j in tokens_idx], filter_idx)
    filter_embeddings = np.array(tokens_vector)[matching_indices]

    if len(filter_embeddings):
        # cosine_similarity_np(np.array(query_vector).reshape(1, -1), filter_embeddings).T
        sim_2d = np.array(query_vector).reshape(1, -1) @ filter_embeddings.T
        # matching_indices = np.isin(filter_idx, [j[0] for j in tokens_idx])
        # similarity[matching_indices] = sim_2d.reshape(-1)
        for idx, score in zip(filter_idx, sim_2d.flatten()):
            similarity[idx] = score

    return similarity


def get_similar_vectors(querys, data, exclude: List[str] = None, topn: int = 10, cutoff: float = 0.0):
    '''
    data={
      "name": ["word1", "word2"],
      "vectors": [[0.1, 0.2, ...], [0.3, 0.4, ...]]
    }
    è¿”å›ï¼š
        List[Tuple[str, List[Tuple[str, float]]]]: æŸ¥è¯¢è¯ä¸ç›¸ä¼¼æ ‡è®°åŠå…¶åˆ†æ•°çš„æ˜ å°„ã€‚
    '''
    index_to_key = data['name']
    vectors = np.array(data['vectors'])

    # è·å–ç´¢å¼•
    query_mask = np.array([w in index_to_key for w in querys])
    exclude_mask = np.array([w in querys + exclude for w in index_to_key])
    # np.delete(vectors, exclude_indices, axis=0)

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    sim_matrix = cosine_similarity_np(vectors[query_mask], vectors[~exclude_mask].T)

    results = []
    for i, w in enumerate(querys):
        if not query_mask[i]:
            continue
        sim_scores = sim_matrix[i]
        if topn > 0:
            top_indices = np.argsort(sim_scores)[::-1][:topn]  # è·å–å‰ topn ä¸ªç´¢å¼•
        else:
            top_indices = np.arange(sim_scores.shape[0])  # è·å–å…¨éƒ¨ç´¢å¼•ï¼Œä¸æ’åº
        top_scores = sim_scores[top_indices]

        valid_indices = top_scores > cutoff  # ä¿ç•™å¤§äº cutoff çš„ç›¸ä¼¼åº¦
        top_words = [index_to_key[j] for j in np.where(~exclude_mask)[0][top_indices[valid_indices]]]
        top_scores = top_scores[valid_indices]
        results.append((w, list(zip(top_words, top_scores))))

    return results  # [(w, list(sim[w].sort_values(ascending=False)[:topn].items())) for w in querys]


async def get_similar_embeddings(querys: List[str], tokens: List[str],
                                 embeddings_calls: Callable[[...], Any] = ai_embeddings, topn=10, **kwargs):
    """
    ä½¿ç”¨åµŒå…¥è®¡ç®—æŸ¥è¯¢ä¸æ ‡è®°ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚
    :param querys: æŸ¥è¯¢è¯åˆ—è¡¨
    :param tokens: æ¯”è¾ƒè¯åˆ—è¡¨
    :param embeddings_calls: åµŒå…¥ç”Ÿæˆå‡½æ•°
    :param topn: è¿”å›ç›¸ä¼¼ç»“æœçš„æ•°é‡
    :param kwargs: å…¶ä»–å‚æ•°
    è¿”å›ï¼š
        List[Tuple[str, List[Tuple[str, float]]]]: æŸ¥è¯¢è¯ä¸ç›¸ä¼¼æ ‡è®°åŠå…¶åˆ†æ•°çš„æ˜ å°„ã€‚
    """
    query_vector, tokens_vector = await asyncio.gather(
        embeddings_calls(querys, **kwargs),
        embeddings_calls(tokens, **kwargs))

    if not query_vector or not tokens_vector:
        return []

    sim_matrix = np.array(query_vector) @ np.array(tokens_vector).T
    results = []
    for i, w in enumerate(querys):
        sim_scores = sim_matrix[i]
        top_indices = np.argsort(sim_scores)[::-1][:topn] if topn > 0 else np.arange(sim_scores.shape[0])
        top_scores = sim_scores[top_indices]
        top_match = [tokens[j] for j in top_indices]
        results.append((w, list(zip(top_match, top_scores, top_indices))))

    return results  # [(q,[(match,score,index),])]


async def find_closest_matches_embeddings(querys, tokens,
                                          embeddings_calls: Callable[[...], Any] = ai_embeddings, **kwargs):
    """
    ä½¿ç”¨åµŒå…¥è®¡ç®—æŸ¥è¯¢ä¸æ ‡è®°ä¹‹é—´çš„æœ€è¿‘åŒ¹é…,æ‰¾åˆ°æ¯ä¸ªæŸ¥è¯¢çš„æœ€ä½³åŒ¹é…æ ‡è®°ã€‚
    è¿”å›ï¼š
        Dict[str, Tuple[str, float]]: æŸ¥è¯¢ä¸æœ€è¿‘åŒ¹é…æ ‡è®°çš„æ˜ å°„å­—å…¸ã€‚
    """
    matchs = {x: (x, 1.0) for x in querys if x in tokens}
    unmatched_queries = list(set(querys) - matchs.keys())
    if not unmatched_queries:
        return matchs
    query_vector, tokens_vector = await asyncio.gather(
        embeddings_calls(unmatched_queries, **kwargs),
        embeddings_calls(tokens, **kwargs))

    sim_matrix = np.array(query_vector) @ np.array(tokens_vector).T
    closest_matches = tokens[sim_matrix.argmax(axis=1)]  # idxmax
    closest_scores = sim_matrix.max(axis=1)
    matchs.update(zip(unmatched_queries, zip(closest_matches, closest_scores)))
    return matchs


def is_city(city, region='å…¨å›½'):
    # https://restapi.amap.com/v3/geocode/geo?parameters
    response = requests.get(url="http://api.map.baidu.com/place/v2/suggestion",
                            params={'query': city, 'region': region,
                                    "output": "json", "ak": Config.BMAP_API_Key, })
    data = response.json()

    # åˆ¤æ–­è¿”å›ç»“æœä¸­æ˜¯å¦æœ‰åŸå¸‚åŒ¹é…
    for result in data.get('result', []):
        if result.get('city') == city:
            return True
    return False


def get_bmap_location(address, city=''):
    response = requests.get(url="https://api.map.baidu.com/geocoding/v3",
                            params={"address": address,
                                    "city": city,
                                    "output": "json",
                                    "ak": Config.BMAP_API_Key, })
    if response.status_code == 200:
        locat = response.json()['result']['location']
        return round(locat['lng'], 6), round(locat['lat'], 6)
    else:
        print(response.text)
    return None, None


# https://lbsyun.baidu.com/faq/api?title=webapi/place-suggestion-api
async def search_bmap_location(query, region='', limit=True):
    url = "http://api.map.baidu.com/place/v2/suggestion"  # 100
    params = {
        "query": query,
        "region": region,
        "city_limit": 'true' if (region and limit) else 'false',
        "output": "json",
        "ak": Config.BMAP_API_Key,
    }

    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
        response = await client.get(url, params=params)
        res = []
        if response.status_code == 200:
            js = response.json()
            for result in js.get('result', []):
                res.append({'lng_lat': (round(result['location']['lng'], 6), round(result['location']['lat'], 6)),
                            'name': result["name"], 'address': result['address']})
        else:
            print(response.text)
        return res  # baidu_nlp(nlp_type='address', text=region+query+result["name"]+ result['address'])


def get_amap_location(address, city=''):
    response = requests.get(url="https://restapi.amap.com/v3/geocode/geo?parameters",
                            params={"address": address,
                                    "city": city,
                                    "output": "json",
                                    "key": Config.AMAP_API_Key, })

    if response.status_code == 200:
        js = response.json()
        if js['status'] == '1':
            s1, s2 = js['geocodes'][0]['location'].split(',')
            return float(s1), float(s2)  # js['geocodes'][0]['formatted_address']
    else:
        print(response.text)

    return None, None


# https://lbs.amap.com/api/webservice/guide/api-advanced/search
async def search_amap_location(query, region='', limit=True):
    url = "https://restapi.amap.com/v5/place/text?parameters"  # 100
    params = {
        "keywords": query,
        "region": region,
        "city_limit": 'true' if (region and limit) else 'false',
        "output": "json",
        "key": Config.AMAP_API_Key,
    }

    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
        response = await client.get(url, params=params)
        res = []
        if response.status_code == 200:
            js = response.json()
            if js['status'] == '1' and int(js['count']) > 0:
                for result in js.get('pois', []):
                    s1, s2 = result['location'].split(',')
                    res.append({'lng_lat': (float(s1), float(s2)),
                                'name': result["name"], 'address': result['address']})
            else:
                print(response.text)
        return res


# https://console.bce.baidu.com/ai/#/ai/machinetranslation/overview/index
async def baidu_translate(text: str, from_lang: str = 'zh', to_lang: str = 'en', trans_type='texttrans'):
    """ç™¾åº¦ç¿»è¯‘ API"""
    salt = str(random.randint(32768, 65536))  # str(int(time.time() * 1000))
    sign_str = Config.BAIDU_trans_AppId + text + salt + Config.BAIDU_trans_Secret_Key
    sign = md5_sign(sign_str)  # éœ€è¦è®¡ç®— sign = MD5(appid+q+salt+å¯†é’¥)
    url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
    lang_map = {'fa': 'fra', 'ja': 'jp', 'ar': 'ara', 'ko': 'kor', 'es': 'spa', 'zh-TW': 'cht', 'vi': 'vie'}

    if from_lang in lang_map.keys():
        from_lang = lang_map[from_lang]
    if to_lang in lang_map.keys():
        to_lang = lang_map[to_lang]

    if to_lang == 'auto':
        to_lang = 'zh'

    params = {
        "q": text,
        "from": from_lang,
        "to": to_lang,
        "appid": Config.BAIDU_trans_AppId,
        "salt": salt,
        "sign": sign
    }

    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
        response = await client.get(url, params=params)

    data = response.json()
    if "trans_result" in data:
        return data["trans_result"][0]["dst"]

    print(response.text)
    # texttrans-with-dict
    url = f"https://aip.baidubce.com/rpc/2.0/mt/{trans_type}/v1?access_token=" + get_baidu_access_token(
        Config.BAIDU_translate_API_Key, Config.BAIDU_translate_Secret_Key)
    headers = {
        'Content-Type': 'application/json',
        'Accept': 'application/json'
    }
    payload = json.dumps({
        "from": from_lang,
        "to": to_lang
    })
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
        response = await client.post(url, headers=headers, json=payload)

    data = response.json()
    if "trans_result" in data:
        return data["trans_result"][0]["dst"]

    # print(response.text)
    return {'error': data.get('error_msg', 'Unknown error')}


# https://cloud.tencent.com/document/product/551/15619
async def tencent_translate(text: str, source: str, target: str):
    payload = {
        "SourceText": text,
        "Source": source,
        "Target": target,
        "ProjectId": 0
    }
    url = "https://tmt.tencentcloudapi.com"
    headers = get_tencent_signature(service="tmt", host="tmt.tencentcloudapi.com", body=payload,
                                    action='TextTranslate',
                                    secret_id=Config.TENCENT_SecretId, secret_key=Config.TENCENT_Secret_Key,
                                    timestamp=int(time.time()), version='2018-03-21')

    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
        response = await client.post(url, headers=headers, json=payload)

    # æ£€æŸ¥å“åº”çŠ¶æ€ç å’Œå†…å®¹
    if response.status_code != 200:
        print(f"Error: Received status code {response.status_code},Response content: {response.text}")
        return {'error': f'{response.status_code},Request failed'}

    try:
        data = response.json()
    except Exception as e:
        print(f"Failed to decode JSON: {e},Response text: {response.text}")
        return {'error': f"Failed to decode JSON: {e},Response text: {response.text}"}

    if "Response" in data and "TargetText" in data["Response"]:
        return data["Response"]["TargetText"]
    else:
        print(f"Unexpected response: {data}")
        return {'error': f"Tencent API Error: {data.get('Response', 'Unknown error')}"}


# https://www.xfyun.cn/doc/nlp/xftrans/API.html
async def xunfei_translate(text: str, source: str = 'en', target: str = 'cn'):
    # å°†æ–‡æœ¬è¿›è¡Œbase64ç¼–ç 
    encoded_text = base64.b64encode(text.encode('utf-8')).decode('utf-8')

    # æ„é€ è¯·æ±‚æ•°æ®
    request_data = {
        "header": {
            "app_id": Config.XF_AppID,  # ä½ åœ¨å¹³å°ç”³è¯·çš„appid
            "status": 3,
            # "res_id": "your_res_id"  # å¯é€‰ï¼šè‡ªå®šä¹‰æœ¯è¯­èµ„æºid
        },
        "parameter": {
            "its": {
                "from": source,
                "to": target,
                "result": {}
            }
        },
        "payload": {
            "input_data": {
                "encoding": "utf8",
                "status": 3,
                "text": encoded_text
            }
        }
    }

    headers, url = get_xfyun_authorization(api_key=Config.XF_API_Key, api_secret=Config.XF_Secret_Key,
                                           host="itrans.xf-yun.com", path="/v1/its", method='POST')
    url = 'https://itrans.xf-yun.com/v1/its'

    # å¼‚æ­¥å‘é€è¯·æ±‚
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
        response = await client.post(url, json=request_data, headers=headers)
        if response.status_code == 200:
            response_data = response.json()

            # è§£ç è¿”å›ç»“æœä¸­çš„textå­—æ®µ
            if "payload" in response_data and "result" in response_data["payload"]:
                base64_text = response_data["payload"]["result"]["text"]
                decoded_result = base64.b64decode(base64_text).decode('utf-8')
                data = json.loads(decoded_result)
                if "trans_result" in data:
                    return data["trans_result"]["dst"]
            else:
                return {"error": "Unexpected response format"}
        else:
            return {"error": f"HTTP Error: {response.status_code}"}


# https://docs.caiyunapp.com/lingocloud-api/
def caiyun_translate(source, direction="auto2zh"):
    url = "http://api.interpreter.caiyunai.com/v1/translator"

    # WARNING, this token is a test token for new developers,
    token = Config.CaiYun_Token

    payload = {
        "source": source,
        "trans_type": direction,
        "request_id": "demo",
        "detect": True,
    }

    headers = {
        "content-type": "application/json",
        "x-authorization": "token " + token,
    }

    response = requests.request("POST", url, data=json.dumps(payload), headers=headers)
    return json.loads(response.text)["target"]


# https://ai.youdao.com/
# https://hcfy.ai/docs/services/youdao-api
async def auto_translate(text: str, model_name='baidu', source: str = 'auto', target: str = 'auto'):
    """
       è‡ªåŠ¨ç¿»è¯‘å‡½æ•°ï¼Œæ ¹æ®è¾“å…¥çš„æ–‡æœ¬å’ŒæŒ‡å®šçš„ç¿»è¯‘æ¨¡å‹è‡ªåŠ¨å®Œæˆè¯­è¨€æ£€æµ‹å’Œç¿»è¯‘ã€‚
        åŠŸèƒ½æè¿°:
    1. è‡ªåŠ¨æ£€æµ‹æºè¯­è¨€ï¼Œå¦‚æœ `source` ä¸º "auto"ï¼š
        - ä½¿ç”¨ `detect` æ–¹æ³•æ£€æµ‹è¯­è¨€ã€‚
        - å¦‚æœæ£€æµ‹ç»“æœæ˜¯ä¸­æ–‡ï¼Œæ ‡å‡†åŒ–ä¸º "zh"ã€‚
        - å¦‚æœæ£€æµ‹ç»“æœä¸æ˜ç¡®ï¼Œé»˜è®¤æ ¹æ®å†…å®¹æ˜¯å¦åŒ…å«ä¸­æ–‡è®¾å®šè¯­è¨€ã€‚
    2. è‡ªåŠ¨è®¾å®šç›®æ ‡è¯­è¨€ï¼Œå¦‚æœ `target` ä¸º "auto"ï¼š
        - å¦‚æœæºè¯­è¨€æ˜¯ä¸­æ–‡ï¼Œåˆ™ç›®æ ‡è¯­è¨€ä¸ºè‹±æ–‡ ("en")ã€‚
        - å¦‚æœæºè¯­è¨€æ˜¯å…¶ä»–è¯­è¨€ï¼Œåˆ™ç›®æ ‡è¯­è¨€ä¸ºä¸­æ–‡ ("zh")ã€‚
    3. æ ¹æ®æŒ‡å®šçš„ `model_name` è°ƒç”¨å¯¹åº”çš„ç¿»è¯‘æ¨¡å‹å¤„ç†æ–‡æœ¬ã€‚
        - å¦‚æœæœªæ‰¾åˆ°å¯¹åº”æ¨¡å‹ï¼Œåˆ™è°ƒç”¨ `ai_generate` ç”Ÿæˆç¿»è¯‘ã€‚
    4. è¿”å›ç¿»è¯‘ç»“æœåŠç›¸å…³ä¿¡æ¯ã€‚
    """
    if source == 'auto':
        source = detect(text)
        if source == 'zh-cn':
            source = 'zh'
        if source == 'no':
            source = 'zh' if contains_chinese(text) else 'auto'
    if target == 'auto':
        target = 'en' if source == 'zh' else 'zh'

    translate_map: dict = {
        "baidu": baidu_translate,
        "tencent": tencent_translate,
        "xunfei": xunfei_translate
    }
    error = ''
    handler = translate_map.get(model_name)
    if handler:
        translated_text = await handler(text, source, target)
        if isinstance(translated_text, str):
            return {"translated": translated_text, 'from': source, 'to': target, "model": model_name}

        error = translated_text.get('error')

    system_prompt = System_content.get('9').format(source_language=source, target_language=target)

    model_map = {"baidu": 'ernie', "tencent": "hunyuan", "xunfei": 'spark'}
    if model_name in model_map.keys():
        model_name = model_map[model_name]
    if not model_name:
        model_name = 'qwen'

    translated_text = await ai_generate(
        prompt=system_prompt,
        user_request=text,
        model_name=model_name,
        model_id=0,
        stream=False,
    )
    if translated_text:
        return {"translated": translated_text, 'from': source, 'to': target, "model": model_name}

    return {"translated": error, 'from': source, 'to': target, "model": model_name}


def xunfei_ppt_theme(industry, style="ç®€çº¦", color="è“è‰²", appid: str = Config.XF_AppID,
                     api_secret: str = Config.XF_Secret_Key):
    url = "https://zwapi.xfyun.cn/api/ppt/v2/template/list"
    timestamp = int(time.time())
    signature = get_xfyun_signature(appid, api_secret, timestamp)
    headers = {
        "appId": appid,
        "timestamp": str(timestamp),
        "signature": signature,
        "Content-Type": "application/json; charset=utf-8"
    }
    # body ={
    #     "query": text,
    #     "templateId": templateId  # æ¨¡æ¿IDä¸¾ä¾‹ï¼Œå…·ä½“ä½¿ç”¨ /template/list æŸ¥è¯¢
    # }
    body = {
        "payType": "not_free",
        "style": style,  # æ”¯æŒæŒ‰ç…§ç±»å‹æŸ¥è¯¢PPT æ¨¡æ¿,é£æ ¼ç±»å‹ï¼š "ç®€çº¦","å¡é€š","å•†åŠ¡","åˆ›æ„","å›½é£","æ¸…æ–°","æ‰å¹³","æ’ç”»","èŠ‚æ—¥"
        "color": color,  # æ”¯æŒæŒ‰ç…§é¢œè‰²æŸ¥è¯¢PPT æ¨¡æ¿,é¢œè‰²ç±»å‹ï¼š "è“è‰²","ç»¿è‰²","çº¢è‰²","ç´«è‰²","é»‘è‰²","ç°è‰²","é»„è‰²","ç²‰è‰²","æ©™è‰²"
        "industry": industry,
        # æ”¯æŒæŒ‰ç…§é¢œè‰²æŸ¥è¯¢PPT æ¨¡æ¿,è¡Œä¸šç±»å‹ï¼š "ç§‘æŠ€äº’è”ç½‘","æ•™è‚²åŸ¹è®­","æ”¿åŠ¡","å­¦é™¢","ç”µå­å•†åŠ¡","é‡‘èæˆ˜ç•¥","æ³•å¾‹","åŒ»ç–—å¥åº·","æ–‡æ—…ä½“è‚²","è‰ºæœ¯å¹¿å‘Š","äººåŠ›èµ„æº","æ¸¸æˆå¨±ä¹"
        "pageNum": 2,
        "pageSize": 10
    }

    response = requests.request("GET", url=url, headers=headers, params=body).text
    return response


# https://www.xfyun.cn/doc/spark/PPTv2.html
async def xunfei_ppt_create(text: str, templateid: str = "20240718489569D", appid: str = Config.XF_AppID,
                            api_secret: str = Config.XF_Secret_Key, max_retries=20):
    from requests_toolbelt.multipart.encoder import MultipartEncoder

    url = 'https://zwapi.xfyun.cn/api/ppt/v2/create'
    timestamp = int(time.time())
    signature = get_xfyun_signature(appid, api_secret, timestamp)
    form_data = MultipartEncoder(
        fields={
            # "file": (path, open(path, 'rb'), 'text/plain'),  # å¦‚æœéœ€è¦ä¸Šä¼ æ–‡ä»¶ï¼Œå¯ä»¥å°†æ–‡ä»¶è·¯å¾„é€šè¿‡path ä¼ å…¥
            # "fileUrl":"",   #æ–‡ä»¶åœ°å€ï¼ˆfileã€fileUrlã€queryå¿…å¡«å…¶ä¸€ï¼‰
            # "fileName":"",   # æ–‡ä»¶å(å¸¦æ–‡ä»¶ååç¼€ï¼›å¦‚æœä¼ fileæˆ–è€…fileUrlï¼ŒfileNameå¿…å¡«)
            "query": text,
            "templateId": templateid,  # æ¨¡æ¿çš„ID,ä»PPTä¸»é¢˜åˆ—è¡¨æŸ¥è¯¢ä¸­è·å–
            "author": "XXXX",  # PPTä½œè€…åï¼šç”¨æˆ·è‡ªè¡Œé€‰æ‹©æ˜¯å¦è®¾ç½®ä½œè€…å
            "isCardNote": str(True),  # æ˜¯å¦ç”ŸæˆPPTæ¼”è®²å¤‡æ³¨, True or False
            "search": str(True),  # æ˜¯å¦è”ç½‘æœç´¢,True or False
            "isFigure": str(True),  # æ˜¯å¦è‡ªåŠ¨é…å›¾, True or False
            "aiImage": "normal"  # aié…å›¾ç±»å‹ï¼š normalã€advanced ï¼ˆisFigureä¸ºtrueçš„è¯ç”Ÿæ•ˆï¼‰ï¼›
            # normal-æ™®é€šé…å›¾ï¼Œ20%æ­£æ–‡é…å›¾ï¼›advanced-é«˜çº§é…å›¾ï¼Œ50%æ­£æ–‡é…å›¾
        }
    )

    print(form_data)
    headers = {
        "appId": appid,
        "timestamp": str(timestamp),
        "signature": signature,
        "Content-Type": form_data.content_type
    }

    response = requests.request(method="POST", url=url, data=form_data, headers=headers).text
    resp = json.loads(response)
    if resp.get('code') != 0:
        print('åˆ›å»ºPPTä»»åŠ¡å¤±è´¥,ç”ŸæˆPPTè¿”å›ç»“æœï¼š', response)
        return None

    task_id = resp['data']['sid']
    ppt_url = ''
    retries = 0
    # è½®è¯¢ä»»åŠ¡è¿›åº¦
    await asyncio.sleep(5)

    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        while task_id is not None and retries < max_retries:
            task_url = f"https://zwapi.xfyun.cn/api/ppt/v2/progress?sid={task_id}"
            response = await cx.get(url=task_url, headers=headers)
            response.raise_for_status()
            resp = json.loads(response)
            task_status = resp['data']['pptStatus']
            aiImageStatus = resp['data']['aiImageStatus']
            cardNoteStatus = resp['data']['cardNoteStatus']

            if ('done' == task_status and 'done' == aiImageStatus and 'done' == cardNoteStatus):
                ppt_url = resp['data']['pptUrl']
                break

            await asyncio.sleep(3)
            retries += 1

    return ppt_url


# https://www.xfyun.cn/doc/spark/ImageGeneration.html#%E9%89%B4%E6%9D%83%E8%AF%B4%E6%98%8E
async def xunfei_picture(text: str, data_folder=None):
    headers, url = get_xfyun_authorization(api_key=Config.XF_API_Key, api_secret=Config.XF_Secret_Key,
                                           host="spark-api.cn-huabei-1.xf-yun.com", path="/v2.1/tti", method='POST')
    url = 'http://spark-api.cn-huabei-1.xf-yun.com/v2.1/tti' + "?" + urlencode(headers)
    # æ„é€ è¯·æ±‚æ•°æ®
    request_body = {
        "header": {
            "app_id": Config.XF_AppID,  # ä½ åœ¨å¹³å°ç”³è¯·çš„appid
            # 'uid'
            # "res_id": "your_res_id"  # å¯é€‰ï¼šè‡ªå®šä¹‰æœ¯è¯­èµ„æºid
        },
        "parameter": {
            "chat": {
                "domain": "general",
                "temperature": 0.5,
                # "max_tokens": 4096,
                "width": 640,  # é»˜è®¤å¤§å° 512*512
                "height": 480
            }
        },
        "payload": {
            "message": {
                "text": [
                    {
                        "role": "user",
                        "content": text
                    }
                ]
            }
        }
    }
    # å¼‚æ­¥å‘é€è¯·æ±‚
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
        response = await client.post(url, json=request_body, headers={'content-type': "application/json"})
        if response.status_code != 200:
            return None, {"error": f"HTTP Error: {response.status_code}"}

        data = response.json()  # json.loads(response.text)
        code = data['header']['code']
        if code != 0:
            return None, {"error": f'è¯·æ±‚é”™è¯¯: {code}, {data}'}

        text = data["payload"]["choices"]["text"]
        image_base = text[0]["content"]  # base64å›¾ç‰‡ç»“æœ,base64_string_data
        image_id = data['header']['sid']
        # è§£ç  Base64 å›¾åƒæ•°æ®
        file_data = base64.b64decode(image_base)
        if data_folder:
            # å°†è§£ç åçš„æ•°æ®è½¬æ¢ä¸ºå›¾ç‰‡
            file_path = f"{data_folder}/{image_id}.jpg"  # .png
            img = Image.open(io.BytesIO(file_data))
            # r, g, b, a = img.split()
            # img = img.convert('RGB') è½¬æ¢ä¸º RGB æ ¼å¼
            img.save(file_path)

            # buffer = io.BytesIO()
            # img.save(buffer, format="JPEG")  # ä¿å­˜ä¸º JPEG æ ¼å¼
            # buffer.seek(0)
            # async with aiofiles.open(file_path, 'wb') as file:
            #     await file.write(buffer.read())
            return file_path, {"urls": '', 'id': image_id}

        return file_data, {"urls": '', 'id': image_id}


# https://www.volcengine.com/docs/6791/1361423
async def ark_visual_picture(image_data, image_urls: List[str], prompt: str = None, logo_info=None, style_name='3Dé£',
                             return_url=False, data_folder=None):
    style_mapping = {
        "3Dé£": ("img2img_disney_3d_style", ""),
        "å†™å®é£": ("img2img_real_mix_style", ""),
        "å¤©ä½¿é£": ("img2img_pastel_boys_style", ""),
        "åŠ¨æ¼«é£": ("img2img_cartoon_style", ""),
        "æ—¥æ¼«é£": ("img2img_makoto_style", ""),
        "å…¬ä¸»é£": ("img2img_rev_animated_style", ""),
        "æ¢¦å¹»é£": ("img2img_blueline_style", ""),
        "æ°´å¢¨é£": ("img2img_water_ink_style", ""),
        "æ–°è«å¥ˆèŠ±å›­": ("i2i_ai_create_monet", ""),
        "æ°´å½©é£": ("img2img_water_paint_style", ""),
        "è«å¥ˆèŠ±å›­": ("img2img_comic_style", "img2img_comic_style_monet"),
        "ç²¾è‡´ç¾æ¼«": ("img2img_comic_style", "img2img_comic_style_marvel"),
        "èµ›åšæœºæ¢°": ("img2img_comic_style", "img2img_comic_style_future"),
        "ç²¾è‡´éŸ©æ¼«": ("img2img_exquisite_style", ""),
        "å›½é£-æ°´å¢¨": ("img2img_pretty_style", "img2img_pretty_style_ink"),
        "æµªæ¼«å…‰å½±": ("img2img_pretty_style", "img2img_pretty_style_light"),
        "é™¶ç“·å¨ƒå¨ƒ": ("img2img_ceramics_style", ""),
        "ä¸­å›½çº¢": ("img2img_chinese_style", ""),
        "ä¸‘èŒç²˜åœŸ": ("img2img_clay_style", "img2img_clay_style_3d"),
        "å¯çˆ±ç©å¶": ("img2img_clay_style", "img2img_clay_style_bubble"),
        "3D-æ¸¸æˆ_Zæ—¶ä»£": ("img2img_3d_style", "img2img_3d_style_era"),
        "åŠ¨ç”»ç”µå½±": ("img2img_3d_style", "img2img_3d_style_movie"),
        "ç©å¶": ("img2img_3d_style", "img2img_3d_style_doll"),
        # "æ–‡ç”Ÿå›¾-2.0": ("high_aes_general_v20", ''),
        "æ–‡ç”Ÿå›¾-2.0Pro": ("high_aes_general_v20_L", ''),
        "æ–‡ç”Ÿå›¾-2.1": ("high_aes_general_v21_L", ''),
        "è§’è‰²ç‰¹å¾ä¿æŒ": ("high_aes_ip_v20", ''),
        "äººåƒèåˆ": ('face_swap3_6', ''),  # æ¢è„¸å›¾åœ¨å‰ï¼ˆæœ€å¤šä¸‰å¼ ï¼‰ï¼Œæ¨¡æ¿å›¾åœ¨åï¼ˆæœ€å¤šä¸€å¼ ï¼‰
    }
    # inpaintingæ¶‚æŠ¹æ¶ˆé™¤,inpaintingæ¶‚æŠ¹ç¼–è¾‘,outpaintingæ™ºèƒ½æ‰©å›¾
    request_body = {'req_key': style_mapping.get(style_name)[0],
                    'sub_req_key': style_mapping.get(style_name)[1],
                    'return_url': return_url  # é“¾æ¥æœ‰æ•ˆæœŸä¸º24å°æ—¶
                    }
    if 'general' in request_body['req_key'] or prompt:
        request_body["prompt"] = prompt
        request_body["use_sr"] = True  # AIGCè¶…åˆ†
        request_body["scale"] = 3.6  # å½±å“æ–‡æœ¬æè¿°çš„ç¨‹åº¦
        request_body["seed"] = -1  # -1ä¸ºä¸éšæœºç§å­
        # request_body["use_pre_llm"] = True  #use_rephraser, promptæ‰©å†™, å¯¹è¾“å…¥promptè¿›è¡Œæ‰©å†™ä¼˜åŒ–,è¾…åŠ©ç”Ÿæˆå›¾ç‰‡çš„åœºæ™¯ä¸‹ä¼ True
    if image_urls and all(image_urls):
        request_body["image_urls"] = image_urls
    if image_data:  # ç›®æ ‡å›¾ç‰‡éœ€å°äº 5 MB,å°äº4096*4096,æ”¯æŒJPGã€JPEGã€PNGæ ¼å¼,ä»…æ”¯æŒä¸€å¼ å›¾,ä¼˜å…ˆç”Ÿæ•ˆ
        request_body["binary_data_base64"] = [base64.b64encode(image_data).decode("utf-8")]  # è¾“å…¥å›¾ç‰‡base64æ•°ç»„
    if logo_info:
        request_body["logo_info"] = logo_info
        # {
        #     "add_logo": True,
        #     "position": 0,
        #     "language": 0,
        #     "opacity": 0.3,
        #     "logo_text_content": "è¿™é‡Œæ˜¯æ˜æ°´å°å†…å®¹"
        # }
    # 'CVSync2AsyncSubmitTask',JPCartoon
    headers, url = get_ark_signature(action='CVProcess', service='cv', host='visual.volcengineapi.com',
                                     region="cn-north-1", version="2022-08-31", http_method="POST", body=request_body,
                                     access_key_id=Config.VOLC_AK_ID_admin,
                                     secret_access_key=Config.VOLC_Secret_Key_admin,
                                     timenow=None)

    # print(headers,request_body)
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        response = await cx.post(url, json=request_body, headers=headers)
        if response.status_code != 200:
            return None, {"error": f"HTTP Error: {response.status_code},\n{response.text}"}
        response.raise_for_status()
        response_data = response.json()
        # print(response_data.keys())
        # {'code': 10000, 'data': {'1905703073': 1905703073, 'algorithm_base_resp': {'status_code': 0, 'status_message': 'Success'}, 'animeoutlineV4_16_strength_clip': 0.2, 'animeoutlineV4_16_strength_model': 0.2, 'apply_id_layer': '0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15', 'binary_data_base64': [], 'clip_skip': -2, 'cn_mode': 2, 'comfyui_cost': 4, 'controlnet_weight': 1, 'ddim_steps': 20, 'i2t_tag_text': '', 'id_weight': 0, 'image_urls': ['https://p26-aiop-sign.byteimg.com/tos-cn-i-vuqhorh59i/20241225114813D0211FF38607A612BD65-0~tplv-vuqhorh59i-image.image?rk3s=7f9e702d&x-expires=1735184899&x-signature=A1Jay2TWwSsmtwRjDGzK71gzXpg%3D'], 'long_resolution': 832, 'lora_map': {'animeoutlineV4_16': {'strength_clip': 0.2, 'strength_model': 0.2}, 'null': {'strength_clip': 0.7000000000000001, 'strength_model': 0.7000000000000001}}, 'null_strength_clip': 0.7000000000000001, 'null_strength_model': 0.7000000000000001, 'prompt': '(masterpiece), (((best quality))),light tone, sunny day, shinne,tyndall effect lightï¼Œ landscape in the movie of Suzume no Tojimari, daytime, meteor, aurora,', 'return_url': True, 'scale': 5, 'seed': -1, 'strength': 0.58, 'sub_prompts': ['(masterpiece), (((best quality))),light tone, sunny day, shinne,tyndall effect lightï¼Œ landscape in the movie of Suzume no Tojimari, daytime, meteor, aurora,'], 'sub_req_key': ''}, 'message': 'Success', 'request_id': '20241225114813D0211FF38607A612BD65', 'status': 10000, 'time_elapsed': '6.527672506s'}
        if response_data["status"] == 10000:
            image_base = response_data["data"].get("binary_data_base64", [])
            image_urls = response_data["data"].get("image_urls", [''])
            request_id = response_data["request_id"]
            if len(image_base) == 1:
                image_decode = base64.b64decode(image_base[0])
                if data_folder:
                    # å°†è§£ç åçš„æ•°æ®è½¬æ¢ä¸ºå›¾ç‰‡
                    file_path = f"{data_folder}/{request_id}.jpg"
                    img = Image.open(io.BytesIO(image_decode))
                    img.save(file_path)
                    return file_path, {"urls": image_urls, 'id': request_id}
                return image_decode, {"urls": image_urls, 'id': request_id}
            return None, {"urls": image_urls, 'id': request_id}
        return None, response_data


async def ark_drawing_picture(image_data, image_urls: List[str], whitening: float = 1.0, dermabrasion: float = 1.2,
                              logo_info=None, style_name='3däººå¶', return_url=False):
    style_mapping = {
        # å¤´åƒé£æ ¼ï¼ˆå•äººã€ç”·å¥³å‡æ”¯æŒ)
        "ç¾æ¼«é£æ ¼": "img2img_photoverse_american_comics",
        "å•†åŠ¡è¯ä»¶ç…§": "img2img_photoverse_executive_ID_photo",
        "3däººå¶": "img2img_photoverse_3d_weird",
        "èµ›åšæœ‹å…‹": "img2img_photoverse_cyberpunk",
        # èƒ¸åƒå†™çœŸé£æ ¼(å•äººã€åªæ”¯æŒå¥³ç”Ÿ)
        "å¤å ¡": "img2img_xiezhen_gubao",
        "èŠ­æ¯”ç‰›ä»”": "img2img_xiezhen_babi_niuzai",
        "æµ´è¢é£æ ¼": "img2img_xiezhen_bathrobe",
        "è´è¶æœºæ¢°": "img2img_xiezhen_butterfly_machine",
        "èŒåœºè¯ä»¶ç…§": "img2img_xiezhen_zhichangzhengjianzhao",
        "åœ£è¯": "img2img_xiezhen_christmas",
        "ç¾å¼ç”œç‚¹å¸ˆ": "img2img_xiezhen_dessert",
        "old_money": "img2img_xiezhen_old_money",
        "æœ€ç¾æ ¡å›­": "img2img_xiezhen_school"
    }

    request_body = {'req_key': style_mapping.get(style_name),
                    'return_url': return_url,  # é“¾æ¥æœ‰æ•ˆæœŸä¸º24å°æ—¶
                    "beautify_info": {"whitening": whitening,  # è‡ªå®šä¹‰ç¾ç™½å‚æ•°ï¼Œfloatç±»å‹ï¼Œæ•°å€¼è¶Šå¤§ï¼Œæ•ˆæœè¶Šæ˜æ˜¾ï¼Œæœªåšå‚æ•°èŒƒå›´æ ¡éªŒï¼Œå»ºè®®[0, 2]
                                      "dermabrasion": dermabrasion  # è‡ªå®šä¹‰ç£¨çš®å‚æ•°ï¼Œfloatç±»å‹, æ•°å€¼è¶Šå¤§ï¼Œæ•ˆæœè¶Šæ˜æ˜¾ï¼Œæœªåšå‚æ•°èŒƒå›´æ ¡éªŒï¼Œå»ºè®®[0, 2]
                                      }
                    }
    if image_urls and all(image_urls):
        request_body["image_urls"] = image_urls
    if image_data:  # è¾“å…¥å›¾ç‰‡base64æ•°ç»„,ä»…æ”¯æŒä¸€å¼ å›¾,ä¼˜å…ˆç”Ÿæ•ˆ
        request_body["binary_data_base64"] = [base64.b64encode(image_data).decode("utf-8")]
    if logo_info:
        request_body["logo_info"] = logo_info

    headers, url = get_ark_signature(action='HighAesSmartDrawing', service='cv', host='visual.volcengineapi.com',
                                     region="cn-north-1", version="2022-08-31", http_method="POST", body=request_body,
                                     access_key_id=Config.VOLC_AK_ID_admin,
                                     secret_access_key=Config.VOLC_Secret_Key_admin,
                                     timenow=None)

    # print(headers,request_body)
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        response = await cx.post(url, json=request_body, headers=headers)
        if response.status_code != 200:
            return None, {"error": f"HTTP Error: {response.status_code},\n{response.text}"}
        response.raise_for_status()
        response_data = response.json()
        if response_data["status"] == 10000:
            image_base = response_data["data"].get("binary_data_base64", [])
            image_urls = response_data["data"].get("image_urls", [''])
            if len(image_base) == 1:
                image_decode = base64.b64decode(image_base[0])
                return image_decode, {"urls": image_urls, 'id': response_data["request_id"]}
            return None, {"urls": image_urls, 'id': response_data["request_id"]}
        return None, response_data


# https://help.aliyun.com/zh/viapi/developer-reference/api-overview?spm=a2c4g.11186623.help-menu-142958.d_4_3_1.13e65733U2m63s
# https://help.aliyun.com/zh/viapi/developer-reference/api-version?spm=a2c4g.11186623.help-menu-142958.d_4_3_0.290f6593LRs5Lt&scm=20140722.H_464194._.OR_help-T_cn~zh-V_1
async def ali_cartoon_picture(image_url, style_name='å¤å¤æ¼«ç”»'):
    style_mapping = {
        "å¤å¤æ¼«ç”»": '0',
        "3Dç«¥è¯": '1',
        "äºŒæ¬¡å…ƒ": '2',
        "å°æ¸…æ–°": '3',
        "æœªæ¥ç§‘æŠ€": '4',
        "å›½ç”»å¤é£": '5',
        "å°†å†›ç™¾æˆ˜": '6',
        "ç‚«å½©å¡é€š": '7',
        "æ¸…é›…å›½é£": '8'
    }
    # å›¾ç‰‡å¤§å°ä¸è¶…è¿‡10MBã€‚æ”¯æŒçš„å›¾ç‰‡ç±»å‹ï¼šJPEGã€PNGã€JPGã€BMPã€WEBPã€‚
    request_body = {'Index': style_mapping.get(style_name, '0'),
                    'ImageUrl': image_url, }
    # è§†è§‰æ™ºèƒ½å¼€æ”¾å¹³å°å„æœåŠ¡æ”¯æŒçš„åŒºåŸŸä¸ºåä¸œ2ï¼ˆä¸Šæµ·ï¼‰
    parameters, url = get_aliyun_access_token(service="imageenhan", region="cn-shanghai",
                                              action='GenerateCartoonizedImage', http_method="POST",
                                              body=request_body, version='2019-09-30')

    print(request_body)
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        try:
            response = await cx.post(url, json=request_body)  # , headers=headers
            print(response.text)
            if response.status_code != 200:
                return {"error": f"HTTP Error: {response.status_code},\n{response.text}"}

            response.raise_for_status()
            response_data = response.json()
            request_body = {'JobId': response_data["RequestId"]}

            parameters, url = get_aliyun_access_token(service="imageenhan", region="cn-shanghai",
                                                      action='GetAsyncJobResult', http_method="POST",
                                                      body=request_body, version='2019-09-30')
            response = await cx.post(url, json=request_body)  # , headers=headers
            # response_data["Data"].get("ResultUrl")
            print(response.text)
            # å›¾ç‰‡é“¾æ¥éæ³•ï¼Œè¯·æ£€æŸ¥å›¾ç‰‡é“¾æ¥æ˜¯å¦å¯è®¿é—® ,å›¾ç‰‡é“¾æ¥åœ°åŸŸä¸å¯¹ï¼Œè¯·å‚è€ƒhttps://help.aliyun.com/document_detail/155645.html - imageUrl is invalid region oss url
            if response.status_code != 200:
                return {"error": f"HTTP Error: {response.status_code},\n{response.text}"}
            response_data = response.json()
            if response_data["Data"]["Status"] == "PROCESS_SUCCESS":
                image_url = response_data["Data"]["Result"].get("ImageUrls")

                return {"urls": image_url, 'id': response_data["RequestId"]}
        except httpx.ConnectError as e:
            print(f"Connection error: {e},Request URL: {url},Request body: {request_body}")


# https://cloud.tencent.com/document/product/1668/88066
# https://cloud.tencent.com/document/product/1668/107799
async def tencent_drawing_picture(image_data, image_url: str = '', prompt: str = '', negative_prompt: str = '',
                                  style_name='æ—¥ç³»åŠ¨æ¼«', return_url=False):
    # å•è¾¹åˆ†è¾¨ç‡å°äº5000ä¸”å¤§äº50ï¼Œè½¬æˆ Base64 å­—ç¬¦ä¸²åå°äº 8MBï¼Œæ ¼å¼æ”¯æŒ jpgã€jpegã€pngã€bmpã€tiffã€webpã€‚
    style_mapping = {
        "æ°´å½©ç”»": '104',
        "å¡é€šæ’ç”»": '107',
        "3D å¡é€š": '116',
        "æ—¥ç³»åŠ¨æ¼«": '201',
        "å”¯ç¾å¤é£": '203',
        "2.5D åŠ¨ç”»": '210',
        "æœ¨é›•": '120',
        "é»åœŸ": '121',
        "æ¸…æ–°æ—¥æ¼«": '123',
        "å°äººä¹¦æ’ç”»": '124',
        "å›½é£å·¥ç¬”": '125',
        "ç‰çŸ³": '126',
        "ç“·å™¨": '127',
        "æ¯›æ¯¡ï¼ˆäºšæ´²ç‰ˆï¼‰": '135',
        "æ¯›æ¯¡ï¼ˆæ¬§ç¾ç‰ˆï¼‰": '128',
        "ç¾å¼å¤å¤": '129',
        "è’¸æ±½æœ‹å…‹": '130',
        "èµ›åšæœ‹å…‹": '131',
        "ç´ æ": '132',
        "è«å¥ˆèŠ±å›­": '133',
        "åšæ¶‚æ‰‹ç»˜": '134',
        "å¤å¤ç¹èŠ±": "flower",
        "èŠ­æ¯”": "babi",
        "ç™½é¢†ç²¾è‹±": "commerce",
        "å©šçº±æ—¥è®°": "wedding",
        "é†‰æ¢¦çº¢å°˜": "gufeng",
        "æš´å¯Œ": "coin",
        "å¤æ—¥æ°´é•œ": "water",
        "å¤å¤æ¸¯æ¼«": "retro",
        "æ¸¸ä¹åœº": "amusement",
        "å®‡èˆªå‘˜": "astronaut",
        "ä¼‘é—²æ—¶åˆ»": "cartoon",
        "å›åˆ°ç«¥å¹´": "star",
        "å¤šå·´èƒº": "dopamine",
        "å¿ƒåŠ¨åˆå¤": "comic",
        "å¤æ—¥æ²™æ»©": "beach"
    }

    style_type = style_mapping.get(style_name, '201')
    if style_type.isdigit():
        action = 'ImageToImage'  # å›¾åƒé£æ ¼åŒ–
        payload = {'Strength': 0.6,  # ç”Ÿæˆè‡ªç”±åº¦(0, 1]
                   'EnhanceImage': 1,  # ç”»è´¨å¢å¼ºå¼€å…³
                   'RestoreFace': 1,  # ç»†èŠ‚ä¼˜åŒ–çš„é¢éƒ¨æ•°é‡ä¸Šé™ï¼Œæ”¯æŒ0 ~ 6ï¼Œé»˜è®¤ä¸º0ã€‚
                   'RspImgType': 'url' if return_url else 'base64',
                   'Styles': [style_type],
                   'LogoAdd': 0
                   # 'ResultConfig': {"Resolution": "768:768"},  # origin
                   }
        if prompt:
            payload["Prompt"] = prompt
        if negative_prompt:
            payload["NegativePrompt"] = negative_prompt
    else:
        action = 'GenerateAvatar'  # ç™¾å˜å¤´åƒ
        payload = {'RspImgType': 'url' if return_url else 'base64',
                   'Style': style_type,
                   'Type': 'human',  # pet,å›¾åƒç±»å‹
                   'Filter': 1,  # äººåƒå›¾çš„è´¨é‡æ£€æµ‹å¼€å…³ï¼Œé»˜è®¤å¼€å¯ï¼Œä»…åœ¨äººåƒæ¨¡å¼ä¸‹ç”Ÿæ•ˆã€‚
                   'LogoAdd': 0
                   }
    if image_data:
        payload["InputImage"] = base64.b64encode(image_data).decode("utf-8")
    if image_url:
        payload["InputUrl"] = image_url

    url = "https://aiart.tencentcloudapi.com"
    headers = get_tencent_signature(service="aiart", host="aiart.tencentcloudapi.com", body=payload,
                                    action=action, timestamp=int(time.time()), region="ap-shanghai",
                                    version='2022-12-29')

    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
        response = await client.post(url, headers=headers, json=payload)
        response.raise_for_status()
        if response.status_code != 200:
            return None, {'error': f'{response.status_code},Request failed,Response content: {response.text}'}

        response_data = response.json()["Response"]
        if return_url:
            return None, {"urls": [response_data["ResultImage"]], 'id': response_data["RequestId"]}

        image_decode = base64.b64decode(response_data["ResultImage"])
        return image_decode, {"urls": [''], 'id': response_data["RequestId"]}


# https://cloud.tencent.com/document/product/1729/108738
async def tencent_generate_image(prompt: str = '', negative_prompt: str = '', style_name='ä¸é™å®šé£æ ¼',
                                 return_url=False):
    style_mapping = {
        "é»˜è®¤": "000",
        "ä¸é™å®šé£æ ¼": "000",
        "æ°´å¢¨ç”»": "101",
        "æ¦‚å¿µè‰ºæœ¯": "102",
        "æ²¹ç”»1": "103",
        "æ²¹ç”»2ï¼ˆæ¢µé«˜ï¼‰": "118",
        "æ°´å½©ç”»": "104",
        "åƒç´ ç”»": "105",
        "åšæ¶‚é£æ ¼": "106",
        "æ’å›¾": "107",
        "å‰ªçº¸é£æ ¼": "108",
        "å°è±¡æ´¾1ï¼ˆè«å¥ˆï¼‰": "109",
        "å°è±¡æ´¾2": "119",
        "2.5D": "110",
        "å¤å…¸è‚–åƒç”»": "111",
        "é»‘ç™½ç´ æç”»": "112",
        "èµ›åšæœ‹å…‹": "113",
        "ç§‘å¹»é£æ ¼": "114",
        "æš—é»‘é£æ ¼": "115",
        "3D": "116",
        "è’¸æ±½æ³¢": "117",
        "æ—¥ç³»åŠ¨æ¼«": "201",
        "æ€ªå…½é£æ ¼": "202",
        "å”¯ç¾å¤é£": "203",
        "å¤å¤åŠ¨æ¼«": "204",
        "æ¸¸æˆå¡é€šæ‰‹ç»˜": "301",
        "é€šç”¨å†™å®é£æ ¼": "401"
    }
    payload = {'Style': style_mapping.get(style_name, '000'),
               'Prompt': prompt,
               'RspImgType': 'url' if return_url else 'base64',
               'LogoAdd': 0,
               "Resolution": "1024:1024",  # origin
               }

    if negative_prompt:
        payload["NegativePrompt"] = negative_prompt

    url = "https://hunyuan.tencentcloudapi.com"
    headers = get_tencent_signature(service="hunyuan", host="hunyuan.tencentcloudapi.com", body=payload,
                                    action='TextToImageLite', timestamp=int(time.time()), region="ap-guangzhou",
                                    version='2023-09-01')

    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
        response = await client.post(url, headers=headers, json=payload)
        response.raise_for_status()
        if response.status_code != 200:
            return None, {'error': f'{response.status_code},Request failed,Response content: {response.text}'}

        response_data = response.json()["Response"]
        if return_url:
            return None, {"urls": [response_data["ResultImage"]], 'id': response_data["RequestId"]}

        image_decode = base64.b64decode(response_data["ResultImage"])
        return image_decode, {"urls": [''], 'id': response_data["RequestId"]}


async def siliconflow_generate_image(prompt: str = '', negative_prompt: str = '', model_name='siliconflow', model_id=0):
    model_info, name = find_ai_model(model_name, model_id, 'image')

    url = "https://api.siliconflow.cn/v1/images/generations"

    payload = {
        "model": name,
        "prompt": prompt,
        "seed": random.randint(1, 9999999998),  # Required seed range:0 < x < 9999999999
        'prompt_enhancement': True,
    }
    if negative_prompt:
        payload["negative_prompt"] = negative_prompt

    headers = {
        "Authorization": f"Bearer {Config.Silicon_Service_Key}",
        "Content-Type": "application/json"
    }
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
        response = await client.post(url, headers=headers, json=payload)
        response.raise_for_status()
        if response.status_code != 200:
            return None, {'error': f'{response.status_code},Request failed,Response content: {response.text}'}

        response_data = response.json()

        return None, {"urls": [i['url'] for i in response_data["images"]], 'id': response_data["seed"]}


async def embed_images_as_base64(md_content, image_dir):
    """å¼‚æ­¥å°†Markdownä¸­çš„å›¾ç‰‡è½¬æ¢ä¸ºBase64å¹¶åµŒå…¥åˆ°Markdownä¸­"""
    lines = md_content.split('\n')
    new_lines = []

    for line in lines:
        if line.startswith("![") and "](" in line and ")" in line:
            start_idx = line.index("](") + 2
            end_idx = line.index(")", start_idx)
            img_rel_path = line[start_idx:end_idx]

            img_name = os.path.basename(img_rel_path)
            img_path = os.path.join(image_dir, img_name)

            if os.path.exists(img_path):
                # å¼‚æ­¥è¯»å–å¹¶è½¬æ¢å›¾ç‰‡ä¸ºBase64
                async with aiofiles.open(img_path, 'rb') as img_file:
                    img_data = await img_file.read()
                    img_base64 = base64.b64encode(img_data).decode('utf-8')

                img_extension = os.path.splitext(img_name)[-1].lower()
                # æ ¹æ®æ‰©å±•åç¡®å®š MIME ç±»å‹
                if img_extension in ['.jpg', '.jpeg']:
                    mime_type = 'image/jpeg'
                elif img_extension == '.gif':
                    mime_type = 'image/gif'
                else:
                    mime_type = 'image/png'
                # ä¿®æ”¹Markdownä¸­çš„å›¾ç‰‡è·¯å¾„ä¸ºBase64ç¼–ç 
                new_line = f'{line[:start_idx]}data:{mime_type};base64,{img_base64}{line[end_idx:]}'
                new_lines.append(new_line)
            else:  # å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¿ç•™åŸå§‹Markdownæ ¼å¼
                new_lines.append(line)
        else:  # ä¿ç•™éå›¾ç‰‡é“¾æ¥çš„åŸå§‹è¡Œ
            new_lines.append(line)

    return '\n'.join(new_lines)


async def baidu_nlp(nlp_type='ecnet', **kwargs):  # text': text
    '''
    address:åœ°å€è¯†åˆ«
    sentiment_classify:æƒ…æ„Ÿå€¾å‘åˆ†æ
    emotion:å¯¹è¯æƒ…ç»ªè¯†åˆ«
    entity_analysis:å®ä½“åˆ†æ,text,mention
    simnet:çŸ­æ–‡æœ¬ç›¸ä¼¼åº¦,text_1,text_2,
    ecnet,text_correction:æ–‡æœ¬çº é”™,æœç´¢å¼•æ“ã€è¯­éŸ³è¯†åˆ«ã€å†…å®¹å®¡æŸ¥ç­‰åŠŸèƒ½
    txt_keywords_extraction:å…³é”®è¯æå–,text[],num
    txt_monet:æ–‡æœ¬ä¿¡æ¯æå–,content_list[{content,query_lis[{query}]}]
    sentiment_classify:
    depparser:ä¾å­˜å¥æ³•åˆ†æ,åˆ©ç”¨å¥å­ä¸­è¯ä¸è¯ä¹‹é—´çš„ä¾å­˜å…³ç³»æ¥è¡¨ç¤ºè¯è¯­çš„å¥æ³•ç»“æ„ä¿¡æ¯
    lexer:è¯æ³•åˆ†æ,æä¾›åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€ä¸“åè¯†åˆ«ä¸‰å¤§åŠŸèƒ½
    keyword:æ–‡ç« æ ‡ç­¾,title,content
    topic:æ–‡ç« åˆ†ç±»,title,content
    news_summary:æ–°é—»æ‘˜è¦,title,content,max_summary_len
    titlepredictor,æ–‡ç« æ ‡é¢˜ç”Ÿæˆ,doc
    '''
    # https://aip.baidubce.com/rpc/2.0/ernievilg/v1/txt2imgv2
    url = f'https://aip.baidubce.com/rpc/2.0/nlp/v1/{nlp_type}'
    access_token = get_baidu_access_token(Config.BAIDU_nlp_API_Key, Config.BAIDU_nlp_Secret_Key)
    url = build_url(url, access_token, charset='UTF-8')  # f"{url}&access_token={access_token}"
    headers = {
        'Content-Type': 'application/json',  # 'application/x-www-form-urlencoded'
        # 'host': 'aip.baidubce.com',
        # 'authorization': 'bce-auth',
        # 'x-bce-date': '2015-03-24T13: 02:00Z',
        # 'x-bce-request-id':'',
    }
    body = {**kwargs}

    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        response = await cx.post(url, headers=headers, json=body)
        response.raise_for_status()
        data = response.json()  # .get('items')
        # éå†é”®åˆ—è¡¨ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„é”®å¯¹åº”çš„å€¼
        for key in ['items', 'item', 'results', 'results_list', 'error_msg']:
            if key in data:
                if key == 'error_msg':
                    print(response.text)
                return data[key]
        return data


# https://ai.baidu.com/ai-doc/OCR/Ek3h7y961,  https://aip.baidubce.com/rest/2.0/solution/v1/iocr/recognise"
# https://console.bce.baidu.com/ai/#/ai/ocr/overview/index
async def baidu_ocr_recognise(image_data, image_url, ocr_type='accurate_basic'):
    '''
    general:é€šç”¨æ–‡å­—è¯†åˆ«(å«ä½ç½®)
    accurate:é€šç”¨æ–‡å­—è¯†åˆ«(é«˜è¿›åº¦å«ä½ç½®)
    accurate_basic:é€šç”¨æ–‡å­—è¯†åˆ«ï¼ˆé«˜è¿›åº¦ï¼‰
    general_basic:é€šç”¨æ–‡å­—è¯†åˆ«
    doc_analysis_office:åŠå…¬æ–‡æ¡£è¯†åˆ«
    idcard:èº«ä»½è¯è¯†åˆ«
    table:è¡¨æ ¼æ–‡å­—è¯†åˆ«
    numbers:æ•°å­—è¯†åˆ«
    qrcode:äºŒç»´ç è¯†åˆ«
    account_opening:å¼€æˆ·è®¸å¯è¯è¯†åˆ«
    handwriting:æ‰‹å†™æ–‡å­—è¯†åˆ«
    webimage:
    '''
    headers = {
        'Content-Type': "application/x-www-form-urlencoded",
        'Accept': 'application/json',
        # 'charset': "utf-8",
    }
    url = f"https://aip.baidubce.com/rest/2.0/ocr/v1/{ocr_type}"
    access_token = get_baidu_access_token(Config.BAIDU_ocr_API_Key, Config.BAIDU_ocr_Secret_Key)
    params = {
        "access_token": access_token,
        "language_type": 'CHN_ENG',
    }
    if image_url:
        params["url"] = image_url
    if image_data:
        params["image"] = base64.b64encode(image_data).decode("utf-8")
    # å°†å›¾åƒæ•°æ®ç¼–ç ä¸ºbase64
    # image_b64 = base64.b64encode(image_data).decode().replace("\r", "")
    # if template_sign:
    #     params["templateSign"] = template_sign
    # if classifier_id:
    #     params["classifierId"] = classifier_id
    # # è¯·æ±‚æ¨¡æ¿çš„bodys
    # recognise_bodys = "access_token=" + access_token + "&templateSign=" + template_sign + "&image=" + quote(image_b64.encode("utf8"))
    # # è¯·æ±‚åˆ†ç±»å™¨çš„bodys
    # classifier_bodys = "access_token=" + access_token + "&classifierId=" + classifier_id + "&image=" + quote(image_b64.encode("utf8"))
    # request_body = "&".join(f"{key}={value}" for key, value in params.items())
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        response = await cx.post(url, headers=headers, data=params)
        response.raise_for_status()
        data = response.json()
        for key in ['result', 'results', 'error_msg']:
            if key in data:
                return data[key]
        return data


# https://cloud.tencent.com/document/product/866/36210
async def tencent_ocr_recognise(image_data, image_url, ocr_type='GeneralBasicOCR'):
    '''
    GeneralBasicOCR:é€šç”¨å°åˆ·ä½“è¯†åˆ«,TextDetections
    RecognizeTableDDSNOCR: è¡¨æ ¼è¯†åˆ«,TableDetections
    RecognizeGeneralTextImageWarn:è¯ä»¶æœ‰æ•ˆæ€§æ£€æµ‹å‘Šè­¦
    GeneralAccurateOCR:é€šç”¨å°åˆ·ä½“è¯†åˆ«ï¼ˆé«˜ç²¾åº¦ç‰ˆï¼‰
    VatInvoiceOCR:å¢å€¼ç¨å‘ç¥¨è¯†åˆ«
    VatInvoiceVerifyNew:å¢å€¼ç¨å‘ç¥¨æ ¸éªŒ
    ImageEnhancement:æ–‡æœ¬å›¾åƒå¢å¼º,åŒ…æ‹¬åˆ‡è¾¹å¢å¼ºã€å›¾åƒçŸ«æ­£ã€é˜´å½±å»é™¤ã€æ‘©å°”çº¹å»é™¤ç­‰ï¼›
    QrcodeOCR:æ¡å½¢ç å’ŒäºŒç»´ç çš„è¯†åˆ«
    SmartStructuralOCRV2:æ™ºèƒ½ç»“æ„åŒ–è¯†åˆ«,æ™ºèƒ½æå–å„ç±»è¯ç…§ã€ç¥¨æ®ã€è¡¨å•ã€åˆåŒç­‰ç»“æ„åŒ–åœºæ™¯çš„key:valueå­—æ®µä¿¡æ¯
    '''
    url = 'https://ocr.tencentcloudapi.com'
    host = url.split("//")[-1]
    payload = {
        # 'Action': ocr_type,
        # 'Version': '2018-11-19'
        # 'Region': 'ap-shanghai',
        # 'ImageBase64': '',
        # 'ImageUrl': image_url,
    }
    if image_url:
        payload['ImageUrl'] = image_url
    else:
        if isinstance(image_data, bytes):
            payload['ImageBase64'] = base64.b64encode(image_data).decode("utf-8")
        else:
            payload['ImageBase64'] = base64.b64encode(image_data)

    headers = get_tencent_signature('ocr', host, body=payload, action=ocr_type,
                                    secret_id=Config.TENCENT_SecretId, secret_key=Config.TENCENT_Secret_Key,
                                    version='2018-11-19')

    # payload = convert_keys_to_pascal_case(params)

    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        response = await cx.post(url, headers=headers, json=payload)
        response.raise_for_status()
        data = response.json()
        return data["Response"]  # "TextDetections"


# https://help.aliyun.com/zh/ocr/developer-reference/api-ocr-api-2021-07-07-dir/?spm=a2c4g.11186623.help-menu-252763.d_2_2_4.3aba47bauq0U2j
async def ali_ocr_recognise(image_data, image_url, ocr_type='Advanced'):
    # accurate,general_basic,webimage
    # RecognizeAllText
    url = 'https://ocr-api.cn-hangzhou.aliyuncs.com'
    token, _ = get_aliyun_access_token(service="ocr-api", region="cn-hangzhou", access_key_id=Config.ALIYUN_AK_ID,
                                       access_key_secret=Config.ALIYUN_Secret_Key, version='2021-07-07')
    headers = {
        'Content-Type': "application/x-www-form-urlencoded",
        'charset': "utf-8"
    }
    try:
        # å°†å›¾åƒæ•°æ®ç¼–ç ä¸ºbase64
        # image_b64 = base64.b64encode(image_data).decode().replace("\r", "")
        params = {
            "access_token": token,
            "language_type": 'CHN_ENG',

            'Type': ocr_type,  # Advanced,HandWriting,General,Table,GeneralStructure
            'PageNo': 1,
            'OutputRow': False,
            'OutputParagraph': False,
            'OutputKVExcel': False,
            'OutputTableHtml': False,
        }
        if image_data:
            params["body"] = base64.b64encode(image_data)  # quote(image_b64.encode("utf8"))
        if url:
            params["Url"] = image_url

        # if template_sign:
        #     params["templateSign"] = template_sign
        # if classifier_id:
        #     params["classifierId"] = classifier_id
        # # è¯·æ±‚æ¨¡æ¿çš„bodys
        # recognise_bodys = "access_token=" + access_token + "&templateSign=" + template_sign + "&image=" + quote(image_b64.encode("utf8"))
        # # è¯·æ±‚åˆ†ç±»å™¨çš„bodys
        # classifier_bodys = "access_token=" + access_token + "&classifierId=" + classifier_id + "&image=" + quote(image_b64.encode("utf8"))
        # request_body = "&".join(f"{key}={value}" for key, value in params.items())
        response = requests.post(url, data=params, headers=headers, timeout=Config.HTTP_TIMEOUT_SEC)
        response.raise_for_status()

        return response.json()
    except:
        pass


async def ali_nlp(text):
    url = 'alinlp.cn-hangzhou.aliyuncs.com'
    token, _ = get_aliyun_access_token(service="alinlp", region="cn-hangzhou", access_key_id=Config.ALIYUN_AK_ID,
                                       access_key_secret=Config.ALIYUN_Secret_Key)
    if not token:
        print("No permission!")

    headers = {
        "Authorization": f"Bearer {Config.ALIYUN_AK_ID}",
        'Content-Type': "application/x-www-form-urlencoded",
        "X-NLS-Token": token,

    }
    params = {
        "appkey": Config.ALIYUN_nls_AppId,
        'ServiceCode': 'alinlp',
        'Action': 'GetWeChGeneral',
        'Text': text,
        'TokenizerId': 'GENERAL_CHN',
    }


# https://nls-portal.console.aliyun.com/overview
async def ali_speech_to_text(audio_data, format='pcm'):
    """é˜¿é‡Œäº‘è¯­éŸ³è½¬æ–‡å­—"""
    params = {
        "appkey": Config.ALIYUN_nls_AppId,
        "format": format,  # ä¹Ÿå¯ä»¥ä¼ å…¥å…¶ä»–æ ¼å¼ï¼Œå¦‚ wav, mp3
        "sample_rate": 16000,  # éŸ³é¢‘é‡‡æ ·ç‡
        "version": "4.0",
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "SignatureMethod": "HMAC-SHA1",
        "SignatureVersion": "1.0",
        "SignatureNonce": str(uuid.uuid4())
    }
    signature = generate_hmac_signature(Config.ALIYUN_Secret_Key, "POST", params)
    params["signature"] = signature
    token, _ = get_aliyun_access_token(service="nls-meta", region="cn-shanghai", action='CreateToken',
                                       http_method="GET",
                                       access_key_id=Config.ALIYUN_AK_ID, access_key_secret=Config.ALIYUN_Secret_Key)
    if not token:
        print("No permission!")

    headers = {
        "Authorization": f"Bearer {Config.ALIYUN_AK_ID}",
        # "Content-Type": "audio/pcm",
        "Content-Type": "application/octet-stream",
        "X-NLS-Token": token,
    }

    # host = 'nls-gateway-cn-shanghai.aliyuncs.com'
    # conn = http.client.HTTPSConnection(host)
    # http://nls-meta.cn-shanghai.aliyuncs.com/
    # "wss://nls-gateway-cn-shanghai.aliyuncs.com/ws/v1"
    url = "https://nls-gateway.cn-shanghai.aliyuncs.com/stream/v1/asr"
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
        response = await client.post(url, headers=headers, params=params, data=audio_data.getvalue())

    result = response.json()
    if result.get("status") == 20000000:  # "SUCCESS":
        return {"text": result.get("result")}

    return {"error": result.get('message')}


# {
#     "task_id": "cf7b0c5339244ee29cd4e43fb97f****",
#     "result": "åŒ—äº¬çš„å¤©æ°”ã€‚",
#     "status":20000000,
#     "message":"SUCCESS"
# }

# 1536: é€‚ç”¨äºæ™®é€šè¯è¾“å…¥æ³•æ¨¡å‹ï¼ˆæ”¯æŒç®€å•çš„è‹±æ–‡ï¼‰ã€‚
# 1537: é€‚ç”¨äºæ™®é€šè¯è¾“å…¥æ³•æ¨¡å‹ï¼ˆçº¯ä¸­æ–‡ï¼‰ã€‚
# 1737: é€‚ç”¨äºè‹±æ–‡ã€‚
# 1936: é€‚ç”¨äºç²¤è¯­ã€‚
# audio/pcm pcmï¼ˆä¸å‹ç¼©ï¼‰ã€wavï¼ˆä¸å‹ç¼©ï¼Œpcmç¼–ç ï¼‰ã€amrï¼ˆå‹ç¼©æ ¼å¼ï¼‰ã€m4aï¼ˆå‹ç¼©æ ¼å¼ï¼‰
# https://console.bce.baidu.com/ai/#/ai/speech/overview/index
async def baidu_speech_to_text(audio_data, format='pcm', dev_pid=1536):  #: io.BytesIO
    url = "https://vop.baidu.com/server_api"  # 'https://vop.baidu.com/pro_api'
    access_token = get_baidu_access_token(Config.BAIDU_speech_API_Key, Config.BAIDU_speech_Secret_Key)
    # Config.BAIDU_speech_AppId
    url = f"{url}?dev_pid={dev_pid}&cuid={Config.DEVICE_ID}&token={access_token}"
    headers = {'Content-Type': f'audio/{format}; rate=16000'}

    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
        response = await client.post(url, headers=headers, data=audio_data.getvalue())

    result = response.json()
    if result.get("err_no") == 0:
        return {"text": result.get("result")[0]}

    return {"error": result.get('err_msg')}


# Paraformerè¯­éŸ³è¯†åˆ«APIåŸºäºé€šä¹‰å®éªŒå®¤æ–°ä¸€ä»£éè‡ªå›å½’ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œæä¾›åŸºäºå®æ—¶éŸ³é¢‘æµçš„è¯­éŸ³è¯†åˆ«ä»¥åŠå¯¹è¾“å…¥çš„å„ç±»éŸ³è§†é¢‘æ–‡ä»¶è¿›è¡Œè¯­éŸ³è¯†åˆ«çš„èƒ½åŠ›ã€‚å¯è¢«åº”ç”¨äºï¼š
# å¯¹è¯­éŸ³è¯†åˆ«ç»“æœè¿”å›çš„å³æ—¶æ€§æœ‰ä¸¥æ ¼è¦æ±‚çš„å®æ—¶åœºæ™¯ï¼Œå¦‚å®æ—¶ä¼šè®®è®°å½•ã€å®æ—¶ç›´æ’­å­—å¹•ã€ç”µè¯å®¢æœç­‰ã€‚
# å¯¹éŸ³è§†é¢‘æ–‡ä»¶ä¸­è¯­éŸ³å†…å®¹çš„è¯†åˆ«ï¼Œä»è€Œè¿›è¡Œå†…å®¹ç†è§£åˆ†æã€å­—å¹•ç”Ÿæˆç­‰ã€‚
# å¯¹ç”µè¯å®¢æœå‘¼å«ä¸­å¿ƒå½•éŸ³è¿›è¡Œè¯†åˆ«ï¼Œä»è€Œè¿›è¡Œå®¢æœè´¨æ£€ç­‰
async def dashscope_speech_to_text(audio_path, format='wav', language: List[str] = ['zh', 'en']):
    recognition = Recognition(model='paraformer-realtime-v2', format=format, sample_rate=16000,
                              language_hints=language, callback=None)
    result = await asyncio.to_thread(recognition.call, audio_path)  # recognition.call(audio_path)
    if result.status_code == 200:
        texts = [sentence.get('text', '') for sentence in result.get_sentence()]
        return {"text": texts[0]}

    return {"error": result.message}


# SenseVoiceè¯­éŸ³è¯†åˆ«å¤§æ¨¡å‹ä¸“æ³¨äºé«˜ç²¾åº¦å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ã€æƒ…æ„Ÿè¾¨è¯†å’ŒéŸ³é¢‘äº‹ä»¶æ£€æµ‹ï¼Œæ”¯æŒè¶…è¿‡50ç§è¯­è¨€çš„è¯†åˆ«ï¼Œæ•´ä½“æ•ˆæœä¼˜äºWhisperæ¨¡å‹ï¼Œä¸­æ–‡ä¸ç²¤è¯­è¯†åˆ«å‡†ç¡®ç‡ç›¸å¯¹æå‡åœ¨50%ä»¥ä¸Šã€‚
# SenseVoiceè¯­éŸ³è¯†åˆ«æä¾›çš„æ–‡ä»¶è½¬å†™APIï¼Œèƒ½å¤Ÿå¯¹å¸¸è§çš„éŸ³é¢‘æˆ–éŸ³è§†é¢‘æ–‡ä»¶è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œå¹¶å°†ç»“æœè¿”å›ç»™è°ƒç”¨è€…ã€‚
# SenseVoiceè¯­éŸ³è¯†åˆ«è¿”å›è¾ƒä¸ºä¸°å¯Œçš„ç»“æœä¾›è°ƒç”¨è€…é€‰æ‹©ä½¿ç”¨ï¼ŒåŒ…æ‹¬å…¨æ–‡çº§æ–‡å­—ã€å¥å­çº§æ–‡å­—ã€è¯ã€æ—¶é—´æˆ³ã€è¯­éŸ³æƒ…ç»ªå’ŒéŸ³é¢‘äº‹ä»¶ç­‰ã€‚æ¨¡å‹é»˜è®¤è¿›è¡Œæ ‡ç‚¹ç¬¦å·é¢„æµ‹å’Œé€†æ–‡æœ¬æ­£åˆ™åŒ–ã€‚
async def dashscope_speech_to_text_url(file_urls, model='paraformer-v1', language: List[str] = ['zh', 'en']):
    task_response = Transcription.async_call(
        model=model,  # paraformer-8k-v1, paraformer-mtl-v1
        file_urls=file_urls, language_hints=language)

    transcribe_response = Transcription.wait(task=task_response.output.task_id)
    transcription_texts = []
    for r in transcribe_response.output["results"]:
        if r["subtask_status"] == "SUCCEEDED":
            async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as client:
                response = await client.get(r["transcription_url"])
                if response.status_code == 200:
                    transcription_data = response.json()
                    if len(transcription_data["transcripts"]) > 0:
                        transcription_texts.append({"file_url": transcription_data["file_url"],
                                                    "transcripts": transcription_data["transcripts"][0]['text']
                                                    })  # transcription_data["transcripts"][0]["sentences"][0]["text"]
                    else:
                        print(f"No transcription text found in the response.Transcription Result: {response.text}")
                else:
                    print(f"Failed to fetch transcription. Status code: {response.status_code}")
        else:
            print(f"Subtask status: {r['subtask_status']}")

    if len(file_urls) != len(transcription_texts):
        print(json.dumps(transcribe_response.output, indent=4, ensure_ascii=False))

    return transcription_texts, task_response.output.task_id


# éæµå¼åˆæˆ
async def dashscope_text_to_speech(sentences, model="cosyvoice-v1", voice="longxiaochun"):
    synthesizer = dashscope.audio.tts_v2.SpeechSynthesizer(model=model, voice=voice)
    audio_data = synthesizer.call(sentences)  # sample_rate=48000
    return audio_data, synthesizer.get_last_request_id()

    # SpeechSynthesizer.call(model='sambert-zhichu-v1',
    #                        text='ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·',
    #                        sample_rate=48000,
    #                        format='pcm',
    #                        callback=callback)
    # if result.get_audio_data() is not None:


# https://bailian.console.aliyun.com/?spm=5176.28197581.0.0.2e2d29a4n0Mukq#/model-market/detail/wanx-v1?tabKey=sdk
def dashscope_image_call(prompt: str, negative_prompt: str = '', image_url: str = '', style_name="é»˜è®¤",
                         model_name="wanx-v1", data_folder=None):
    style_mapping = {
        "é»˜è®¤": "<auto>",
        "æ‘„å½±": "<photography>",
        "äººåƒå†™çœŸ": "<portrait>",
        "3Då¡é€š": "<3d cartoon>",
        "åŠ¨ç”»": "<anime>",
        "æ²¹ç”»": "<oil painting>",
        "æ°´å½©": "<watercolor>",
        "ç´ æ": "<sketch>",
        "ä¸­å›½ç”»": "<chinese painting>",
        "æ‰å¹³æ’ç”»": "<flat illustration>"
    }
    style = style_mapping.get(style_name, "<auto>")
    rsp = dashscope.ImageSynthesis.call(model=model_name,  # "stable-diffusion-3.5-large"
                                        api_key=Config.Bailian_Service_Key,
                                        prompt=prompt, negative_prompt=negative_prompt, ref_img=image_url,
                                        n=1, size='1024*1024', style=style)

    # ref_strengthï¼šæ§åˆ¶è¾“å‡ºå›¾åƒä¸å‚è€ƒå›¾ï¼ˆå«å›¾ï¼‰çš„ç›¸ä¼¼åº¦ã€‚å–å€¼èŒƒå›´ä¸º[0.0, 1.0]ã€‚å–å€¼è¶Šå¤§ï¼Œä»£è¡¨ç”Ÿæˆçš„å›¾åƒä¸å‚è€ƒå›¾è¶Šç›¸ä¼¼ã€‚
    # ref_modeï¼šåŸºäºå‚è€ƒå›¾ï¼ˆå«å›¾ï¼‰ç”Ÿæˆå›¾åƒçš„æ–¹å¼ã€‚å–å€¼æœ‰ï¼šrepaintä»£è¡¨å‚è€ƒå†…å®¹ï¼Œä¸ºé»˜è®¤å€¼ï¼›refonlyä»£è¡¨å‚è€ƒé£æ ¼ã€‚
    if rsp.status_code == HTTPStatus.OK:
        print(rsp)
        # ä¿å­˜å›¾ç‰‡åˆ°å½“å‰æ–‡ä»¶å¤¹
        image_urls = [result.url for result in rsp.output.results]
        if data_folder:
            image_path = []
            for result in rsp.output.results:
                file_name = PurePosixPath(unquote(urlparse(result.url).path)).parts[-1]
                file_path = f'{data_folder}/%s' % file_name
                with open(file_path, 'wb+') as f:
                    f.write(requests.get(result.url).content)
                image_path.append(file_path)

            return image_path, {"urls": image_urls, 'id': rsp.request_id}
        return requests.get(image_urls[0]).content, {"urls": image_urls, 'id': rsp.request_id}
    return None, {"error": 'Failed, status_code: %s, code: %s, message: %s' % (rsp.status_code, rsp.code, rsp.message)}


# https://help.aliyun.com/zh/model-studio/user-guide/cosplay-anime-character-generation?spm=0.0.0.i1
# https://help.aliyun.com/zh/model-studio/developer-reference/portrait-style-redraw-api-reference?spm=a2c4g.11186623.help-menu-2400256.d_3_3_2_1.3e2f56e5BtF0ok
async def wanx_image_generation(image_urls, style_name="å¤å¤æ¼«ç”»",
                                api_key=Config.DashScope_Service_Key, max_retries=20):
    # JPEGï¼ŒPNGï¼ŒJPGï¼ŒBMPï¼ŒWEB,ä¸è¶…è¿‡10M,ä¸å°äº256*256ï¼Œä¸è¶…è¿‡5760*3240, é•¿å®½æ¯”ä¸è¶…è¿‡2:1
    style_mapping = {
        "å‚è€ƒä¸Šä¼ å›¾åƒé£æ ¼": -1,
        "å¤å¤æ¼«ç”»": 0,
        "3Dç«¥è¯": 1,
        "äºŒæ¬¡å…ƒ": 2,
        "å°æ¸…æ–°": 3,
        "æœªæ¥ç§‘æŠ€": 4,
        "å›½ç”»å¤é£": 5,
        "å°†å†›ç™¾æˆ˜": 6,
        "ç‚«å½©å¡é€š": 7,
        "æ¸…é›…å›½é£": 8,
        "å–œè¿æ–°å¹´": 9
    }
    if style_name == 'CosplayåŠ¨æ¼«äººç‰©':
        model_name = "wanx-style-cosplay-v1"
        input_params = {
            "model_index": 1,
            "face_image_url": image_urls[0],
            "template_image_url": image_urls[1],
        }
    elif len(image_urls) > 1:
        model_name = "wanx-style-repaint-v1"
        input_params = {
            "style_index": -1,
            "image_url": image_urls[0],
            'style_ref_url': image_urls[1]
        }
    else:  # 'äººåƒé£æ ¼é‡ç»˜'
        model_name = "wanx-style-repaint-v1"
        input_params = {
            "style_index": style_mapping.get(style_name, 0),
            "image_url": image_urls[0],
        }

    url = 'https://dashscope.aliyuncs.com/api/v1/services/aigc/image-generation/generation'
    headers = {
        'Content-Type': 'application/json',
        "Authorization": f'Bearer {api_key}',
        'X-DashScope-Async': 'enable'  # ä½¿ç”¨å¼‚æ­¥æ–¹å¼æäº¤ä½œä¸š
    }
    task_headers = {"Authorization": f'Bearer {api_key}'}
    body = {
        "model": model_name,
        "input": input_params,
        # "parameters": {
        #     "style": "<auto>",
        #     "size": "1024*1024",
        #     "n": 1
        # }
    }
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        response = await cx.post(url, headers=headers, json=body)
        response.raise_for_status()
        data = response.json()
        task_id = data["output"]["task_id"]
        task_status = data["output"]["task_status"]
        retries = 0
        # è½®è¯¢ä»»åŠ¡è¿›åº¦
        await asyncio.sleep(3)
        while task_id is not None and retries < max_retries:
            task_url = f'https://dashscope.aliyuncs.com/api/v1/tasks/{task_id}'
            task_response = await cx.get(task_url, headers=task_headers)
            resp = task_response.json()
            task_status = resp["output"]["task_status"]
            # "task_status":"PENDING""RUNNING","SUCCEEDED"->"results", "FAILED"->"message"
            if task_status == 'SUCCEEDED':
                urls = [item['url'] for item in resp['output'].get('results', []) if 'url' in item]
                result = {"urls": urls or [resp['output'].get('result_url')], 'id': task_id}
                if urls:
                    image_response = await cx.get(urls[0])
                    return image_response.content, result

                return None, result

            if task_status == "FAILED":
                print(resp['output']['message'])
                break

            await asyncio.sleep(3)
            retries += 1

        return None, {"urls": [], 'id': task_id, 'status': task_status,
                      'error': "Task did not succeed within the maximum retry limit."}


def dashscope_file_upload(messages, file_path='.pdf', api_key=Config.DashScope_Service_Key):
    url = 'https://dashscope.aliyuncs.com/compatible-mode/v1/files'
    headers = {
        'Content-Type': 'application/json',
        "Authorization": f'Bearer {api_key}',
    }

    try:
        files = {
            'file': open(file_path, 'rb'),
            'purpose': (None, 'file-extract'),
        }
        file_response = requests.post(url, headers=headers, files=files, timeout=Config.HTTP_TIMEOUT_SEC)
        file_response.raise_for_status()
        file_object = file_response.json()
        file_id = file_object.get('id', 'unknown_id')  # ä»å“åº”ä¸­è·å–æ–‡ä»¶ID

        messages.append({"role": "system", "content": f"fileid://{file_id}"})
        return file_object, file_id

    except Exception as e:
        return {"error": str(e)}, None

    finally:
        files['file'][1].close()


def upload_file_to_oss(bucket, file_obj, object_name, expires: int = 604800):
    """
      ä¸Šä¼ æ–‡ä»¶åˆ° OSS æ”¯æŒ `io` å¯¹è±¡ã€‚
      :param bucket: OSS bucket å®ä¾‹
      :param file_obj: æ–‡ä»¶å¯¹è±¡ï¼Œå¯ä»¥æ˜¯ `io.BytesIO` æˆ– `io.BufferedReader`
      :param object_name: OSS ä¸­çš„å¯¹è±¡å
      :param expires: ç­¾åæœ‰æ•ˆæœŸï¼Œé»˜è®¤ä¸€å‘¨ï¼ˆç§’ï¼‰
    """
    file_obj.seek(0, os.SEEK_END)
    total_size = file_obj.tell()  # os.path.getsize(file_path)
    file_obj.seek(0)
    if total_size > 1024 * 1024 * 16:
        part_size = oss2.determine_part_size(total_size, preferred_size=128 * 1024)
        upload_id = bucket.init_multipart_upload(object_name).upload_id
        parts = []
        part_number = 1
        offset = 0
        while offset < total_size:
            size_to_upload = min(part_size, total_size - offset)
            result = bucket.upload_part(object_name, upload_id, part_number,
                                        oss2.SizedFileAdapter(file_obj, size_to_upload))
            parts.append(oss2.models.PartInfo(part_number, result.etag, size=size_to_upload, part_crc=result.crc))
            offset += size_to_upload
            part_number += 1

        # å®Œæˆåˆ†ç‰‡ä¸Šä¼ 
        bucket.complete_multipart_upload(object_name, upload_id, parts)
    else:
        # OSS ä¸Šçš„å­˜å‚¨è·¯å¾„, æœ¬åœ°å›¾ç‰‡è·¯å¾„
        bucket.put_object(object_name, file_obj)
        # bucket.put_object_from_file(object_name, str(file_path))

    if 0 < expires <= 604800:  # å¦‚æœç­¾åsigned_URL
        url = bucket.sign_url("GET", object_name, expires=expires)
    else:  # ä½¿ç”¨åŠ é€ŸåŸŸå
        url = f"{Config.ALIYUN_Bucket_Domain}/{object_name}"
        # bucket.bucket_name
    # è·å–æ–‡ä»¶å¯¹è±¡
    # result = bucket.get_object(object_name)
    # result.read()è·å–æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹,result.headerså…ƒæ•°æ®ï¼ˆå¤´éƒ¨ä¿¡æ¯ï¼‰
    return url


# è·å–æ–‡ä»¶åˆ—è¡¨
def list_files(bucket, prefix='upload/', max_keys=100, max_pages=1):
    """
    åˆ—å‡º OSS ä¸­çš„æ–‡ä»¶ã€‚
    :param bucket: oss2.Bucket å®ä¾‹
    :param prefix: æ–‡ä»¶åå‰ç¼€ï¼Œç”¨äºç­›é€‰
    :param max_keys: æ¯æ¬¡è¿”å›çš„æœ€å¤§æ•°é‡
    :return: æ–‡ä»¶ååˆ—è¡¨
    """
    file_list = []
    if max_pages <= 1:
        for obj in oss2.ObjectIterator(bucket, prefix=prefix, max_keys=max_keys):
            file_list.append(obj.key)
    else:
        i = 0
        next_marker = ''
        while i < max_pages:
            result = bucket.list_objects(prefix=prefix, max_keys=max_keys, marker=next_marker)
            for obj in result.object_list:
                file_list.append(obj.key)
            if not result.is_truncated:  # å¦‚æœæ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œé€€å‡ºå¾ªç¯
                break
            next_marker = result.next_marker
            i += 1

    return file_list


async def download_by_aiohttp(url: str, save_path, chunk_size=4096, in_decode=False):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            # response = await asyncio.wait_for(session.get(url), timeout=timeout)
            if response.status == 200:
                if save_path:
                    save_path.parent.mkdir(parents=True, exist_ok=True)
                    async with aiofiles.open(save_path, mode='wb') as f:
                        # response.content å¼‚æ­¥è¿­ä»£å™¨ï¼ˆæµå¼è¯»å–ï¼‰,iter_chunked éé˜»å¡è°ƒç”¨ï¼Œé€å—è¯»å–
                        async for chunk in response.content.iter_chunked(chunk_size):
                            if isinstance(chunk, (bytes, bytearray)):
                                await f.write(chunk)
                            elif isinstance(chunk, str):
                                await f.write(chunk.encode('utf-8'))  # å°†å­—ç¬¦ä¸²è½¬ä¸ºå­—èŠ‚
                            else:
                                raise TypeError(
                                    f"Unexpected chunk type: {type(chunk)}. Expected bytes or bytearray.")

                    return save_path

                content = await response.read()  # å•æ¬¡å¼‚æ­¥è¯»å–å®Œæ•´å†…å®¹ï¼Œå°æ–‡ä»¶,await response.content.read(chunk_size)
                return content.decode('utf-8') if in_decode else content  # å°†å­—èŠ‚è½¬ä¸ºå­—ç¬¦ä¸²,è§£ç åçš„å­—ç¬¦ä¸² await response.text()

            print(f"Failed to download {url}: {response.status}")

    return None


async def download_by_httpx(url: str, save_path, chunk_size=4096, in_decode=False):
    async with httpx.AsyncClient() as client:
        async with client.stream("GET", url) as response:  # é•¿æ—¶é—´çš„æµå¼è¯·æ±‚
            # response = await client.get(url, stream=True)
            response.raise_for_status()  # å¦‚æœå“åº”ä¸æ˜¯2xx  response.status_code == 200:
            if save_path:
                save_path.parent.mkdir(parents=True, exist_ok=True)
                async with aiofiles.open(save_path, mode="wb") as f:
                    # response.aiter_bytes() å¼‚æ­¥è¿­ä»£å™¨
                    async for chunk in response.aiter_bytes(chunk_size=chunk_size):
                        await f.write(chunk)

                return save_path

            content = bytearray()  # ä½¿ç”¨ `bytearray` æ›´é«˜æ•ˆåœ°æ‹¼æ¥äºŒè¿›åˆ¶å†…å®¹  b""  # raw bytes
            async for chunk in response.aiter_bytes(chunk_size=chunk_size):
                content.extend(chunk)
                # content += chunk
                # yield chunk

            return content.decode(response.encoding or 'utf-8') if in_decode else bytes(content)
            # return response.text if in_decode else response.content


def download_by_requests(url: str, save_path, chunk_size=4096, in_decode=False):
    '''
    åŒæ­¥ä¸‹è½½çš„æµå¼æ–¹æ³•
    å¦‚æœç›®æ ‡æ˜¯ä¿å­˜åˆ°æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨ contentï¼ˆæ— éœ€è§£ç ï¼‰ã€‚ï¼ˆå¦‚å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ã€PDF ç­‰ï¼‰
    å¦‚æœç›®æ ‡æ˜¯å¤„ç†å’Œè§£ææ–‡æœ¬æ•°æ®ï¼Œä¸”ç¡®å®šç¼–ç æ­£ç¡®ï¼Œä½¿ç”¨ textã€‚ï¼ˆå¦‚ HTMLã€JSONï¼‰
    '''
    with requests.get(url, stream=True, timeout=Config.HTTP_TIMEOUT_SEC) as response:
        response.raise_for_status()  # ç¡®ä¿è¯·æ±‚æˆåŠŸ
        if save_path:
            save_path.parent.mkdir(parents=True, exist_ok=True)
            with open(save_path, "wb") as f:
                # requests iter_content åŒæ­¥ä½¿ç”¨,é˜»å¡è°ƒç”¨
                for chunk in response.iter_content(chunk_size=chunk_size):
                    if chunk:  # <class 'bytes'>
                        f.write(chunk)

            return save_path

        return response.text if in_decode else response.content  # ç›´æ¥è·å–æ–‡æœ¬,åŒæ­¥ç›´æ¥è¿”å›å…¨éƒ¨å†…å®¹


def upload_by_requests(url: str, file_path, file_key='snapshot'):
    with open(file_path, "rb") as f:
        files = {file_key: f}
        response = requests.post(url, files=files)
    return response.json()


async def download_file(url: str, dest_folder: Path = None, chunk_size=4096,
                        in_decode=False, in_session=False, retries=3, delay=3):
    """
    ä¸‹è½½URLä¸­çš„æ–‡ä»¶åˆ°ç›®æ ‡æ–‡ä»¶å¤¹
    :param url: ä¸‹è½½é“¾æ¥
    :param dest_folder: ä¿å­˜æ–‡ä»¶çš„è·¯å¾„
    :param chunk_size: æ¯æ¬¡è¯»å–çš„å­—èŠ‚å¤§å°ï¼ˆé»˜è®¤4096å­—èŠ‚ï¼‰
    :param in_decode: æ˜¯å¦è§£ç ä¸ºå­—ç¬¦ä¸²
    :param in_session: æ˜¯å¦ä½¿ç”¨ sessionï¼ˆé•¿è¿æ¥ä¼˜åŒ–ï¼‰
    :param retries: ä¸‹è½½å¤±è´¥åçš„é‡è¯•æ¬¡æ•°
    :param delay: é‡è¯•ä¹‹é—´çš„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    """
    # filename = url.split("/")[-1]  # æå–æ–‡ä»¶å
    file_name = unquote(url.split("/")[-1].split("?")[0])
    save_path = None
    # print(file_name)
    if dest_folder:
        save_path = dest_folder / file_name  # æå–æ–‡ä»¶å

    attempt = 0
    while attempt < retries:
        try:
            if in_session:  # aiohttpé•¿è¿æ¥ä¼˜åŒ–ï¼Œé€‚åˆå‘é€å¤šä¸ªè¯·æ±‚æˆ–éœ€è¦æ›´å¥½çš„è¿æ¥å¤ç”¨,ç»´æŠ¤å¤šä¸ªè¯·æ±‚ä¹‹é—´çš„è¿æ¥
                return await download_by_aiohttp(url, save_path, chunk_size, in_decode), file_name
            else:  # httpxå°‘é‡è¯·æ±‚åœºæ™¯,é€‚åˆç®€å•çš„ã€å•ä¸ªè¯·æ±‚åœºæ™¯
                return await download_by_httpx(url, save_path, chunk_size, in_decode), file_name

        except (httpx.RequestError, aiohttp.ClientError, asyncio.TimeoutError) as e:
            print(f"Download attempt {attempt + 1} failed: {e}")
            attempt += 1
            await asyncio.sleep(delay)  # ç­‰å¾…é‡è¯•
        except httpx.HTTPStatusError as exc:
            print(f"Failed to download {url},HTTP error occurred: {exc.response.status_code} - {exc.response.text}")
        except TypeError:
            return download_by_requests(url, save_path, chunk_size, in_decode), file_name
        except Exception as e:
            print(f"Error: {e}, downloading url: {url} file: {file_name}")
            break

    return None, None


# https://www.weatherapi.com/api-explorer.aspx#forecast
def get_weather(city: str, days: int = 0, date: str = None):
    # ä½¿ç”¨ WeatherAPI çš„ API æ¥è·å–å¤©æ°”ä¿¡æ¯
    api_key = Config.Weather_Service_Key
    base_url = "http://api.weatherapi.com/v1/current.json"
    city = convert_to_pinyin(city)
    params = {
        'key': api_key,
        'q': city,
        # Pass US Zipcode, UK Postcode, Canada Postalcode, IP address, Latitude/Longitude (decimal degree) or city name.
        'aqi': 'no'  # Get air quality data ç©ºæ°”è´¨é‡æ•°æ®
    }
    # Number of days of weather forecast. Value ranges from 1 to 10
    if days > 0:
        params['days'] = days
        params['alerts'] = 'no'
    elif date:
        # Date on or after 1st Jan, 2010 in yyyy-MM-dd format
        # Date between 14 days and 300 days from today in the future in yyyy-MM-dd format
        params['dt'] = date

    response = requests.get(base_url, params=params)
    if response.status_code == 200:
        data = response.json()
        weather = data['current']['condition']['text']
        temperature = data['current']['temp_c']
        return f"The weather in {city} is {weather} with a temperature of {temperature}Â°C."
    else:
        return f"Could not retrieve weather information for {city}."


def send_to_wechat(user_name: str, context: str = None, link: str = None, object_name: str = None):
    url = f"{Config.WECHAT_URL}/sendToChat"
    headers = {'accept': 'application/json', 'Content-Type': 'application/json'}
    body = {'user': user_name, 'context': context, 'url': link,
            'object_name': object_name, 'file_type': get_file_type_wx(object_name)}

    try:
        with httpx.Client(timeout=(10, Config.HTTP_TIMEOUT_SEC)) as client:
            response = client.post(url, json=body, headers=headers)
            response.raise_for_status()
        return response.json()

    except Exception as e:
        print(datetime.now(), body)
        print(f"Error occurred while sending message: {e}")

    return None


if __name__ == "__main__":
    AccessToken = 'd04149e455d44ac09432f0f89c3e0a41'
    # https://nls-portal-service.aliyun.com/ptts?p=eyJleHBpcmF0aW9uIjoiMjAyNC0wOS0wNlQwOToxNjoyNy42MDRaIiwiY29uZGl0aW9ucyI6W1sic3RhcnRzLXdpdGgiLCIka2V5IiwidHRwLzEzODE0NTkxNjIwMDc4MjIiXV19&s=k4sDIZ4lCmUiQ%2BV%2FcTEnFteey54%3D&e=1725614187&d=ttp%2F1381459162007822&a=LTAIiIg37IN8xeMa&h=https%3A%2F%2Ftuatara-cn-shanghai.oss-cn-shanghai.aliyuncs.com&u=qnKV1N8muiAIFiL22JTrgdYExxHS%2BPSxccg9VPiL0Nc%3D
    fileLink = "https://gw.alipayobjects.com/os/bmw-prod/0574ee2e-f494-45a5-820f-63aee583045a.wav"
    import asyncio
    import io

    dashscope.api_key = Config.DashScope_Service_Key
    file_urls = ['https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_female.wav',
                 'https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_male.wav'
                 ]
    # r = requests.get(
    #     'https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_female2.wav'
    # )
    # with open('asr_example.wav', 'wb') as f:
    #     f.write(r.content)

    # asyncio.run(dashscope_speech_to_text_url(file_urls))

    task_id = "8d47c5d9-06bb-47aa-8986-1df91b6c8dd2"
    audio_file = 'data/nls-sample-16k.wav'


    # with open(audio_file, 'rb') as f:
    #     audio_data = io.BytesIO(f.read())

    # print(dashscope_speech_to_text('data/nls-sample-16k.wav'))

    # fetch()è°ƒç”¨ä¸ä¼šé˜»å¡ï¼Œå°†ç«‹å³è¿”å›æ‰€æŸ¥è¯¢ä»»åŠ¡çš„çŠ¶æ€å’Œç»“æœ
    # transcribe_response = dashscope.audio.asr.Transcription.fetch(task=task_id)
    # print(json.dumps(transcribe_response.output, indent=4, ensure_ascii=False))
    # for r in transcribe_response.output["results"]:
    #     if r["subtask_status"] == "SUCCEEDED":
    #         url = r["transcription_url"]
    #         response = requests.get(url)
    #         if response.status_code == 200:
    #             transcription_data = response.text  # å¯ä»¥ä½¿ç”¨ response.json() æ¥å¤„ç† JSON å“åº”
    #             print(f"Transcription Result: {transcription_data}")
    #             data = response.json()
    #             print(data["transcripts"][0]['text'])
    #         else:
    #             print(f"Failed to fetch transcription. Status code: {response.status_code}")
    #     else:
    #         print(f"Subtask status: {r['subtask_status']}")

    async def test():
        # audio_file = 'data/nls-sample-16k.wav'
        # with open(audio_file, 'rb') as f:
        #     audio_data = io.BytesIO(f.read())
        #
        # result = await  baidu_speech_to_text(audio_data, 'wav')  # ali_speech_to_text(audio_data,'wav')  #
        result = await web_search_async('æ˜“å¾—èä¿¡æ˜¯ä»€ä¹ˆå…¬å¸')
        print(result)


    # asyncio.run(test())
    asyncio.run(baidu_nlp("ecnet", text="ç™¾åº¦æ˜¯ä¸€å®¶äººå·¥åªèƒ½å…¬å¸"))

    # asyncio.run(tencent_translate('tencent translate is ok', 'en', 'cn'))
    # from lagent import list_tools, get_tool
    #
    # list_tools()
