import re, json, io, os, threading, time
import  aiofiles, asyncio
from itertools import groupby
from pathlib import Path
from difflib import get_close_matches, SequenceMatcher
from collections import Counter, deque, defaultdict
from pypinyin import lazy_pinyin
from utils.base import *


async def embed_images_as_base64(md_content, image_dir):
    """å¼‚æ­¥å°†Markdownä¸­çš„å›¾ç‰‡è½¬æ¢ä¸ºBase64å¹¶åµŒå…¥åˆ°Markdownä¸­"""
    lines = md_content.split('\n')
    new_lines = []

    for line in lines:
        if line.startswith("![") and "](" in line and ")" in line:
            start_idx = line.index("](") + 2
            end_idx = line.index(")", start_idx)
            img_rel_path = line[start_idx:end_idx]

            img_name = os.path.basename(img_rel_path)
            img_path = os.path.join(image_dir, img_name)

            if os.path.exists(img_path):
                # å¼‚æ­¥è¯»å–å¹¶è½¬æ¢å›¾ç‰‡ä¸ºBase64
                async with aiofiles.open(img_path, 'rb') as img_file:
                    img_data = await img_file.read()
                    img_base64 = base64.b64encode(img_data).decode('utf-8')

                img_extension = os.path.splitext(img_name)[-1].lower()
                # æ ¹æ®æ‰©å±•åç¡®å®š MIME ç±»å‹
                if img_extension in ['.jpg', '.jpeg']:
                    mime_type = 'image/jpeg'
                elif img_extension == '.gif':
                    mime_type = 'image/gif'
                else:
                    mime_type = 'image/png'
                # ä¿®æ”¹Markdownä¸­çš„å›¾ç‰‡è·¯å¾„ä¸ºBase64ç¼–ç 
                new_line = f'{line[:start_idx]}data:{mime_type};base64,{img_base64}{line[end_idx:]}'
                new_lines.append(new_line)
            else:  # å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¿ç•™åŸå§‹Markdownæ ¼å¼
                new_lines.append(line)
        else:  # ä¿ç•™éå›¾ç‰‡é“¾æ¥çš„åŸå§‹è¡Œ
            new_lines.append(line)

    return '\n'.join(new_lines)


def qwen3_think_index(output_ids):
    # parsing thinking content
    try:
        # rindex finding 151668 (</think>)
        index = len(output_ids) - output_ids[::-1].index(151668)
    except ValueError:
        index = 0
    return index


def process_assistant_think(content):
    if '<think>' in content and '</think>' in content:
        content = re.sub(r'(<think>)(.*?)(</think>)',
                         r'<details style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†å†…å®¹ï¼ˆå±•å¼€ï¼‰</summary>\2</details>',
                         content,
                         flags=re.DOTALL)

    if '<think>' in content and '</think>' not in content:
        content = re.sub(r'<think>(.*?)$',
                         r'<details open style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†ä¸­...</summary>\1</details>',
                         content,
                         flags=re.DOTALL)

    if '<think>' not in content and '</think>' in content:
        content = re.sub(r'(.*?)</think>',
                         r'<details style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†å†…å®¹ï¼ˆå±•å¼€ï¼‰</summary>\1</details>',
                         content,
                         flags=re.DOTALL)

    return content


# class Partial:
#     def __init__(self, func, *args, **kwargs):
#         self.func = func
#         self.args = args
#         self.kwargs = kwargs
#
#     def __call__(self, *more_args, **more_kwargs):
#         # åˆå¹¶å›ºå®šå‚æ•°å’Œæ–°å‚æ•°
#         all_args = self.args + more_args
#         all_kwargs = {**self.kwargs, **more_kwargs}
#         return self.func(*all_args, **all_kwargs)

def dict2xml(tag, d):
    """å°†å­—å…¸è½¬æ¢ä¸º XML å­—ç¬¦ä¸²"""
    import xml.etree.ElementTree as ET
    elem = ET.Element(tag)
    for key, val in d.items():
        child = ET.SubElement(elem, key)
        if isinstance(val, list):
            for item in val:
                item_elem = ET.SubElement(child, "item")
                item_elem.text = str(item)
        else:
            child.text = str(val)
    return ET.tostring(elem, encoding='unicode')


def list2xml(tag, lst):
    """å°†åˆ—è¡¨è½¬æ¢ä¸º XML å­—ç¬¦ä¸²"""
    import xml.etree.ElementTree as ET
    elem = ET.Element(tag)
    for item in lst:
        item_elem = ET.SubElement(elem, "item")
        item_elem.text = str(item)
    return ET.tostring(elem, encoding='unicode')


def df2markdown(df, index=False):
    # df.fillna('N/A').to_markdown()
    headers = df.columns.tolist()
    md = "| " + " | ".join(headers) + " |\n"
    md += "| " + " | ".join(["---"] * len(headers)) + " |\n"
    for _, row in df.iterrows():
        md += "| " + " | ".join(row.astype(str)) + " |\n"

    return md if index else md.replace("| Index |", "|")  # å¯é€‰ç§»é™¤ç´¢å¼•åˆ—


def df2doc(data, use_index=True) -> list[str]:
    """
    å°† DataFrame ä¸­æ¯ä¸€è¡Œè½¬æ¢ä¸ºä¸€æ®µæ–‡æœ¬ï¼Œè·³è¿‡ None å€¼
    :param data: è¾“å…¥ DataFrame
    :param use_index: æ˜¯å¦åœ¨æ–‡æœ¬å‰å¢åŠ è¡Œç´¢å¼•
    :return: æ–‡æœ¬è®°å½•åˆ—è¡¨
    """
    docs = []
    try:
        import pandas as pd
        if use_index:
            for item in zip(data.index, data.to_dict(orient='records')):
                docs.append(f'{item[0]}\t' + '|'.join(
                    f'{k}#{v.strip() if isinstance(v, str) else v}' for k, v in item[1].items() if pd.notna(v)).strip())
        else:
            for item in data.to_dict(orient='records'):
                docs.append('|'.join(
                    f'{k}#{v.strip() if isinstance(v, str) else v}' for k, v in item.items() if pd.notna(v)).strip())
    except ImportError:
        if not isinstance(data, list) or not all(isinstance(d, dict) for d in data):
            raise ValueError("è¾“å…¥æ•°æ®åº”ä¸ºåˆ—è¡¨çš„å­—å…¸æ ¼å¼ï¼Œä¾‹å¦‚ [{'key1': 'value1', 'key2': 'value2'}, ...]")

        for idx, record in enumerate(data):  # data.iterrows()
            # æ‹¼æ¥æ¯ä¸ªå­—æ®µï¼Œè·³è¿‡ None å€¼ï¼Œå¹¶å¯¹å­—ç¬¦ä¸²åš strip å¤„ç†
            doc_line = '|'.join(
                f"{k}#{v.strip() if isinstance(v, str) else v}"
                for k, v in record.items() if v is not None
            )
            # å¦‚æœ use_index=Trueï¼Œåˆ™åœ¨å‰é¢åŠ ä¸Šç´¢å¼•
            if use_index:
                doc_line = f"{idx}\t" + doc_line

            docs.append(doc_line)
    except Exception as e:
        print(e)

    return docs


def df2doc_batch(records, batch_size: int = 5):
    """
    å°† DataFrame æˆ–åˆ—è¡¨æ•°æ®æŒ‰ batch_size åˆ†æ‰¹ï¼Œyield æ¯ä¸ªæ‰¹æ¬¡çš„è®°å½•ï¼ˆåˆ—è¡¨ of dictsï¼‰ã€‚
    """
    try:
        import pandas as pd
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient='records')
    except ImportError:
        if not isinstance(records, list) or not all(isinstance(d, dict) for d in records):
            raise ValueError("è¾“å…¥æ•°æ®åº”ä¸ºåˆ—è¡¨çš„å­—å…¸æ ¼å¼ï¼Œä¾‹å¦‚ [{'key1': 'value1', 'key2': 'value2'}, ...]")
    except Exception as e:
        print(e)

    batch = []
    for i, item in enumerate(records):
        batch.append(item)
        # æ¯ batch_size ç»„ä¸€ä¸ª batch
        if (i + 1) % batch_size == 0 or i == len(records) - 1:
            yield batch
            batch = []


def df2doc_split(records, max_tokens: int = 4000, tokenizer=None):
    """
    å°† DataFrame æˆ–åˆ—è¡¨æ•°æ®æŒ‰ token æ•°åˆ†å—ï¼Œyield æ¯ä¸ªåˆ†å—ï¼ˆlist[str]ï¼‰ã€‚
    for block in df2doc_split(...
    :param records: pandas.DataFrame æˆ– list[dict]
    :param max_tokens: æ¯ä¸ªåˆ†å—å…è®¸çš„æœ€å¤§ token æ•°
    :param tokenizer: tokenizer å¯¹è±¡ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨ len(str) è¿‘ä¼¼
    :return: generatorï¼Œæ¯æ¬¡ yield ä¸€ä¸ª list[dict]ï¼Œå³ä¸€å—è®°å½•
    """
    try:
        import pandas as pd
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")
    except ImportError:
        if not isinstance(records, list) or not all(isinstance(d, dict) for d in records):
            raise ValueError("è¾“å…¥æ•°æ®åº”ä¸ºåˆ—è¡¨çš„å­—å…¸æ ¼å¼ï¼Œä¾‹å¦‚ [{'key1': 'value1'}, ...]")

    chunk = []
    current_tokens = 0

    for item in records:
        item_str = json.dumps(item, ensure_ascii=False)
        item_tokens = lang_token_size(item_str, tokenizer=tokenizer)

        # å¦‚æœå½“å‰è®°å½•æœ¬èº«å°±è¶…è¿‡ max_tokensï¼Œåˆ™å•ç‹¬ä½œä¸ºä¸€ä¸ªå—è¾“å‡º
        if item_tokens > max_tokens:
            if chunk:
                yield chunk
                chunk = []
                current_tokens = 0
            yield [item]
            continue

        # ç´¯åŠ åè¶…è¿‡ max_tokensï¼Œå…ˆè¾“å‡ºå½“å‰å—
        if current_tokens + item_tokens > max_tokens:
            yield chunk
            chunk = []
            current_tokens = 0

        chunk.append(item)
        current_tokens += item_tokens

    if chunk:
        yield chunk


def get_max_items_from_list(records: list[dict], max_tokens: int = 4000, tokenizer=None) -> list[dict]:
    """
        Get max items from list of items based on defined max tokens (based on openai compute)
        æ ¹æ®ç»™å®šçš„æœ€å¤§ token æ•°ï¼Œä»ä¸€ç»„å­—å…¸æ•°æ®ä¸­é€‰å–é€‚åˆçš„é¡¹ç›®ï¼Œç›´åˆ°è¾¾åˆ° token é™åˆ¶ä¸ºæ­¢,ä»å¤´å¼€å§‹
        :param records: åŒ…å«å­—å…¸çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸è¡¨ç¤ºä¸€ä¸ªé¡¹ç›®
        :param max_tokens: å…è®¸çš„æœ€å¤§ token æ•°
        :param tokenizer: å¯é€‰çš„ tokenizerï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼Œåˆ™æ ¹æ®è¯­è¨€è‡ªåŠ¨å¤„ç†ï¼‰
        :return: é€‚åˆçš„é¡¹ç›®åˆ—è¡¨
        List[Dict[str, str]]
    """
    # encoding = tiktoken.encoding_for_model(encoding_name)
    # tiktoken.get_encoding("cl100k_base")
    return next(df2doc_split(records, max_tokens=max_tokens, tokenizer=tokenizer))


def get_last_entries_records(records: list[dict], fields: list = None, use_index: bool = False, max_tokens: int = 8000,
                             tokenizer=None) -> list[str]:
    texts = []
    total_chars = 0
    # ä»æœ€æ–°è®°å½•å¼€å§‹æ‹¼æ¥ï¼Œç›´åˆ°æ€»å­—ç¬¦æ•°è¶…è¿‡ max_tokens æ—¶åœæ­¢æ·»åŠ ï¼ˆè¿”å›æœ€åä¸è¶³ max_chars å­—ç¬¦çš„éƒ¨åˆ†ï¼‰
    for idx, record in enumerate(records):
        use_fields = fields or list(record.keys())
        prefix = f"{idx}\t" if use_index else ""
        item_str = prefix + '|'.join(
            f"{k}#{(str(record[k]).strip() if isinstance(record[k], str) else record[k])}"
            for k in use_fields if record.get(k) is not None
        )
        entry_length = lang_token_size(item_str, tokenizer=tokenizer)  # len(item_str)
        if total_chars + entry_length > max_tokens:
            break

        texts.append(item_str)
        total_chars += entry_length

    # å¦‚æœæœ‰å¤šä¸ªè®°å½•ï¼Œå€’åºæ‹¼æ¥ï¼ˆä¿è¯æœ€æ—©çš„è®°å½•åœ¨æœ€å‰é¢ï¼‰
    return list(reversed(texts))  # "\n\n".join(reversed(texts))


def find_similar_word(target_keyword, template_list: list[str]) -> int:
    max_ratio = 0
    similar_word_index = -1
    for i, token in enumerate(template_list):
        ratio = SequenceMatcher(None, target_keyword, token).ratio()
        if ratio > max_ratio:
            max_ratio = ratio
            similar_word_index = i
    return similar_word_index


def find_best_matches(query: str, template_list: list[str], top_n=3, cutoff=0.8, best=True) -> list[tuple]:
    # è·å–æ»¡è¶³ cutoff çš„åŒ¹é…
    matches = get_close_matches(query, template_list, n=top_n, cutoff=cutoff)
    # è®¡ç®—æ¯ä¸ªåŒ¹é…é¡¹ä¸æŸ¥è¯¢è¯çš„ç›¸ä¼¼åº¦
    if matches:
        return [(match, SequenceMatcher(None, query, match).ratio(), template_list.index(match))
                for match in matches]
    # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œåˆ™å¼ºåˆ¶è¿”å›æœ€ç›¸ä¼¼çš„ 1 ä¸ª
    if best and template_list:
        scores = [(text, SequenceMatcher(None, query, text).ratio(), i)
                  for i, text in enumerate(template_list)]
        return [max(scores, key=lambda x: x[1])]
        # sorted( [item for item in scores if item[1] >= cutoff], key=lambda x: -x[1])[:top_n]

    return []  # text, score, idx [(åŒ¹é…æ–‡æœ¬, ç›¸ä¼¼åº¦, å¯¹åº”åŸå§‹idx)]


def fuzzy_match_template(query, template_list: list[str], threshold=0.8):
    if not isinstance(query, str):
        return None

    matches = get_close_matches(query, template_list, n=1, cutoff=threshold)
    return matches[0] if matches else None


def contains_chinese(text):
    # æ£€æµ‹å­—ç¬¦ä¸²ä¸­æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
    chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
    return bool(chinese_pattern.search(text))
    # return detect(text)=='zh-cn'


def contains_hebrew_arabic(text):
    return bool(re.search(r'[\u0590-\u05FF\u0600-\u06FF]', text))


def contains_cjk(text):
    """æ£€æµ‹æ˜¯å¦åŒ…å« CJKï¼ˆä¸­æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ï¼‰å­—ç¬¦"""
    return bool(re.search(r'[\u4e00-\u9fff\u3040-\u30ff\uac00-\ud7af]', text))


def convert_to_pinyin(text):
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºä¸­å›½åŸå¸‚åç§°ï¼ˆä»…ä¸­æ–‡ï¼‰ï¼Œç„¶åè½¬æ¢ä¸ºæ‹¼éŸ³
    if all('\u4e00' <= char <= '\u9fff' for char in text):
        return ''.join(lazy_pinyin(text))
    return text


def lang_detect_to_trans(text):
    t = detect(text)
    if t == 'zh-cn':
        t = 'zh'
    if t == 'no':
        t = 'zh' if contains_chinese(text) else 'auto'
    return t


def build_prompt(messages: list, use_role=False) -> str:
    """
    Build a single prompt string from a list of messages.
    Each message is expected to be a dictionary with 'role' and 'content' keys.
    This function concatenates all message contents, preserving the training format.
    """
    if all(isinstance(msg, str) for msg in messages):
        return '\n\n'.join(msg.strip() for msg in messages if msg.strip())
    if use_role:
        # OpenAI-style messages are transformed to a structured conversation format for Ollama.
        return "\n".join(
            f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content'].strip()}"
            for msg in messages if msg.get('role', 'user') != 'system')
    return "\n\n".join(msg["content"].strip() for msg in messages)


def alternate_chat_history(messages: list):
    # ç¡®ä¿ user å’Œ assistant æ¶ˆæ¯äº¤æ›¿å‡ºç°ï¼Œæ’å…¥é»˜è®¤æ¶ˆæ¯æˆ–åˆ é™¤å¤šä½™æ¶ˆæ¯
    i = 0
    while i < len(messages) - 1:
        # if (
        #     isinstance(message, dict) and
        #     message.get("role") in ["user", "assistant"] and
        #     isinstance(message.get("content"), str) and
        #     message["content"].strip()  # ç¡®ä¿ content éç©º
        # ):
        message = messages[i]
        next_message = messages[i + 1]
        # å¤„ç†è¿ç»­ç›¸åŒè§’è‰²çš„æƒ…å†µ
        if message['role'] == next_message['role']:  # messages.insert(0, messages.pop(i))
            if i % 2 == 0:
                if message['role'] == 'user':
                    messages.insert(i + 1, {'role': 'assistant', 'content': 'è¿™æ˜¯ä¸€ä¸ªé»˜è®¤çš„å›ç­”ã€‚'})
                else:
                    messages.insert(i + 1, {'role': 'user', 'content': 'è¯·é—®æ‚¨æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ'})
            else:
                del messages[i + 1]
                i -= 1
        i += 1
    return messages


def cut_chat_history(user_history: list[dict], max_size=33000, max_pairs=0, model_name="gpt-3.5-turbo"):
    """
    æ ¹æ® token æ•°æˆªæ–­å¯¹è¯å†å²ï¼Œä¿ç•™æœ€è¿‘çš„ä¸Šä¸‹æ–‡ã€‚

    :param user_history: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯é¡¹ {'role':..., 'content':...}
    :param max_size: æœ€å¤§å…è®¸çš„ token æ•°
    :param max_pairs: æœ€å¤§å…è®¸çš„ æ¶ˆæ¯å¯¹æ•°
    :param  model_name: tokenizer model
    :return: æˆªæ–­åçš„æ¶ˆæ¯åˆ—è¡¨
    32K tokens
    64K tokens
    128K token
    """

    if max_size <= 0 and max_pairs <= 0:
        return user_history

    tokenizer = get_tokenizer(model_name)

    pair_count = 0
    total_size = 0
    last_records = []

    i = len(user_history) - 1
    while i >= 0:
        if i >= 1:
            # å°è¯•ç»„æˆ user+assistant å¯¹
            if user_history[i - 1].get("role") == "user" and user_history[i].get("role") == "assistant":
                pair = user_history[i - 1:i + 1]
                pair_len = sum(lang_token_size(record.get("content", ""), tokenizer) for record in pair)

                if pair_count >= max_pairs > 0:
                    break
                if total_size + pair_len > max_size > 0:
                    break

                last_records = pair + last_records
                total_size += pair_len
                pair_count += 1
                i -= 2
                continue

        # å¦åˆ™å•æ¡å¤„ç†ï¼ˆå¦‚å¼€å¤´æˆ–éæˆå¯¹ï¼‰
        pair = [user_history[i]]
        pair_len = lang_token_size(pair[0].get("content", ""), tokenizer)

        if total_size + pair_len > max_size > 0:
            break

        last_records = pair + last_records
        total_size += pair_len
        i -= 1

    return last_records


def split_whitespace_nonwhitespace(s, max_len=5):
    # æŒ‰ç…§ ç©ºç™½/éç©ºç™½ äº¤æ›¿æ‹†åˆ†å­—ç¬¦ä¸²ï¼Œæ§åˆ¶æ¯æ®µçš„æœ€å¤§é•¿åº¦ï¼Œé¢„åˆ‡å‰²
    for k, g in groupby(s, key=str.isspace):
        chunk = list(g)
        for i in range(0, len(chunk), max_len):
            yield ''.join(chunk[i:i + max_len])


LINE_STOP_FLAG = (
    '.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', ')', 'ï¼‰', '"', 'â€', ':', 'ï¼š', ';', 'ï¼›', ']', 'ã€‘', '}', '}', '>', 'ã€‹', 'ã€', ',', 'ï¼Œ',
    '-', 'â€”', 'â€“',)
LINE_START_FLAG = ('(', 'ï¼ˆ', '"', 'â€œ', 'ã€', '{', 'ã€Š', '<', 'ã€Œ', 'ã€', 'ã€', '[',)


def find_last_punctuation(text, punctuations=("ã€‚", "ï¼Ÿ", "ï¼", "ï¼›", "ï¼š")):
    """æ‰¾åˆ°æ–‡æœ¬ä¸­æœ€åä¸€ä¸ªæœ‰æ•ˆçš„æ ‡ç‚¹ç¬¦å·ä½ç½®"""
    return max(text.rfind(p) for p in punctuations)


def is_punctuation_or_emoji(char):
    """æ£€æŸ¥å­—ç¬¦æ˜¯å¦ä¸ºç©ºæ ¼ã€æŒ‡å®šæ ‡ç‚¹æˆ–è¡¨æƒ…ç¬¦å·"""
    # å®šä¹‰éœ€è¦å»é™¤çš„ä¸­è‹±æ–‡æ ‡ç‚¹ï¼ˆåŒ…æ‹¬å…¨è§’/åŠè§’ï¼‰
    punctuation_set = {
        'ï¼Œ', ',',  # ä¸­æ–‡é€—å· + è‹±æ–‡é€—å·
        'ã€‚', '.',  # ä¸­æ–‡å¥å· + è‹±æ–‡å¥å·
        'ï¼', '!',  # ä¸­æ–‡æ„Ÿå¹å· + è‹±æ–‡æ„Ÿå¹å·
        '-', 'ï¼',  # è‹±æ–‡è¿å­—ç¬¦ + ä¸­æ–‡å…¨è§’æ¨ªçº¿
        'ã€'  # ä¸­æ–‡é¡¿å·
    }
    if char.isspace() or char in punctuation_set:
        return True
    # æ£€æŸ¥è¡¨æƒ…ç¬¦å·ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
    code_point = ord(char)
    emoji_ranges = [
        (0x1F600, 0x1F64F), (0x1F300, 0x1F5FF),
        (0x1F680, 0x1F6FF), (0x1F900, 0x1F9FF),
        (0x1FA70, 0x1FAFF), (0x2600, 0x26FF),
        (0x2700, 0x27BF)
    ]
    return any(start <= code_point <= end for start, end in emoji_ranges)


def get_string_no_punctuation_or_emoji(s):
    """å»é™¤å­—ç¬¦ä¸²é¦–å°¾çš„ç©ºæ ¼ã€æ ‡ç‚¹ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·,åªæ¸…ç†é¦–å°¾ï¼Œä¸å½±å“ä¸­é—´çš„å†…å®¹"""
    chars = list(s)
    # å¤„ç†å¼€å¤´çš„å­—ç¬¦
    start = 0
    while start < len(chars) and is_punctuation_or_emoji(chars[start]):
        start += 1
    # å¤„ç†ç»“å°¾çš„å­—ç¬¦
    end = len(chars) - 1
    while end >= start and is_punctuation_or_emoji(chars[end]):
        end -= 1
    return ''.join(chars[start:end + 1])


class AsyncAbortController:
    def __init__(self):
        self._abort_event = asyncio.Event()  # çº¿ç¨‹å®‰å…¨ threading.Event()

    def should_abort(self) -> bool:
        """æŸ¥è¯¢æ˜¯å¦å·²è§¦å‘ç»ˆæ­¢ä¿¡å·,å®æ—¶æ£€æŸ¥æ˜¯å¦ç»ˆæ­¢"""
        return self._abort_event.is_set()

    async def wait_abort(self):
        """ç­‰å¾…ç»ˆæ­¢ä¿¡å·ï¼ˆå¯ç”¨äºå¹¶å‘ awaitï¼‰,å¯ await ç­‰å¾…ä¸­æ–­"""
        await self._abort_event.wait()

    def abort(self):
        """å¤–éƒ¨è§¦å‘ç»ˆæ­¢ä¿¡å·,è§¦å‘ç»ˆæ­¢,æ˜¯å¦æå‰ç»ˆæ­¢,å¯ä¾›å¤–éƒ¨è§¦å‘"""
        self._abort_event.set()

    def reset(self):
        """æ¸…é™¤ç»ˆæ­¢ä¿¡å·ï¼Œä¸ºä¸‹ä¸€è½®ä»»åŠ¡åšå‡†å¤‡,é‡æ–°å¯åŠ¨å‰å¤ä½,å¯å¤šè½®å¤ç”¨"""
        self._abort_event.clear()


LLM_Controller = AsyncAbortController()


async def process_llm_stream(llm_responses_stream, token_size=20, model_name="gpt-3.5-turbo"):
    """
    å¤„ç†å¤§æ¨¡å‹è¿”å›çš„æ–‡æœ¬æµï¼Œå¹¶æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²äº¤ç»™ TTS æœ—è¯»ã€‚
    :param llm_responses_stream: å¤§æ¨¡å‹è¿”å›çš„æ–‡æœ¬æµ
    :param token_size: æ ‡ç‚¹ä¸è¶³æ—¶ï¼Œå…è®¸çš„æœ€å°ç¼“å†²åŒºé•¿åº¦
    :param  model_name: tokenizer model
    """
    response_message = []
    text_index = 0
    processed_chars = 0
    tokenizer = get_tokenizer(model_name)
    async for content in llm_responses_stream:
        response_message.append(content)
        if LLM_Controller.should_abort():  # å®æ—¶æ£€æŸ¥æ˜¯å¦ç»ˆæ­¢
            break

        # è·å–å½“å‰æœªå¤„ç†çš„æ–‡æœ¬
        full_text = "".join(response_message)
        current_text = full_text[processed_chars:]

        # æŸ¥æ‰¾æœ€åä¸€ä¸ªæœ‰æ•ˆæ ‡ç‚¹
        last_punct_pos = find_last_punctuation(current_text)
        if last_punct_pos != -1 or lang_token_size(current_text, tokenizer) > token_size:
            split_pos = last_punct_pos if last_punct_pos != -1 else token_size  # é€‰å–æœ€åˆé€‚çš„åˆ‡å‰²ç‚¹
            segment_text_raw = current_text[:split_pos + 1]
            segment_text = get_string_no_punctuation_or_emoji(segment_text_raw)  # å¤„ç†æ— æ•ˆå­—ç¬¦
            if segment_text:
                text_index += 1
                yield segment_text, text_index
                processed_chars += len(segment_text_raw)  # æ›´æ–°å·²å¤„ç†å­—ç¬¦ä½ç½®

    # å¤„ç†å‰©ä½™æœªåˆ†å‰²çš„æ–‡æœ¬
    remaining_text = "".join(response_message)[processed_chars:]
    if remaining_text:
        segment_text = get_string_no_punctuation_or_emoji(remaining_text)
        if segment_text:
            text_index += 1
            yield segment_text, text_index

    yield response_message, -1  # finish_task


async def start_llm_stream(new_llm_stream):
    """å¤ä½ç»ˆæ­¢ä¿¡å·ï¼Œå¹¶é‡æ–°å¯åŠ¨å¤§æ¨¡å‹æµ"""
    LLM_Controller.reset()  # é‡æ–°å¯åŠ¨å‰å¤ä½
    async for text, idx in process_llm_stream(new_llm_stream):
        if idx > 0:
            print(f"ğŸ”Š æœ—è¯»: {text}")


# æ”¯æŒçš„æ‰©å±•å
def get_local_suffix(folder_path, supported_suffix=None, recursive=False):
    supported_extensions = (ext.lower() for ext in supported_suffix or [".jpg", ".jpeg", ".png", ".bmp"])
    folder = Path(folder_path)
    if not folder.is_dir():
        raise ValueError(f"The path '{folder_path}' is not a valid directory.")
    pattern = "**/*" if recursive else "*"
    return [str(f_path) for f_path in folder.glob(pattern) if f_path.suffix.lower() in supported_extensions]


def get_doc_type(text):
    doc_type = 'text'
    sample = text[:5000].strip()

    # 1. ä»£ç å—ï¼ˆMarkdown é£æ ¼ï¼‰
    if '```' in sample and sample.count('```') > 2:
        return 'code'

    # 2. Markdown æ–‡æ¡£
    if sample.count('#') > 5 or re.search(r'\[.*?\]\(.*?\)', sample):
        return 'md'

    # 3. JSON æ ¼å¼
    if sample.startswith('{') or sample.startswith('['):
        try:
            import json
            json.loads(sample)
            return 'json'
        except Exception:
            pass

    # 4. HTML/XML
    if re.search(r'<(html|div|span|head|body|title)[\s>]', sample, re.IGNORECASE):
        return 'html'

    # 5. CSV / TSV
    if sample.count(',') > 5 and re.search(r'(?:\n|^)([^,\n]+,){2,}', sample):
        return 'csv'
    if '\t' in sample and re.search(r'(?:\n|^)([^\t\n]+\t){2,}', sample):
        return 'tsv'

    # 6. æ—¥å¿—æ–‡ä»¶
    if re.search(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', sample):
        return 'log'

    # é»˜è®¤æ–‡æœ¬
    return doc_type


def get_file_type(object_name: str) -> str:
    """
    æ ¹æ®æ–‡ä»¶åæˆ–è·¯å¾„åˆ¤æ–­æ–‡ä»¶ç±»å‹ã€‚

    :param object_name: æ–‡ä»¶åæˆ–è·¯å¾„
    :return: æ–‡ä»¶ç±»å‹ï¼ˆå¦‚ 'image', 'audio', 'video', 'text', 'compressed', '*'ï¼‰
    .pdf .txt .csv .doc .docx .xls .xlsx .ppt .pptx .md .jpeg .png .bmp .gif .svg .svgz .webp .ico .xbm .dib .pjp .tif .pjpeg .avif .dot .apng .epub .tiff .jfif .html .json .mobi .log .go .h .c .cpp .cxx .cc .cs .java .js .css .jsp .php .py .py3 .asp .yaml .yml .ini .conf .ts .tsx
    """
    if not object_name:
        return ""

    _, file_extension = os.path.splitext(object_name.lower())

    # å®šä¹‰æ–‡ä»¶ç±»å‹åˆ†ç±»
    file_types = {
        "image": [".png", ".jpg", ".jpeg", ".gif", ".bmp", ".svg", ".webp", ".tiff", ".heic", ".heif"],
        "audio": [".mp3", ".wav", ".ogg", ".aac", ".flac", ".m4a"],
        "video": [".mp4", ".avi", ".mkv", ".mov", ".wmv", ".flv", ".webm", ".3gp"],
        "text": [".txt", ".csv", ".md", ".html", ".json", ".xml"],
        "document": [".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".numbers"],
        "compressed": [".zip", ".rar", ".7z", ".tar", ".gz", ".bz2"],
        "code": [".py", ".java", ".c", ".cpp", ".js", ".ts", ".html", ".css", ".sql"]
    }

    for file_type, extensions in file_types.items():
        if file_extension in extensions:
            return file_type

    return "*"


def get_file_type_wx(object_name: str) -> str:
    if not object_name:  # object_name.endswith()
        return ""
    '''
    æ–‡æ¡£ï¼šDOCã€DOCXã€XLSã€XLSXã€PPTã€PPTXã€PDFã€Numbersã€CSV
    å›¾ç‰‡ï¼šJPGã€JPG2ã€PNGã€GIFã€WEBPã€HEICã€HEIFã€BMPã€PCDã€TIFF
    æ–‡ä»¶ä¸Šä¼ å¤§å°é™åˆ¶ï¼šæ¯ä¸ªæ–‡ä»¶æœ€å¤§512MBã€‚
    '''
    _, file_extension = os.path.splitext(object_name.lower())
    # æ ¹æ®æ–‡ä»¶åç¼€åˆ¤æ–­ç±»å‹
    if file_extension in [".png", ".jpg", ".jpeg", ".gif", ".bmp", ".svg", ".webp"]:
        return "image"
    elif file_extension in [".mp3", ".wav", ".ogg", ".aac", ".flac"]:
        return "audio"
    elif file_extension in [".mp4", ".avi", ".mkv", ".mov", ".wmv", ".flv", ".webm"]:
        return "video"
    elif file_extension in [".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".txt", ".csv",
                            '.zip', '.rar', '.html']:
        return "*"
    return ""
