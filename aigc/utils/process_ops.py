import io, os, sys, socket
import re, json
import joblib
import subprocess
import platform
import inspect, importlib, ast
from functools import partial, wraps  # cache, lru_cache, partial, wraps
import gc  # æ·»åŠ åƒåœ¾å›æ”¶æ¨¡å—
import psutil  # æ·»åŠ ç³»ç»Ÿç›‘æ§æ¨¡å—
import tracemalloc
from pathlib import Path
from collections import deque
from typing import Union, Iterable, Callable, AsyncGenerator, Any, get_origin, get_args
import aiofiles, asyncio
from concurrent.futures import ThreadPoolExecutor


def execute_code_results(text):
    """
    æ‰§è¡Œç»™å®šæ–‡æœ¬ä¸­çš„æ‰€æœ‰Pythonä»£ç å—ï¼Œå¹¶è¿”å›æ¯ä¸ªä»£ç å—çš„è¾“å‡ºã€å‘½åç©ºé—´å’Œé”™è¯¯ä¿¡æ¯ã€‚éœ€è¦åŒ…å«Pythonä»£ç å—çš„æ–‡æœ¬ã€‚
    ä»…ç”¨äºå·²çŸ¥å®‰å…¨çš„æœ¬åœ°ä»£ç å—æ‰§è¡Œã€‚ä¸è¦ç”¨äºä¸‹è½½ã€å®‰è£…æˆ–ç³»ç»Ÿçº§æ“ä½œã€‚
    ä»…é™æœ¬åœ°å·²ç»æœ‰çš„æˆ–æ˜ç¡®æŒ‡å‡ºçš„ï¼Œä»…é™æœ¬åœ°ç¯å¢ƒã€åªè¯»ã€æ— å¤–éƒ¨ä¾èµ–çš„ä»£ç å—ã€‚ä¸æ”¯æŒ pip å®‰è£…ã€ç³»ç»Ÿå‘½ä»¤ï¼ˆå¦‚ cdã€gitã€curl ç­‰ï¼‰ã€‚ä»…å½“å·²çŸ¥ä»£ç å¯ä¿¡ä¸”æ‰§è¡Œç¯å¢ƒæ”¯æŒæ—¶è°ƒç”¨ã€‚
    :param text:åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ª Python ä»£ç å—çš„çº¯æ–‡æœ¬å†…å®¹ã€‚æ”¯æŒ Markdown ä»£ç å—é£æ ¼çš„æ ¼å¼ã€‚
    :return:è¿è¡Œç»“æœ
    """
    from contextlib import redirect_stdout
    from .convert_ops import extract_python_code
    code_blocks = extract_python_code(text)
    results = []
    global_namespace = globals()  # å¼•ç”¨å…¨å±€å‘½åç©ºé—´ {"__builtins__": dict(__builtins__)}
    for code in code_blocks:
        local_namespace = {}  # ç”¨äºå­˜å‚¨ä»£ç çš„å±€éƒ¨å˜é‡
        captured_output = io.StringIO()  # ç”¨äºæ•è· `print` è¾“å‡º
        try:
            with redirect_stdout(captured_output):  # é‡å®šå‘ `print` è¾“å‡º
                exec(code, global_namespace, local_namespace)
                # exec(code, globals=None, locals=None)ç”¨äºåŠ¨æ€æ‰§è¡Œè¾ƒå¤æ‚çš„ä»£ç å—,ä¸è¿”å›ç»“æœ,éœ€è¦é€šè¿‡å…¨å±€æˆ–å±€éƒ¨å˜é‡è·å–ç»“æœ
            output = captured_output.getvalue()  # è·å– `print` çš„å†…å®¹
            results.append({
                "output": output.strip(),
                "namespace": local_namespace,
                "error": None
            })
        except Exception as e:
            results.append({
                "output": captured_output.getvalue().strip(),
                "namespace": local_namespace,
                "error": f"Error executing code block: {e},Code:{code}"
            })
        finally:
            captured_output.close()

    return results


def safe_eval_call(func_name: str, func_args: dict):
    allowed_names = {
        **globals(), **locals(),
        "__builtins__": {"str": str, "int": int, "float": float, "list": list, "dict": dict}
    }
    if func_name not in allowed_names:
        raise ValueError(f"ä¸å…è®¸åŠ¨æ€è°ƒç”¨å‡½æ•°ï¼š{func_name}")
    # compile(code, '<string>', 'eval')
    # eval(expression, globals=None, locals=None) æ‰§è¡Œå•ä¸ªè¡¨è¾¾å¼,åŠ¨æ€è®¡ç®—ç®€å•çš„è¡¨è¾¾å¼æˆ–ä»å­—ç¬¦ä¸²ä¸­è§£æå€¼
    return eval(f'{func_name}(**{func_args})', allowed_names)


def pip_installed_list():
    # from importlib.metadata import distributions
    # return sorted(
    #     [{"name": d.metadata["Name"], "version": d.version} for d in distributions()],
    #     key=lambda x: x["name"].lower()
    # )
    result = subprocess.run(
        [sys.executable, "-m", "pip", "list", "--format=json"],
        capture_output=True, text=True, check=True
    )
    return json.loads(result.stdout)


def pip_install(*packages, index_url="https://pypi.tuna.tsinghua.edu.cn/simple"):
    """
    å®‰è£… pip åŒ…ï¼ˆæ”¯æŒç©ºæ ¼/é€—å·åˆ†éš”çš„å¤šä¸ªåŒ…ï¼‰ï¼Œå¹¶å°è¯• importã€‚

    ç¤ºä¾‹ï¼š
    pip_install("pandas")
    pip_install("pandas scikit-learn")
    pip_install("numpy", "matplotlib==3.7.1")
    """
    pkg_list = []
    for p in packages:
        # æ‹†åˆ†æ”¯æŒç©ºæ ¼ã€é€—å·ç­‰æ ¼å¼
        if isinstance(p, str):
            parts = p.replace(",", " ").split()
            pkg_list.extend(parts)
        else:
            pkg_list.append(str(p))

    try:
        print(f"æ­£åœ¨å®‰è£…: {pkg_list}")
        subprocess.run([sys.executable, "-m", "pip", "install", "-i", index_url, *pkg_list],
                       # shell=(platform.system() == "Windows"),
                       check=True)
    except subprocess.CalledProcessError as e:
        print("âŒ å®‰è£…å¤±è´¥:", str(e))
        return {"error": str(e), "installed": pkg_list}

    return pkg_list


def import_packages(pkg_list):
    success_imports = []
    for pkg in pkg_list:
        try:
            mod_name = pkg.split("==")[0].split(">")[0].split("<")[0]
            importlib.import_module(mod_name)
            success_imports.append(mod_name)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è‡ªåŠ¨å¯¼å…¥ {pkg}: {e}")
    return success_imports


def git_repo_clone(repo_url: str, revision: str = None, add_to_sys_path: bool = True) -> Path:
    # è‡ªåŠ¨åŒ–ç¯å¢ƒéƒ¨ç½²ã€åŠ¨æ€åŠ è½½ Git ä»“åº“ä»£ç 
    repo_path = Path(repo_url.split("/")[-1].replace(".git", ""))
    if not repo_path.exists():  # æœ¬åœ°æ²¡æœ‰è¯¥ä»“åº“
        try:
            subprocess.run(["git", "clone", repo_url], check=True,
                           stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        except Exception as exc:
            print(f"Failed to clone the repository: {exc.stderr}")
            raise

        if revision:  # åˆ‡æ¢åˆ†æ”¯æˆ–æäº¤
            subprocess.Popen(["git", "checkout", revision], cwd=str(repo_path))
    if add_to_sys_path and str(repo_path.resolve()) not in sys.path:
        sys.path.insert(0, str(repo_path.resolve()))
        # åŠ å…¥ Python çš„ sys.pathï¼Œä»¥ä¾¿ importè¯¥ä»“åº“çš„æ¨¡å—

    return repo_path


def start_app_instance(port=7000, workers=4):
    return subprocess.Popen([
        "uvicorn", "main:app",  # å¯åŠ¨ main.py ä¸­çš„ app å®ä¾‹
        "--host", "127.0.0.1",
        "--port", str(port),
        "--workers", str(workers)
    ])


def configure_event_loop():
    if platform.system() != "Windows":
        try:
            import uvloop
            uvloop.install()
            print("ğŸš€ uvloop activated")
        except ImportError:
            print("âš ï¸ uvloop not available, using default event loop")
    else:
        # å¯ç”¨IOCP
        if sys.platform.startswith('win'):
            if sys.version_info >= (3, 8):
                # Python 3.8+ ä½¿ç”¨æ›´é«˜æ•ˆçš„ Proactor
                asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
            else:
                asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())  # ä½¿ç”¨ Selector
        print("ğŸ–¥ï¸ Windows detected, using optimized event loop policy")


def kill_process_tree(pid: int):
    import signal, contextlib
    """
    Kills all descendant processes of the given pid by sending SIGKILL.

    Args:
        pid (int): Process ID of the parent process
    """
    try:
        parent = psutil.Process(pid)
    except psutil.NoSuchProcess:
        return

    # Get all children recursively
    children = parent.children(recursive=True)

    # Send SIGKILL to all children first
    for child in children:
        with contextlib.suppress(ProcessLookupError):
            os.kill(child.pid, signal.SIGKILL)

    # Finally kill the parent
    with contextlib.suppress(ProcessLookupError):
        os.kill(pid, signal.SIGKILL)


def chunks_iterable(lst, n: int):
    """å°†å¤§æ•°æ®åˆ†æˆå°å—"""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


def is_iterable(obj):
    try:
        iter(obj)
        return not isinstance(obj, (str, bytes))
    except TypeError:
        return False


def named_partial(name, func, *args, **kwargs):
    p = partial(func, *args, **kwargs)
    p.__name__ = name
    return p


def merge_partial(task, *args, **kwargs):
    if isinstance(task, partial):
        return partial(task.func, *(task.args + args), **{**task.keywords, **kwargs})
    return partial(task, *args, **kwargs)


def is_empty_lambda(func):
    try:
        if callable(func) and getattr(func, "__name__", "") == "<lambda>" and len(
                inspect.signature(func).parameters) == 0:
            return func() == []
    except:
        return False
    return False


def async_to_sync(func: Callable, *args, **kwargs):
    '''åŒæ­¥ç¯å¢ƒä¸­è°ƒç”¨å¼‚æ­¥å‡½æ•°ï¼Œå¹¶ä¸”ç­‰å¾…ç»“æœè¿”å›'''
    coro = func(*args, **kwargs)
    if not inspect.iscoroutine(coro):
        raise TypeError("func must be an async function")
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(coro)  # æ²¡åœ¨äº‹ä»¶å¾ªç¯é‡Œ â†’ åˆ›å»ºæ–°å¾ªç¯æ‰§è¡Œ

    if loop.is_running():
        future = asyncio.run_coroutine_threadsafe(coro, loop)
        return future.result()  # æäº¤åˆ°å·²æœ‰å¾ªç¯ï¼Œé˜»å¡ç­‰å¾…ç»“æœ

    return loop.run_until_complete(coro)


def run_by_future(func: Callable, *args, **kwargs):
    """
    ä¸ç­‰å¾…ç»“æœï¼Œä¸é˜»å¡è°ƒç”¨è€… fire-and-forget
    """
    coro = func(*args, **kwargs)
    try:
        loop = asyncio.get_running_loop()
        loop.create_task(coro)
        # if loop.is_running():
        #    task = asyncio.ensure_future(coro)  # å·²æœ‰äº‹ä»¶å¾ªç¯.run_coroutine_threadsafe
        # while not task.done():
        #     time.sleep(0.05)  # é˜»å¡ç›´åˆ°å®Œæˆ
        # return task.result()
    except RuntimeError:
        asyncio.run(coro)  # æ²¡æœ‰äº‹ä»¶å¾ªç¯ â†’ åˆ›å»ºæ–°å¾ªç¯æ‰§è¡Œ


async def run_with_async(func: Callable, *args, **kwargs):
    """
    é€šç”¨æ–¹æ³•ï¼šæ ¹æ®å‡½æ•°æ˜¯å¦ä¸ºåç¨‹è‡ªåŠ¨é€‰æ‹© await æˆ–ç›´æ¥è°ƒç”¨
    æ”¯æŒåœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­ç»Ÿä¸€å¤„ç†åŒæ­¥/å¼‚æ­¥å‡½æ•°è°ƒç”¨
    # åŒæ­¥ä»£ç è½¬æ¢ä¸ºå¼‚æ­¥æ‰§è¡Œ,åœ¨åå°ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ,ä»¥é¿å…é˜»å¡ä¸»äº‹ä»¶å¾ªç¯,ä½¿ç”¨ await æ¥æ‰§è¡Œ
    # await asyncio.get_event_loop().run_in_executor(None, func, *args)
    :param func: å¾…æ‰§è¡Œå‡½æ•°ï¼ˆåŒæ­¥æˆ–å¼‚æ­¥ï¼‰
    :param args: ä½ç½®å‚æ•°
    :param kwargs: å…³é”®å­—å‚æ•°
    :return: å‡½æ•°è¿”å›ç»“æœ
    """
    if inspect.iscoroutinefunction(func):
        return await func(*args, **kwargs)
    else:
        return await asyncio.to_thread(func, *args, **kwargs)  # ç”¨ asyncio.to_thread ä»¥é¿å…é˜»å¡, ç­‰ä»·äºä¼ ç»Ÿçš„ run_in_executor


async def run_bound_tasks(bound_funcs: list):
    """
    å¹¶å‘æ‰§è¡Œä»»åŠ¡ï¼ˆæ”¯æŒåç¨‹å‡½æ•°ä¸æ™®é€šå‡½æ•°ï¼‰
    """
    if not bound_funcs:
        return []

    coro_tasks = []  # ç”Ÿæˆ coroutine
    for t in bound_funcs:
        if asyncio.iscoroutinefunction(t.func):
            coro_tasks.append(t())
        else:  # task() æ”¯æŒæ™®é€šå‡½æ•°
            coro_tasks.append(asyncio.to_thread(t))  # loop.run_in_executor(None, t))

    results = await asyncio.gather(*coro_tasks, return_exceptions=True)
    for t, res in zip(bound_funcs, results):
        if isinstance(res, Exception):
            print(f"Task {t.func.__name__} execution failed with error: {res}")
    return results


async def run_with_executor(func, args_list: list[tuple], max_workers: int = 10):
    """
    å¼‚æ­¥çº¿ç¨‹æ± æ‰§è¡Œå¤šä¸ªä»»åŠ¡
    :param func: è¦æ‰§è¡Œçš„åŒæ­¥å‡½æ•°
    :param args_list: å‚æ•°åˆ—è¡¨ï¼Œæ¯é¡¹æ˜¯ä¸€ä¸ª tupleï¼ˆä¼ ç»™ funcï¼‰
    :param max_workers: æœ€å¤§çº¿ç¨‹æ•°
    """
    loop = asyncio.get_running_loop()
    if len(args_list) == 1:
        return await loop.run_in_executor(None, func, *args_list[0])  # to_thread
    # å¤šä»»åŠ¡å¹¶å‘æ‰§è¡Œ
    with ThreadPoolExecutor(max_workers=max_workers) as pool:
        tasks = [loop.run_in_executor(pool, func, *args) for args in args_list]
        return await asyncio.gather(*tasks, return_exceptions=True)


async def run_with_semaphore(func, *args, semaphore=None, **kwargs):
    if semaphore:
        async with semaphore:
            return await func(*args, **kwargs)
    else:
        return await func(*args, **kwargs)


def run_togather(max_concurrent: int = 100, batch_size: int = -1, input_key: str = None, return_exceptions=True):
    """
    concurrency å¹¶å‘é™åˆ¶è£…é¥°å™¨ï¼Œæ”¯æŒæ‰¹é‡è°ƒç”¨æ§åˆ¶å¹¶å‘æ•°é‡
    å‚æ•°ï¼š
        max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ 100ï¼›è‹¥ <=0 åˆ™ä¸é™åˆ¶ã€‚
        batch_size: æ‰¹é‡å¤§å°ï¼Œé»˜è®¤ -1 è¡¨ç¤ºä¸åˆ†æ‰¹ï¼›è‹¥ >0 åˆ™æŒ‰æ‰¹è°ƒç”¨å‡½æ•°ã€‚
        - ç¬¬ä¸€ä¸ªå‚æ•°ä¸ºå•ä¸ªè¾“å…¥ï¼ˆå¦‚ str|list[str],list[tuple]ï¼‰ï¼Œæˆ–ç”¨ inputs=... å…³é”®å­—ä¼ å…¥ã€‚
        input_key: å¦‚æœä¸ä¸º Noneï¼Œåˆ™ä»¥å…³é”®å­—å‚æ•°æ–¹å¼ä¼ é€’è¾“å…¥å€¼ï¼Œå¦‚ func(**{input_key: x})
    ç”¨æ³•ï¼š
        @run_togather(max_concurrent=10, batch_size=16)
        async def create_embeddings(input: List[str]|str, ..., model='xxx'):
            ...
        process_func = run_togather(max_concurrent=100, input_key="input")(cls.generate_metadata)
        results = await process_func(inputs=func_list,**kwargs)
    """
    semaphore = asyncio.Semaphore(max_concurrent) if max_concurrent > 0 else None

    def decorator(func: Callable[..., Any]):
        @wraps(func)
        async def wrapper(*args, inputs: list = None, **kwargs):
            _args = list(args)
            if inputs is None and _args:
                inputs = _args.pop(0)  # å…è®¸ positional æ–¹å¼ä¼  list
            if not isinstance(inputs, list):
                inputs = [inputs]

            async def run_semaphore(x):
                if input_key:
                    coro = func(*_args, **{input_key: x}, **kwargs)
                else:
                    coro = func(x, *_args, **kwargs)
                if semaphore:
                    async with semaphore:
                        return await coro
                return await coro

            if batch_size <= 0:
                # æ‰€æœ‰ input ç‹¬ç«‹è¯·æ±‚
                tasks = [run_semaphore(item) for item in inputs]
            else:
                tasks = [run_semaphore(inputs[i:i + batch_size]) for i in range(0, len(inputs), batch_size)]

            return await asyncio.gather(*tasks, return_exceptions=return_exceptions)  # ç¡®ä¿ä»»åŠ¡å–æ¶ˆæ—¶ä¸ä¼šå¼•å‘å¼‚å¸¸ï¼Œå¹¶å‘æ‰§è¡Œå¤šä¸ªå¼‚æ­¥ä»»åŠ¡

        return wrapper

    return decorator


def run_repeat(max_concurrent: int = 100, repeat: int = 1, return_exceptions: bool = True):
    # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘,æŒ‰å®Œæˆé¡ºåºè¿”å›(index, result),å¹¶å‘ç”Ÿæˆå™¨è£…é¥°å™¨ï¼ŒAIå¤šæ¬¡ç”Ÿæˆ
    semaphore = asyncio.Semaphore(max_concurrent) if max_concurrent > 0 else None

    def decorator(func: Callable[..., Any]):
        @wraps(func)
        async def async_generator(*args, **kwargs) -> AsyncGenerator[tuple[int, Any], None]:
            async def worker(index):
                try:
                    if semaphore:
                        async with semaphore:
                            return index, await func(*args, **kwargs)
                    return index, await func(*args, **kwargs)
                except Exception as e:
                    if return_exceptions:
                        return index, e  # isinstance(r, Exception)
                    raise

            tasks = [asyncio.create_task(worker(i)) for i in range(repeat)]
            # ä½¿ç”¨ as_completed æŒ‰å®Œæˆé¡ºåºå¤„ç†ç»“æœ
            for coro in asyncio.as_completed(tasks):
                yield await coro  # (index, result or exception)/ task.get_coro().cr_frame.f_locals['index']

        return async_generator

    return decorator


def run_generator(max_concurrent: int = 100, input_key: str = None, return_exceptions: bool = True):
    """
    å¹¶å‘ã€å¼‚å¸¸å¤„ç†ã€ç´¢å¼•æ˜ å°„éƒ½ç»§æ‰¿è‡ª run_repeat
    å¹¶å‘ç”Ÿæˆå™¨è£…é¥°å™¨ï¼Œå°†å¼‚æ­¥å‡½æ•°è½¬æ¢ä¸ºæŒ‰å®Œæˆé¡ºåºç”Ÿæˆç»“æœçš„å¼‚æ­¥ç”Ÿæˆå™¨
    async for idx, res in ai_generates(inputs=[...])
    å‚æ•°:
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        input_key: å¦‚æœä¸ä¸º Noneï¼Œåˆ™ä»¥å…³é”®å­—å‚æ•°æ–¹å¼ä¼ é€’è¾“å…¥å€¼
        return_exceptions: æ˜¯å¦å°†å¼‚å¸¸ä½œä¸ºç»“æœè¿”å›ï¼Œè€Œä¸æ˜¯æŠ›å‡º

    è¿”å›:
        å¼‚æ­¥ç”Ÿæˆå™¨ï¼ŒæŒ‰å®Œæˆé¡ºåºç”Ÿæˆ (index, result)
    """

    def decorator(func: Callable[..., Any]):
        @wraps(func)
        async def async_generator(*args, inputs: list = None, **kwargs):
            _args = list(args)
            if inputs is None and _args:  # è·å–è¾“å…¥åˆ—è¡¨
                inputs = _args.pop(0)
            if not isinstance(inputs, list):
                inputs = [inputs]

            semaphore = asyncio.Semaphore(max_concurrent) if max_concurrent > 0 else None

            async def wrapper(index: int):
                try:
                    if input_key:
                        coro = func(*_args, **{input_key: inputs[index]}, **kwargs)
                    else:
                        coro = func(inputs[index], *_args, **kwargs)
                    if semaphore:
                        async with semaphore:
                            return index, await coro
                    return index, await coro
                except Exception as e:
                    if return_exceptions:
                        return index, e
                    raise

            tasks = [asyncio.create_task(wrapper(i)) for i in range(len(inputs))]
            for coro in asyncio.as_completed(tasks):
                yield await coro  # (index, result æˆ– exception)

        return async_generator

    return decorator


def make_runner(func: Callable, max_concurrent: int = 10, stream: bool = True, input_key: str = None,
                return_exceptions=True, **kwargs):
    """
    æ ¹æ® stream å‚æ•°ï¼Œè¿”å› run_generator æˆ– run_togather åŒ…è£…åçš„å‡½æ•°
    """
    if stream:
        @run_generator(max_concurrent=max_concurrent, input_key=input_key, return_exceptions=return_exceptions)
        async def wrapped(item):
            return await func(item, **kwargs)
    else:
        @run_togather(max_concurrent=max_concurrent, batch_size=-1, input_key=input_key,
                      return_exceptions=return_exceptions)
        async def wrapped(data: tuple[int, Any]):
            idx, item = data
            return idx, await func(item, **kwargs)
    return wrapped


async def runner_togather_sample(params: list, func: Callable, max_concurrent: int = 10, stream=True,
                                 input_key: str = None, **kwargs):
    """
     é€šç”¨å¹¶å‘æ‰§è¡Œå…¥å£
     :param params: è¾“å…¥åˆ—è¡¨
     :param func: å•ä¸ªå…ƒç´ å¤„ç†å‡½æ•° async def func(item, **kwargs)
     :param stream: True -> run_generator (æµå¼è¾“å‡º), False -> run_togather (æ‰¹é‡è¾“å‡º)
     :param max_concurrent: æœ€å¤§å¹¶å‘æ•°
     :param input_key
     """
    runner = make_runner(func, max_concurrent=max_concurrent, stream=stream, input_key=input_key, **kwargs)

    def wrap_result(i, r):
        """ç»Ÿä¸€ç»“æœæ ¼å¼ï¼ŒåŒºåˆ† dict / Exception / æ™®é€šå€¼"""
        if isinstance(r, dict):
            return {'id': i, **r}
        elif isinstance(r, Exception):
            return {'id': i, 'error': str(r)}
        else:
            return {'id': i, 'value': r}

    if stream:
        results = []
        async for i, r in runner(inputs=params):
            print(f"[stream] idx={i} res={r}")
            # yield wrap_result(i, r)
            results.append(wrap_result(i, r))
        return sorted(results, key=lambda x: x['id'])
    else:
        results = await runner(inputs=[(i, p) for i, p in enumerate(params)])  # å·²ç»æ˜¯ [(idx, result), ...] ä¿æŒè¾“å…¥é¡º
        return [wrap_result(i, r) for i, r in results]


def run_by_threads(max_concurrency: int = 10, repeat: int = 1):
    """
    é€šç”¨å¤šçº¿ç¨‹æ‰¹å¤„ç†è£…é¥°å™¨
    :param max_concurrency: å¹¶å‘çº¿ç¨‹æ•°
    :param repeat: æ¯ä¸ªä»»åŠ¡é‡å¤æ¬¡æ•°
    """
    import threading
    from queue import Queue
    def decorator(func):
        @wraps(func)
        def wrapper(inputs: list, **kwargs):
            results = [None] * (len(inputs) * repeat)
            lock = threading.Lock()
            q = Queue()

            # å¡«å……é˜Ÿåˆ—
            for i, x in enumerate(inputs):
                for j in range(repeat):
                    q.put((i, j, x))

            def worker():
                while True:
                    item = q.get()
                    if item is None:
                        break
                    _i, _j, _x = item
                    idx: int = _i * repeat + _j
                    try:
                        res = func(_x, **kwargs)
                        with lock:
                            results[idx] = res
                    except Exception as e:
                        with lock:
                            results[idx] = {"error": str(e), "task": item}
                    finally:
                        q.task_done()

            # å¯åŠ¨çº¿ç¨‹
            threads = []
            for _ in range(max_concurrency):
                t = threading.Thread(target=worker)
                t.start()
                threads.append(t)

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            q.join()

            # åœæ­¢çº¿ç¨‹
            for _ in range(max_concurrency):
                q.put(None)
            for t in threads:
                t.join()

            return results

        return wrapper

    return decorator


def class_property(attr_name: str):
    """
    ç¼“å­˜è£…é¥°å™¨ï¼Œæ”¯æŒé¦–æ¬¡è°ƒç”¨ç”Ÿæˆå€¼å¹¶ç¼“å­˜åˆ°ç±»å±æ€§ã€‚
    è‡ªå®šä¹‰ç¼“å­˜å±æ€§åï¼Œé€‚åˆæ— å‚æˆ–å›ºå®šå‚æ•°çš„æ‡’åŠ è½½ã€‚
    ç”¨äºç±»çº§æ‡’åŠ è½½ï¼ˆlazy loadï¼‰å‹ç±»å±æ€§ã€‚ä»…æ£€æŸ¥å½“å‰ç±»ã€‚
    :param attr_name: ç¼“å­˜å±æ€§å
    @class_property("cached_value")
    """

    def decorator(func) -> classmethod:
        def wrapper(cls, *args):
            if not hasattr(cls, attr_name):
                print(f"{attr_name} -> {cls.__name__}.{func.__name__}")
                setattr(cls, attr_name, func(cls, *args))
            return getattr(cls, attr_name)

        return classmethod(wrapper)

    return decorator


def chainable_method(func):
    """è£…é¥°å™¨ï¼Œä½¿æ–¹æ³•æ”¯æŒé“¾å¼è°ƒç”¨ï¼Œä¿ç•™æ˜¾å¼è¿”å›å€¼"""

    @wraps(func)
    def wrapper(self, *args, **kwargs):
        result = func(self, *args, **kwargs)
        return self if result is None else result

    return wrapper


_StringLikeT = Union[bytes, str, memoryview]


def list_or_args_keys(keys: Union[_StringLikeT, Iterable[_StringLikeT]],
                      args: tuple[_StringLikeT, ...] = None) -> list[_StringLikeT]:
    # å°† keys å’Œ args åˆå¹¶æˆä¸€ä¸ªæ–°çš„åˆ—è¡¨
    # returns a single new list combining keys and args
    try:
        iter(keys)
        # a string or bytes instance can be iterated, but indicates
        # keys wasn't passed as a list
        if isinstance(keys, (bytes, str)):
            keys = [keys]
        else:
            keys = list(keys)  # itertools.chain.from_iterable(keys)
    except TypeError:
        keys = [keys]
    if args:
        keys.extend(args)
    return keys


def get_function_parameters(func) -> list:
    """è¿”å›å‚æ•°ååˆ—è¡¨"""
    return list(inspect.signature(func).parameters.keys())


def get_function_params(func) -> dict:
    """åªå–å¯ä¼ é€’çš„å‚æ•° ï¼ˆæ’é™¤ *argsï¼‰"""
    signature = inspect.signature(func)
    return {
        k: (v.default if v.default is not inspect.Parameter.empty else None)
        for k, v in signature.parameters.items()
        if v.kind in (v.POSITIONAL_OR_KEYWORD, v.KEYWORD_ONLY)
    }


def remove_function_decorators(func) -> str:
    """
    è·å–å‡½æ•°æºç ï¼Œå»æ‰æ‰€æœ‰è£…é¥°å™¨
    """
    source = inspect.getsource(func)  # è·å–åŸå§‹ä»£ç  function_code
    tree = ast.parse(source)  # è§£æ AST

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):  # æ‰¾åˆ°å‡½æ•°å®šä¹‰
            node.decorator_list = []  # æ¸…ç©ºè£…é¥°å™¨åˆ—è¡¨

    return ast.unparse(tree)  # é‡æ–°è½¬æ¢æˆä»£ç 
    # import textwrap
    # textwrap.dedent(source)  # å¤„ç†ç¼©è¿›


def strip_kwargs_wrapper(func):
    """è¿”å›ä¸€ä¸ªå»æ‰ **kwargs çš„åŒ…è£…å‡½æ•°"""
    sig = inspect.signature(func)
    allowed_params = []
    # any(p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values())
    for p in sig.parameters.values():
        if p.kind == inspect.Parameter.VAR_KEYWORD:
            continue
        if p.annotation == inspect._empty:
            allowed_params.append(p)
        elif isinstance(p.annotation, type) and p.annotation.__module__.startswith("httpx"):
            continue  # è·³è¿‡ httpx ç­‰å¤æ‚ç±»å‹
        else:
            allowed_params.append(p)
    # åˆ›å»ºæ–°çš„å‡½æ•°ç­¾å
    new_sig = inspect.Signature(allowed_params)

    @wraps(func)
    def wrapper(*args, **kwargs):
        bound_args = new_sig.bind(*args, **kwargs)
        bound_args.apply_defaults()
        return func(*bound_args.args, **bound_args.kwargs)

    wrapper.__signature__ = new_sig  # æ›¿æ¢ç­¾åï¼ˆä»…ç”¨äºå·¥å…·æ³¨å†Œï¼Œä¸å½±å“å®é™…è°ƒç”¨ï¼‰
    return wrapper


def python_type_to_json_schema_type(python_type):
    origin = get_origin(python_type) or python_type

    # å¤„ç† Union/Optional
    if origin is Union:
        args = [a for a in get_args(python_type) if a is not type(None)]
        if len(args) == 1:
            return python_type_to_json_schema_type(args[0])
        else:
            return "string"  # fallback for complex Unions

    mapping = {
        str: "string",
        int: "integer",
        float: "number",
        bool: "boolean",
        list: "array",
        dict: "object",
    }
    return mapping.get(origin, "string")


def extract_function_metadatas(code):
    # ç”¨äºè§£æPythonå‡½æ•°å¹¶æå–å…ƒæ•°æ®,ä»æºä»£ç çº§åˆ«æå–è¯¦ç»†çš„ä»£ç ä¿¡æ¯
    # ä½¿ç”¨ASTæ¥è§£æPythonä»£ç ,å°† Python æºä»£ç ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰è½¬æ¢ä¸ºæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰
    tree = ast.parse(code)
    functions = []

    # éå†ASTèŠ‚ç‚¹ï¼Œæ‰¾åˆ°å‡½æ•°å®šä¹‰
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            function_name = node.name
            arguments = [arg.arg for arg in node.args.args]
            docstring = ast.get_docstring(node) or ""
            function_code = ast.unparse(node) if hasattr(ast, 'unparse') else ast.dump(node)
            # if generate_docstring and not docstring:
            #     docstring = generate_docstring(function_name, function_code)

            metadata = {
                'name': function_name,
                'args': arguments,
                'docstring': docstring,
                'code': function_code
            }
            functions.append(metadata)

    return functions


def extract_function_metadata(func) -> dict:
    # è·å–å‡½æ•°çš„åç§°ã€å‚æ•°ä»¥åŠdocstring
    func_name = func.__name__
    # print(f"[tools] æœªæ‰¾åˆ°ç¼“å­˜: {func_name},ç”Ÿæˆ metadata")
    # è·å–å‡½æ•°å‚æ•°åˆ—è¡¨
    signature = inspect.signature(func)
    docstring = func.__doc__

    parameters = {}
    required_params = []

    for param_name, param in signature.parameters.items():
        param_type = param.annotation if param.annotation != inspect.Parameter.empty else str
        json_type = python_type_to_json_schema_type(param_type)
        # å¦‚æœæœ‰ç±»å‹æç¤ºï¼Œæ¨æ–­å‚æ•°ç±»å‹str(param.annotation).replace("<class '", "").replace("'>", "")
        param_info = {
            "type": json_type,
            "description": f"Description of the {param_name} parameter."
        }

        if param.default == inspect.Parameter.empty:
            required_params.append(param_name)
        else:
            param_info["default"] = param.default
        parameters[param_name] = param_info

    # åˆå§‹metadataç»“æ„
    metadata = {
        "type": "function",
        "function": {
            "name": func_name,
            "description": docstring or "No description available.",
            "parameters": {
                "type": "object",
                "properties": parameters,
                "required": required_params
            },
            # "x-url":å·¥å…·çš„å®é™… API åœ°å€
        },
        # "func": func,
    }
    return metadata


class ClassMethodRegistry:
    """ç±»æ–¹æ³•æ³¨å†Œè¡¨ - ä½¿ç”¨ç±»å˜é‡"""
    _registry: dict[str, dict] = {}
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    @classmethod
    def register(cls, target_class, exclude: list[str] = None):
        """æ³¨å†Œä¸€ä¸ªç±»åŠå…¶æ‰€æœ‰ç±»æ–¹æ³•"""
        if exclude is None:
            exclude = []

        class_name = target_class.__name__
        class_info = {
            "class": target_class,
            "doc": target_class.__doc__,
            "methods": {}
        }

        # æ”¶é›†æ‰€æœ‰ç±»æ–¹æ³•
        for name in dir(target_class):
            if name.startswith('_') or name in exclude:
                continue
            attr = getattr(target_class, name)  #
            if callable(attr):
                func = attr.__func__ if isinstance(attr, classmethod) else attr
                sig = inspect.signature(attr)
                class_info["methods"][name] = {
                    "function": func,
                    "doc": func.__doc__,
                    "signature": sig,
                    "type": cls.determine_method_type(sig, attr, target_class.__dict__.get(name, None))
                }

        cls._registry[class_name] = class_info
        print(f"Registered {class_name} with [{len(class_info['methods'])}] methods.")
        # print(f"Registered {class_name} with methods: {list(cls._registry[class_name]['methods'].keys())}")
        return target_class

    @classmethod
    def call_method(cls, class_name: str, method_name: str, *args, **kwargs):
        """è°ƒç”¨æ³¨å†Œçš„ç±»æ–¹æ³•"""
        if class_name not in cls._registry:
            raise ValueError(f"Class '{class_name}' not registered")

        class_info = cls._registry[class_name]
        if method_name not in class_info["methods"]:
            raise ValueError(
                f"Method '{method_name}' not found in class '{class_name}'. Available: {list(class_info['methods'].keys())}")

        method_info = class_info["methods"][method_name]
        func = getattr(class_info["class"], method_name, method_info["function"])
        try:
            if method_info.get("type") == "instance":
                instance = class_info["class"]()  # å¯¹äºå®ä¾‹æ–¹æ³•ï¼Œåˆ›å»ºå®ä¾‹åè°ƒç”¨
                return func(instance, *args, **kwargs)
            return func(*args, **kwargs)  # å¯¹äºç±»æ–¹æ³•å’Œé™æ€æ–¹æ³•ï¼Œç›´æ¥è°ƒç”¨
        except TypeError as e:
            raise TypeError(f"å‚æ•°ä¸åŒ¹é…: {e}")

    @classmethod
    def list_classes(cls) -> dict[str, Any] | None:
        """åˆ—å‡ºæ‰€æœ‰æ³¨å†Œçš„ç±»"""
        return {
            name: {
                "doc": info["doc"],
                "methods": list(info["methods"].keys())
            }
            for name, info in cls._registry.items()
        }

    @classmethod
    def get_class_info(cls, class_name: str) -> dict[str, Any] | None:
        """è·å–ç±»çš„è¯¦ç»†ä¿¡æ¯"""
        if class_name not in cls._registry:
            raise ValueError(f"Class '{class_name}' not registered")

        info = cls._registry[class_name]
        method_details = {}

        for method_name, method_info in info["methods"].items():
            method_details[method_name] = {
                "doc": method_info["doc"],
                "type": method_info.get("type", cls.determine_method_type(method_info["signature"],
                                                                          method_info["function"])),
                "parameters": cls.get_parameters_info(method_info["signature"]),
            }

        return {
            "class": class_name,
            "doc": info["doc"],
            "methods": method_details
        }

    @staticmethod
    def get_parameters_info(signature) -> dict[str, Any]:
        """è·å–å‚æ•°ä¿¡æ¯"""
        params = {}
        for param_name, param in signature.parameters.items():
            if param_name in ['cls', 'self']:  # è·³è¿‡cls,selfå‚æ•°
                continue
            params[param_name] = {
                "kind": str(param.kind),
                "default": param.default if param.default != param.empty else None,
                "annotation": str(param.annotation) if param.annotation != param.empty else "any"
            }
        return params

    @staticmethod
    def determine_method_type(signature, method_obj=None, raw_obj=None) -> str:
        """ç¡®å®šæ–¹æ³•çš„ç±»å‹"""
        # ä¼˜å…ˆä»ç±»å®šä¹‰æœ¬èº«ï¼ˆæœªç»‘å®šï¼‰ä¸­å–
        if isinstance(raw_obj, classmethod):
            return "classmethod"
        if isinstance(raw_obj, staticmethod):
            return "staticmethod"

        params = list(signature.parameters.values())
        # å¦‚æœæœ‰å‚æ•°ï¼Œæ£€æŸ¥ç¬¬ä¸€ä¸ªå‚æ•°å
        if params:
            first_param = params[0].name
            if first_param == 'cls':
                return "classmethod"
            elif first_param == 'self':
                return "instance"

        # åˆ¤æ–­ç»‘å®šå¯¹è±¡ï¼ˆé€‚ç”¨äº bound methodï¼‰
        if method_obj is not None and hasattr(method_obj, "__self__"):
            bound_self = getattr(method_obj, "__self__")
            if isinstance(bound_self, type):
                return "classmethod"
            elif bound_self is not None:
                return "instance"

        # é»˜è®¤è®¤ä¸ºæ˜¯ç±»æ–¹æ³•ï¼ˆå¯¹äºä½¿ç”¨è‡ªå®šä¹‰@class_propertyè£…é¥°å™¨çš„æ–¹æ³•ï¼‰
        return "classmethod"

    @classmethod
    def clear_registry(cls):
        """æ¸…ç©ºæ³¨å†Œè¡¨ï¼ˆä¸»è¦ç”¨äºæµ‹è¯•ï¼‰"""
        cls._registry.clear()


def register_class(cls, exclude: list[str] = None):
    """è£…é¥°å™¨ï¼Œç”¨äºè‡ªåŠ¨æ³¨å†Œç±»"""
    return ClassMethodRegistry.register(cls, exclude)


def load_class_from_string(class_path: str, path=None):
    """æŒ‰å­—ç¬¦ä¸²è·¯å¾„åŠ¨æ€åŠ è½½ç±» path="/plugins" """
    path_in_sys = False
    if path:
        if path not in sys.path:
            path_in_sys = True
            sys.path.insert(0, path)  # ä¸´æ—¶å…è®¸ä»è‡ªå®šä¹‰ç›®å½•åŠ è½½æ¨¡å—

    try:
        module_name, class_name = class_path.rsplit('.', 1)
        module = importlib.import_module(module_name)
        cls = getattr(module, class_name)
        return cls
    finally:
        if path and path_in_sys:
            sys.path.remove(path)  # æ¢å¤åŸå§‹å¯¼å…¥ç¯å¢ƒ


def get_module_functions(module_name: str = None):
    module = importlib.import_module(module_name) if module_name else inspect.getmodule(inspect.currentframe())
    module_name = module.__name__
    funcs = inspect.getmembers(module, inspect.isfunction)
    local_funcs = [(name, func) for name, func in funcs if func.__module__ == module_name]
    # for name, _func in funcs:
    #     print(f"Function: {name}")
    return local_funcs


def functions_registry(functions_list: list, safe_path=True, module_name: str = None) -> dict:
    """
    æ ¹æ®å‡½æ•°åç§°åˆ—è¡¨,åˆ›å»ºå…¨å±€å‡½æ•°æ³¨å†Œè¡¨,æˆ–è€…æŒ‡å®šæ¨¡å—ä¸­åŠ¨æ€åŠ è½½
    1. ä»å½“å‰å…¨å±€ä½œç”¨åŸŸæŸ¥æ‰¾å‡½æ•°åï¼›
    2. æŒ‡å®š module_nameï¼Œæ‰¹é‡ä»è¯¥æ¨¡å—åŠ è½½ï¼›
    3. ä½¿ç”¨ 'module.path:func' æ ¼å¼ï¼Œå•ä¸ªåŠ¨æ€åŠ è½½ã€‚

    :param functions_list: éœ€è¦æ³¨å†Œçš„å‡½æ•°ååˆ—è¡¨
    :param safe_path: å–æ¶ˆä¸æ£€æŸ¥æ˜¯å¦å¯è°ƒç”¨ã€‚
    :param module_name: æ¨¡å—åç§°ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰ï¼Œé€‚åˆä»ä¸€ä¸ªæ¨¡å—ä¸­åŒ¹é…åŠ è½½å¤šä¸ªå‡½æ•°ã€‚
    :return: Dict[str, Callable[..., Any]]
    """
    if not safe_path:
        module = importlib.import_module(module_name) if module_name else globals()
        return {name: getattr(module, name)
                for name in functions_list
                if hasattr(module, name) and callable(getattr(module, name))}

    return functions_registry_safe(functions_list, module_name)


def functions_registry_safe(functions_list: list, module_name: str = None) -> dict:
    """
    æ ¹æ®å‡½æ•°åç§°åˆ—è¡¨,åˆ›å»ºå…¨å±€å‡½æ•°æ³¨å†Œè¡¨,æˆ–è€…æŒ‡å®šæ¨¡å—ä¸­åŠ¨æ€åŠ è½½,æ£€æŸ¥æ˜¯å¦å¯è°ƒç”¨
    1. ä»å½“å‰å…¨å±€ä½œç”¨åŸŸæŸ¥æ‰¾å‡½æ•°åï¼›
    2. æŒ‡å®š module_nameï¼Œæ‰¹é‡ä»è¯¥æ¨¡å—åŠ è½½ï¼›
    3. ä½¿ç”¨ 'module.path:func' æ ¼å¼ï¼Œå•ä¸ªåŠ¨æ€åŠ è½½ã€‚ä¼˜å…ˆåŒ¹é…

    :param functions_list: éœ€è¦æ³¨å†Œçš„å‡½æ•°ååˆ—è¡¨
    :param module_name: æ¨¡å—åç§°ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰ï¼Œé€‚åˆä»ä¸€ä¸ªæ¨¡å—ä¸­åŠ è½½å¤šä¸ªå‡½æ•°ã€‚
    :return: Dict[str, Callable[..., Any]]
    """
    # å¦‚æœæ²¡æœ‰æ¨¡å—åï¼Œå›é€€åˆ°å…¨å±€ä½œç”¨åŸŸ
    try:
        module = importlib.import_module(module_name) if module_name else globals()
    except ModuleNotFoundError:
        print(f"Module '{module_name}' not found.")
        module = globals()

    registry = {}
    for name in functions_list:
        if name in registry:  # é¿å…é‡å¤è¦†ç›–ï¼Œåªæ³¨å†Œç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„
            continue
        try:
            if ":" in name:
                module_path, func_name = name.rsplit(":", 1)
                _module = importlib.import_module(module_path)
                func_obj = getattr(_module, func_name, None)
                name = func_name
            else:
                func_obj = getattr(module, name, None)

            if not callable(func_obj):
                raise ValueError(f"å‡½æ•° {name} ä¸å­˜åœ¨æˆ–ä¸æ˜¯å¯è°ƒç”¨å¯¹è±¡,æœªåœ¨å½“å‰ä½œç”¨åŸŸä¸­æ‰¾åˆ°,å¯èƒ½æœªå¯¼å…¥æˆ–æ¨¡å—æœªæŒ‡å®šã€‚")

            registry[name] = func_obj

        except ModuleNotFoundError:
            print(f"Module '{module_name}','{name}' not found.")
            registry[name] = None
            # importlib.reload(module_path)

        except Exception as e:
            print(f"[âš ï¸] åŠ è½½å‡½æ•°å¤±è´¥: {name} â†’ {type(e).__name__}: {e}")
            registry[name] = None

    return registry


def parse_tool_text(text):
    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ¥åŒ¹é… <tags>, <tool_call>, <content> åŠå…¶å†…å®¹
    tags_pattern = r'<tags>(.*?)</tags>'
    tool_call_pattern = r'<tool_call>(.*?)</tool_call>'
    content_pattern = r'<content>(.*?)</content>'
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾åŒ¹é…çš„å†…å®¹
    tags_match = re.search(tags_pattern, text, re.DOTALL)
    tool_call_match = re.search(tool_call_pattern, text, re.DOTALL)
    content_match = re.search(content_pattern, text, re.DOTALL)
    # æå–åŒ¹é…çš„å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    tags = tags_match.group(1).strip() if tags_match else ""
    tool_call = tool_call_match.group(1).strip() if tool_call_match else ""
    content = content_match.group(1).strip() if content_match else ""
    # å°†æå–çš„å†…å®¹å­˜å‚¨åœ¨å­—å…¸ä¸­
    result = {
        "tags": tags,
        "tool_call": tool_call,
        "content": content
    }
    return result


def openai_tools_to_mcp(tools: list[dict]):
    def decorator(mcp_server):
        for tool in tools:
            if tool["type"] != "function":
                continue

            func_info = tool["function"]

            def make_handler(func_name, func_desc):
                async def handler(params):
                    # è¿™é‡Œå¯ä»¥æ·»åŠ é€šç”¨å¤„ç†é€»è¾‘
                    print(f"Handling {func_name} with params: {params}")
                    # å®é™…å¤„ç†å‡½æ•°åº”è¯¥åœ¨åˆ«å¤„å®šä¹‰
                    return await globals()[f"handle_{func_name}"](params)

                handler.__name__ = func_name
                handler.__doc__ = func_desc
                return handler

            mcp_server.add_handler(
                action=func_info["name"],
                handler=make_handler(func_info["name"], func_info["description"])
            )
        return mcp_server

    return decorator


def deduplicate_functions_by_name(tools: list[dict]) -> list[dict]:
    seen = set()
    tools_metadata = []
    for tool in tools:
        name = tool.get("function", {}).get("name") or tool.get("name")
        if name and name not in seen:
            seen.add(name)
            tools_metadata.append(tool)
    return tools_metadata


def get_tools_params(tools: list[dict]) -> list[tuple[str, dict]]:
    return [(tool["type"], tool.get(tool["type"], {}))
            for tool in tools
            if isinstance(tool, dict) and tool.get("type") and tool["type"] != "function"]


def description_tools(tools: list[dict]) -> list[dict]:
    return [{'name': tool.get("function", {}).get("name"),
             'description': tool.get("function", {}).get("description")}
            for tool in tools if isinstance(tool, dict)]


def filter_directory(base_path: str, config: dict) -> list:
    """
    æ ¹æ®é…ç½®è¿‡æ»¤ç›®å½•ï¼Œè¿”å›åŒ¹é…çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆç›¸å¯¹äº base_pathï¼‰

    config å­—æ®µè¯´æ˜ï¼š
      - include: list of glob patterns to include at root level
      - recursive_include: dict of {dir_name: [patterns]}ï¼Œè¡¨ç¤ºåœ¨æŒ‡å®šå­ç›®å½•ä¸‹é€’å½’åŒ¹é…æ¨¡å¼
      - prune: list of directory names toå®Œå…¨è·³è¿‡
      - exclude: list of glob patterns toæ’é™¤çš„æ–‡ä»¶æˆ–è·¯å¾„ï¼ˆç›¸å¯¹äº baseï¼‰
      - global_exclude: list of glob patterns to æ’é™¤çš„æ–‡ä»¶æˆ–ç›®å½•ï¼ˆç›¸å¯¹äº base, ä½œç”¨äºä»»æ„å±‚çº§ï¼‰
    """
    import fnmatch
    base = Path(base_path)
    matched = set()

    include = config.get("include", [])
    recursive_include = config.get("recursive_include", {})
    prune = set(config.get("prune", []))
    exclude = config.get("exclude", [])
    global_exclude = config.get("global_exclude", [])

    # Helper to check global exclude
    def is_glob_excluded(rel_path: Path) -> bool:
        for pattern in global_exclude:
            if fnmatch.fnmatch(str(rel_path), pattern):
                return True
        return False

    # 1. Root include: only files in base directory matching include patterns
    for pattern in include:
        for p in base.glob(pattern):
            rel = p.relative_to(base)
            if p.is_file() and not is_glob_excluded(rel):
                matched.add(str(rel))

    # 2. Recursive include: for each specified directory, walk under it
    for dir_name, patterns in recursive_include.items():
        dir_path = base / dir_name
        if not dir_path.exists() or not dir_path.is_dir():
            continue
        for root, dirs, files in os.walk(dir_path):
            # Prune dirs in-place
            dirs[:] = [d for d in dirs if d not in prune]
            for fname in files:
                rel = Path(root).relative_to(base) / fname
                # Check global exclude
                if is_glob_excluded(rel):
                    continue
                # Check exclude list
                skip = False
                for ex in exclude:
                    if fnmatch.fnmatch(str(rel), ex):
                        skip = True
                        break
                if skip:
                    continue
                # Check patterns
                for pat in patterns:
                    if fnmatch.fnmatch(fname, pat):
                        matched.add(str(rel))
                        break

    # Convert to sorted list
    return sorted(matched)


def get_file_info(directory):
    """
    è·å–æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶ä¿¡æ¯ï¼ŒåŒ…æ‹¬åç§°ã€ç±»å‹ã€å¤§å°ã€åˆ›å»ºæ—¶é—´ã€ä¿®æ”¹æ—¶é—´ï¼Œå¹¶è®¡ç®—å¤§å°ï¼ˆä»¥MBä¸ºå•ä½ï¼‰ã€‚
    get_file_info("E://Documents//Jupyter")
    """
    import pandas as pd
    file_info_list = []
    for root, dirs, files in os.walk(directory):
        # å…ˆè·å–æ–‡ä»¶å¤¹çš„ä¿¡æ¯
        for dir_name in dirs:
            dir_path = os.path.join(root, dir_name)
            # æ£€æŸ¥æ˜¯å¦ä¸ºç¬¦å·é“¾æ¥
            if os.path.islink(dir_path):
                link_target = os.readlink(dir_path)  # è·å–ç¬¦å·é“¾æ¥æŒ‡å‘çš„ç›®æ ‡
                if os.path.exists(link_target):  # æ£€æŸ¥ç›®æ ‡è·¯å¾„æ˜¯å¦å­˜åœ¨
                    # éå†ç¬¦å·é“¾æ¥ç›®æ ‡çš„å†…å®¹
                    for sub_root, sub_dirs, sub_files in os.walk(link_target):
                        for sub_file in sub_files:
                            sub_file_path = os.path.join(sub_root, sub_file)
                            sub_file_stat = os.stat(sub_file_path)
                            file_info_list.append({
                                "Name": sub_file,
                                "Type": os.path.splitext(sub_file)[1],  # æ–‡ä»¶æ‰©å±•å
                                "Size (Bytes)": sub_file_stat.st_size,  # æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
                                "Size (MB)": round(sub_file_stat.st_size / (1024 * 1024), 2),  # æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰
                                "Creation Time": pd.to_datetime(sub_file_stat.st_ctime, unit='s'),
                                "Modification Time": pd.to_datetime(sub_file_stat.st_mtime, unit='s'),
                                "Path": sub_file_path,
                                "Link Target": link_target  # è®°å½•ç¬¦å·é“¾æ¥ç›®æ ‡è·¯å¾„
                            })
            else:
                # å¦‚æœæ˜¯æ™®é€šæ–‡ä»¶å¤¹
                dir_stat = os.stat(dir_path)
                file_info_list.append({
                    "Name": dir_name,
                    "Type": "Directory",  # æ ‡è®°ä¸ºæ–‡ä»¶å¤¹
                    "Size (Bytes)": 0,  # æ–‡ä»¶å¤¹å¤§å°è®¾ä¸º0
                    "Size (MB)": 0,  # æ–‡ä»¶å¤¹å¤§å°è®¾ä¸º0
                    "Creation Time": pd.to_datetime(dir_stat.st_ctime, unit='s'),
                    "Modification Time": pd.to_datetime(dir_stat.st_mtime, unit='s'),
                    "Path": dir_path,
                    "Link Target": None  # æ²¡æœ‰ç¬¦å·é“¾æ¥ç›®æ ‡
                })

        # è·å–æ–‡ä»¶çš„ä¿¡æ¯
        for file in files:
            file_path = os.path.join(root, file)
            if os.path.islink(file_path):  # å¦‚æœæ˜¯ç¬¦å·é“¾æ¥
                link_target = os.readlink(file_path)  # è·å–ç¬¦å·é“¾æ¥ç›®æ ‡
                if os.path.exists(link_target):  # æ£€æŸ¥ç¬¦å·é“¾æ¥ç›®æ ‡æ˜¯å¦å­˜åœ¨
                    file_stat = os.stat(link_target)  # è·å–ç›®æ ‡æ–‡ä»¶çš„çŠ¶æ€ä¿¡æ¯
                    file_info_list.append({
                        "Name": file,
                        "Type": os.path.splitext(file)[1],
                        "Size (Bytes)": file_stat.st_size,
                        "Size (MB)": round(file_stat.st_size / (1024 * 1024), 2),
                        "Creation Time": pd.to_datetime(file_stat.st_ctime, unit='s'),
                        "Modification Time": pd.to_datetime(file_stat.st_mtime, unit='s'),
                        "Path": file_path,
                        "Link Target": link_target  # è®°å½•ç¬¦å·é“¾æ¥ç›®æ ‡è·¯å¾„
                    })
            else:  # æ™®é€šæ–‡ä»¶
                file_stat = os.stat(file_path)
                file_info_list.append({
                    "Name": file,
                    "Type": os.path.splitext(file)[1],
                    "Size (Bytes)": file_stat.st_size,
                    "Size (MB)": round(file_stat.st_size / (1024 * 1024), 2),
                    "Creation Time": pd.to_datetime(file_stat.st_ctime, unit='s'),
                    "Modification Time": pd.to_datetime(file_stat.st_mtime, unit='s'),
                    "Path": file_path,
                    "Link Target": None
                })

    return pd.DataFrame(file_info_list)


def batch_convert_encoding(input_dir, output_dir, from_encoding='gb2312', to_encoding='utf-8'):
    """
    æ‰¹é‡å°†æŒ‡å®šç›®å½•ä¸­çš„æ–‡ä»¶ä»ä¸€ç§ç¼–ç è½¬æ¢ä¸ºå¦ä¸€ç§ç¼–ç ã€‚

    :param input_dir: è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
    :param output_dir: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    :param from_encoding: åŸå§‹æ–‡ä»¶ç¼–ç ï¼ˆé»˜è®¤ gb2312ï¼‰
    :param to_encoding: è½¬æ¢åçš„æ–‡ä»¶ç¼–ç ï¼ˆé»˜è®¤ utf-8ï¼‰
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for root, dirs, files in os.walk(input_dir):
        for file in files:
            input_file_path = os.path.join(root, file)
            relative_path = os.path.relpath(input_file_path, input_dir)
            output_file_path = os.path.join(output_dir, relative_path)

            try:
                # åˆ›å»ºè¾“å‡ºç›®å½•
                os.makedirs(os.path.dirname(output_file_path), exist_ok=True)

                # è½¬æ¢æ–‡ä»¶ç¼–ç 
                with open(input_file_path, 'r', encoding=from_encoding) as f_in:
                    content = f_in.read()
                with open(output_file_path, 'w', encoding=to_encoding) as f_out:
                    f_out.write(content)

                print(f"è½¬æ¢æˆåŠŸ: {input_file_path} -> {output_file_path}")
            except Exception as e:
                print(f"è½¬æ¢å¤±è´¥: {input_file_path} é”™è¯¯ä¿¡æ¯: {e}")


def get_memory_info():
    """ä» /proc/self/status è·å–å†…å­˜ä½¿ç”¨ï¼ˆLinux Onlyï¼‰"""
    try:
        with open("/proc/self/status") as f:
            for line in f:
                if line.startswith("VmRSS:"):
                    mem_kb = int(line.split()[1])
                    break
            return mem_kb / 1024  # è½¬ä¸º MB
    except Exception:
        # psutil.virtual_memory().total / (1024 ** 3)
        return -1


def get_cpu_time():
    """ä» /proc/self/stat è·å– CPU æ—¶é—´ï¼ˆå•ä½ï¼šç§’ï¼‰"""
    try:
        with open("/proc/self/stat") as f:
            values = f.read().split()
            utime = int(values[13])
            stime = int(values[14])
            ticks_per_sec = os.sysconf(os.sysconf_names["SC_CLK_TCK"])
            return (utime + stime) / ticks_per_sec
    except Exception:
        return -1


def get_open_fds_count() -> int:
    try:
        base = f"/proc/{os.getpid()}/fd"
        return len(os.listdir(base))
    except Exception:
        return -1


def count_http_connections(port=8000):
    if platform.system() != "Linux":
        # print("Warning: count_http_connections is only supported on Linux.")
        return -1
    count = 0
    with open("/proc/net/tcp", "r") as f:
        lines = f.readlines()[1:]
        for line in lines:
            parts = line.strip().split()
            local_address = parts[1]
            local_port_hex = local_address.split(":")[1]
            if int(local_port_hex, 16) == port:
                count += 1
    return count


def is_port_open(host, port):
    try:
        with socket.create_connection((host, port), timeout=3):
            return True
    except (OSError, ConnectionRefusedError):
        return False


def get_worker_identity():
    """è·å–å½“å‰ Worker çš„å”¯ä¸€æ ‡è¯†,ä½¿ç”¨è¿›ç¨‹ ID + ä¸»æœºå + å¯åŠ¨æ—¶é—´"""
    worker_id = os.environ.get("GUNICORN_WORKER_ID", None)  # ä½¿ç”¨ Gunicorn ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœä½¿ç”¨ Gunicornï¼‰
    if worker_id is not None:
        return worker_id
    pid = os.getpid()
    hostname = socket.gethostname()
    # start_time = os.times().elapsed  # è¿›ç¨‹å¯åŠ¨åçš„æ—¶é—´
    worker_info = f"{pid}-{hostname}"
    # unique_id = hashlib.sha256(worker_info.encode()).hexdigest()[:16]  # ä½¿ç”¨å”¯ä¸€å“ˆå¸Œæ ‡è¯†
    return f"worker-{worker_info}"


def memory_monitor(threshold_percent: float = 60, desc: bool = False):
    """ç›‘æ§è¿›ç¨‹å†…å­˜ä½¿ç”¨ç‡ï¼Œè¶…å‡ºé˜ˆå€¼åˆ™è§¦å‘åƒåœ¾å›æ”¶"""
    try:
        # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
        process = psutil.Process()
        memory_percent = process.memory_percent()
        memory_mb = process.memory_info().rss / 1024 / 1024
        if desc:
            print(f"[MemoryMonitor] CPU ä½¿ç”¨ç‡: {psutil.cpu_percent(interval=1)}%")
            print(f"[MemoryMonitor] ç³»ç»Ÿå†…å­˜: {psutil.virtual_memory()}")
            if tracemalloc.is_tracing():
                snapshot = tracemalloc.take_snapshot()  # è·å–å†…å­˜åˆ†é…ç»Ÿè®¡
                top_stats = snapshot.statistics('lineno')[:10]
                for stat in top_stats[:10]:
                    print(stat)

        # å½“å†…å­˜ä½¿ç”¨è¶…è¿‡é˜ˆå€¼æ—¶è§¦å‘åƒåœ¾å›æ”¶
        if memory_percent > threshold_percent:  # 70% å†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼
            print(f"[MemoryMonitor] å½“å‰å†…å­˜ä½¿ç”¨ç‡: {memory_percent:.2f}%, RSS: {memory_mb:.2f}MB - è§¦å‘åƒåœ¾å›æ”¶")
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            after = process.memory_info().rss / 1024 / 1024
            print(f"[MemoryMonitor] åƒåœ¾å›æ”¶åå†…å­˜: {after:.2f}MBï¼Œé‡Šæ”¾ï¼š{memory_mb - after:.2f}MB")

    except Exception as e:
        print(f"[MemoryMonitor] error: {e}")


async def get_log_lines(log_path: str | Path = "app.log", lines: int = 100):
    log_file = Path(log_path)
    if not log_file.exists():
        return {"error": f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}"}

    try:
        lines_list = deque(maxlen=lines)
        async with aiofiles.open(log_file, "r", encoding="utf-8", errors="ignore") as f:
            # f.readlines(),await f.read()
            async for line in f:
                lines_list.append(line.strip())

        return {"logs": list(lines_list), "count": len(lines_list)}

    except Exception as e:
        return {"error": f"è¯»å–æ—¥å¿—å¤±è´¥: {str(e)}"}


def get_job_list(scheduler):  # AsyncIOScheduler
    job_list = [
        {'id': job.id,
         'name': job.name,
         'func': str(job.func),
         # 'args': job.args,
         # 'kwargs': job.kwargs,
         'next_run_time': job.next_run_time.isoformat() if job.next_run_time else None,
         'trigger': str(job.trigger)
         } for job in scheduler.get_jobs()
    ]
    return job_list


# ä¿å­˜æ‰€æœ‰æ¨¡å‹åˆ°æ–‡ä»¶
def save_models(models: dict, model_dir='data/models/'):
    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„
    :param models: ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰éœ€è¦ä¿å­˜çš„æ¨¡å‹
    :param model_dir: ä¿å­˜æ¨¡å‹çš„è·¯å¾„
    """
    os.makedirs(model_dir, exist_ok=True)
    for model_name, model in models.items():
        if model is not None:
            joblib.dump(model, f'{model_dir}/{model_name}.pkl')
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³ {model_dir}")


# åŠ è½½æ¨¡å‹
def load_models(model_names: list, model_dir='data/models/'):
    """
    åŠ è½½ä¿å­˜çš„æ¨¡å‹
    :param model_names: æ¨¡å‹åç§°åˆ—è¡¨
    :param model_dir: ä¿å­˜æ¨¡å‹çš„è·¯å¾„
    :return: åŠ è½½çš„æ¨¡å‹å­—å…¸
    """
    models = {}
    for model_name in model_names:
        model_path = f'{model_dir}/{model_name}.pkl'
        if os.path.exists(model_path):
            models[model_name] = joblib.load(model_path)
    print(f"æ¨¡å‹å·²ä» {model_dir} åŠ è½½")
    return models


def save_dictjson(dictionary, file_path, encoding='utf-8', ascii=False):
    with open(file_path, 'w', encoding=encoding) as file:
        json.dump(dictionary, file, ensure_ascii=ascii)


def load_dictjson(file_path, encoding='utf-8'):
    with open(file_path, 'r', encoding=encoding) as file:
        dictionary = json.load(file)
    return dictionary


def load_datasets(path):
    samples = []
    with open(path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            samples.append(json.loads(line.strip()))
    return samples


async def save_markdown(content: str, filename: str, folder="data/output"):
    """å¼‚æ­¥ä¿å­˜å•ä¸ªMarkdownæ–‡ä»¶"""
    if not os.path.exists(folder):
        os.makedirs(folder)

    if not filename.endswith('.md'):
        filename += '.md'

    filepath = os.path.join(folder, filename)
    async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
        await f.write(content)

    return filepath


async def read_markdown(filename: str, folder="data/output") -> str | None:
    """å¼‚æ­¥è¯»å–Markdownæ–‡ä»¶å¹¶è¿”å›å†…å®¹"""
    if not filename.endswith('.md'):
        filename += '.md'

    filepath = os.path.join(folder, filename)
    if not os.path.exists(filepath):
        return None

    async with aiofiles.open(filepath, 'r', encoding='utf-8') as f:
        content = await f.read()

    return content


if __name__ == "__main__":
    def main():
        with open("utils.py", 'r', encoding='utf-8') as file:
            code = file.read()
        functions = extract_function_metadatas(code)
        for func in functions:
            print(func)

        # funcs = get_module_functions('utils.utils')
        # print([i[0] for i in funcs])

        filtered_files = filter_directory(r"/", config={
            "include": ["*.py", "deploy.bat", "requirements.txt", "CI"],
            "recursive_include": {
                "agents": ["*"],
                "script": ["*"],
                "docker": ["*"],
                "data": ["*.yaml", "*.pkl"]
            },
            "prune": [".venv", ".git", "docker_build", "script/data"],
            "exclude": ["script/nlp.py", "script/data/*"],
            "global_exclude": ["__pycache__/*", "*.py[cod]", "*.log"]
        })
        print(json.dumps(filtered_files, indent=2, ensure_ascii=False))


    import random


    async def test(params: list, stream=True):
        @run_togather(max_concurrent=10, batch_size=-1)
        async def single(data: tuple[int, dict]):
            i, param = data
            await asyncio.sleep(10)
            print(i, param)
            result = {**param, 'id': i}
            return result

        @run_generator(max_concurrent=10, return_exceptions=True)
        async def single_item(param):
            await asyncio.sleep(random.randint(3, 7))

            return param

        if stream:
            raw_results = []
            async for idx, r in single_item(inputs=params):
                print(idx, r)
                r = {'id': idx, **r}
                # yield res
                raw_results.append(r)
            results = sorted(raw_results, key=lambda x: x['id'], reverse=True)
        else:
            results = await single(inputs=[(i, p) for i, p in enumerate(params)])

        return results


    async def single_item(param):
        await asyncio.sleep(random.randint(3, 7))

        return param


    res = asyncio.run(runner_togather_sample([{"agents": ["*"],
                                               "script": ["*"],
                                               "docker": ["*"],
                                               "data": ["*.yaml", "*.pkl"]},
                                              {"global_exclude": ["__pycache__/*", "*.py[cod]", "*.log"]}], single_item,
                                             stream=True))
    print(res)

    # main()

    p = partial(single_item)
    print(asyncio.iscoroutinefunction(p.func), asyncio.iscoroutinefunction(p))
    p2 = partial(main)
    print(asyncio.iscoroutinefunction(p2.func), asyncio.iscoroutinefunction(p2), asyncio.iscoroutine(p2))
