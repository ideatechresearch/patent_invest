import io, os, sys, socket
import re, json
import pickle, joblib
import subprocess
import platform
import inspect, importlib, ast
from functools import partial, wraps  # cache, lru_cache, partial, wraps
import gc  # æ·»åŠ åƒåœ¾å›æ”¶æ¨¡å—
import psutil  # æ·»åŠ ç³»ç»Ÿç›‘æ§æ¨¡å—
import tracemalloc
from pathlib import Path
from collections import deque
import base64, hashlib
from typing import Union, Iterable, get_origin, get_args
import aiofiles, asyncio


def execute_code_results(text):
    """
    æ‰§è¡Œç»™å®šæ–‡æœ¬ä¸­çš„æ‰€æœ‰Pythonä»£ç å—ï¼Œå¹¶è¿”å›æ¯ä¸ªä»£ç å—çš„è¾“å‡ºã€å‘½åç©ºé—´å’Œé”™è¯¯ä¿¡æ¯ã€‚éœ€è¦åŒ…å«Pythonä»£ç å—çš„æ–‡æœ¬ã€‚
    ä»…ç”¨äºå·²çŸ¥å®‰å…¨çš„æœ¬åœ°ä»£ç å—æ‰§è¡Œã€‚ä¸è¦ç”¨äºä¸‹è½½ã€å®‰è£…æˆ–ç³»ç»Ÿçº§æ“ä½œã€‚
    ä»…é™æœ¬åœ°å·²ç»æœ‰çš„æˆ–æ˜ç¡®æŒ‡å‡ºçš„ï¼Œä»…é™æœ¬åœ°ç¯å¢ƒã€åªè¯»ã€æ— å¤–éƒ¨ä¾èµ–çš„ä»£ç å—ã€‚ä¸æ”¯æŒ pip å®‰è£…ã€ç³»ç»Ÿå‘½ä»¤ï¼ˆå¦‚ cdã€gitã€curl ç­‰ï¼‰ã€‚ä»…å½“å·²çŸ¥ä»£ç å¯ä¿¡ä¸”æ‰§è¡Œç¯å¢ƒæ”¯æŒæ—¶è°ƒç”¨ã€‚
    :param text:åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ª Python ä»£ç å—çš„çº¯æ–‡æœ¬å†…å®¹ã€‚æ”¯æŒ Markdown ä»£ç å—é£æ ¼çš„æ ¼å¼ã€‚
    :return:è¿è¡Œç»“æœ
    """
    from contextlib import redirect_stdout
    from .convert_ops import extract_python_code
    code_blocks = extract_python_code(text)
    results = []
    global_namespace = globals()  # å¼•ç”¨å…¨å±€å‘½åç©ºé—´ {"__builtins__": dict(__builtins__)}
    for code in code_blocks:
        local_namespace = {}  # ç”¨äºå­˜å‚¨ä»£ç çš„å±€éƒ¨å˜é‡
        captured_output = io.StringIO()  # ç”¨äºæ•è· `print` è¾“å‡º
        try:
            with redirect_stdout(captured_output):  # é‡å®šå‘ `print` è¾“å‡º
                exec(code, global_namespace, local_namespace)
                # exec(code, globals=None, locals=None)ç”¨äºåŠ¨æ€æ‰§è¡Œè¾ƒå¤æ‚çš„ä»£ç å—,ä¸è¿”å›ç»“æœ,éœ€è¦é€šè¿‡å…¨å±€æˆ–å±€éƒ¨å˜é‡è·å–ç»“æœ
            output = captured_output.getvalue()  # è·å– `print` çš„å†…å®¹
            results.append({
                "output": output.strip(),
                "namespace": local_namespace,
                "error": None
            })
        except Exception as e:
            results.append({
                "output": captured_output.getvalue().strip(),
                "namespace": local_namespace,
                "error": f"Error executing code block: {e},Code:{code}"
            })
        finally:
            captured_output.close()

    return results


def safe_eval_call(func_name: str, func_args: dict):
    allowed_names = {
        **globals(), **locals(),
        "__builtins__": {"str": str, "int": int, "float": float, "list": list, "dict": dict}
    }
    if func_name not in allowed_names:
        raise ValueError(f"ä¸å…è®¸åŠ¨æ€è°ƒç”¨å‡½æ•°ï¼š{func_name}")
    # compile(code, '<string>', 'eval')
    # eval(expression, globals=None, locals=None) æ‰§è¡Œå•ä¸ªè¡¨è¾¾å¼,åŠ¨æ€è®¡ç®—ç®€å•çš„è¡¨è¾¾å¼æˆ–ä»å­—ç¬¦ä¸²ä¸­è§£æå€¼
    return eval(f'{func_name}(**{func_args})', allowed_names)


def pip_installed_list():
    # from importlib.metadata import distributions
    # return sorted(
    #     [{"name": d.metadata["Name"], "version": d.version} for d in distributions()],
    #     key=lambda x: x["name"].lower()
    # )
    result = subprocess.run(
        [sys.executable, "-m", "pip", "list", "--format=json"],
        capture_output=True, text=True, check=True
    )
    return json.loads(result.stdout)


def pip_install(*packages, index_url="https://pypi.tuna.tsinghua.edu.cn/simple"):
    """
    å®‰è£… pip åŒ…ï¼ˆæ”¯æŒç©ºæ ¼/é€—å·åˆ†éš”çš„å¤šä¸ªåŒ…ï¼‰ï¼Œå¹¶å°è¯• importã€‚

    ç¤ºä¾‹ï¼š
    pip_install("pandas")
    pip_install("pandas scikit-learn")
    pip_install("numpy", "matplotlib==3.7.1")
    """
    pkg_list = []
    for p in packages:
        # æ‹†åˆ†æ”¯æŒç©ºæ ¼ã€é€—å·ç­‰æ ¼å¼
        if isinstance(p, str):
            parts = p.replace(",", " ").split()
            pkg_list.extend(parts)
        else:
            pkg_list.append(str(p))

    try:
        print(f"æ­£åœ¨å®‰è£…: {pkg_list}")
        subprocess.run([sys.executable, "-m", "pip", "install", "-i", index_url, *pkg_list],
                       # shell=(platform.system() == "Windows"),
                       check=True)
    except subprocess.CalledProcessError as e:
        print("âŒ å®‰è£…å¤±è´¥:", str(e))
        return {"error": str(e), "installed": pkg_list}

    return pkg_list


def import_packages(pkg_list):
    success_imports = []
    for pkg in pkg_list:
        try:
            mod_name = pkg.split("==")[0].split(">")[0].split("<")[0]
            importlib.import_module(mod_name)
            success_imports.append(mod_name)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è‡ªåŠ¨å¯¼å…¥ {pkg}: {e}")
    return success_imports


def git_repo_clone(repo_url: str, revision: str = None, add_to_sys_path: bool = True) -> Path:
    # è‡ªåŠ¨åŒ–ç¯å¢ƒéƒ¨ç½²ã€åŠ¨æ€åŠ è½½ Git ä»“åº“ä»£ç 
    repo_path = Path(repo_url.split("/")[-1].replace(".git", ""))
    if not repo_path.exists():  # æœ¬åœ°æ²¡æœ‰è¯¥ä»“åº“
        try:
            subprocess.run(["git", "clone", repo_url], check=True,
                           stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        except Exception as exc:
            print(f"Failed to clone the repository: {exc.stderr}")
            raise

        if revision:  # åˆ‡æ¢åˆ†æ”¯æˆ–æäº¤
            subprocess.Popen(["git", "checkout", revision], cwd=str(repo_path))
    if add_to_sys_path and str(repo_path.resolve()) not in sys.path:
        sys.path.insert(0, str(repo_path.resolve()))
        # åŠ å…¥ Python çš„ sys.pathï¼Œä»¥ä¾¿ importè¯¥ä»“åº“çš„æ¨¡å—

    return repo_path


def start_app_instance(port=7000, workers=4):
    return subprocess.Popen([
        "uvicorn", "main:app",  # å¯åŠ¨ main.py ä¸­çš„ app å®ä¾‹
        "--host", "127.0.0.1",
        "--port", str(port),
        "--workers", str(workers)
    ])


def configure_event_loop():
    if platform.system() != "Windows":
        try:
            import uvloop
            uvloop.install()
            print("ğŸš€ uvloop activated")
        except ImportError:
            print("âš ï¸ uvloop not available, using default event loop")
    else:
        # å¯ç”¨IOCP
        if sys.platform.startswith('win'):
            if sys.version_info >= (3, 8):
                # Python 3.8+ ä½¿ç”¨æ›´é«˜æ•ˆçš„ Proactor
                asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
            else:
                asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())  # ä½¿ç”¨ Selector
        print("ğŸ–¥ï¸ Windows detected, using optimized event loop policy")


def kill_process_tree(pid: int):
    import signal, contextlib
    """
    Kills all descendant processes of the given pid by sending SIGKILL.

    Args:
        pid (int): Process ID of the parent process
    """
    try:
        parent = psutil.Process(pid)
    except psutil.NoSuchProcess:
        return

    # Get all children recursively
    children = parent.children(recursive=True)

    # Send SIGKILL to all children first
    for child in children:
        with contextlib.suppress(ProcessLookupError):
            os.kill(child.pid, signal.SIGKILL)

    # Finally kill the parent
    with contextlib.suppress(ProcessLookupError):
        os.kill(pid, signal.SIGKILL)


def named_partial(name, func, *args, **kwargs):
    p = partial(func, *args, **kwargs)
    p.__name__ = name
    return p


def is_empty_lambda(func):
    try:
        if callable(func) and getattr(func, "__name__", "") == "<lambda>" and len(
                inspect.signature(func).parameters) == 0:
            return func() == []
    except:
        return False
    return False


def async_to_sync(func, *args, **kwargs):
    # å¼‚æ­¥ä»£ç è½¬æ¢ä¸ºåŒæ­¥ä»£ç 
    return asyncio.run(func(*args, **kwargs))


async def wrap_sync(func, *args, **kwargs):
    # åŒæ­¥ä»£ç è½¬æ¢ä¸ºå¼‚æ­¥æ‰§è¡Œ,åœ¨åå°ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ,ä»¥é¿å…é˜»å¡ä¸»äº‹ä»¶å¾ªç¯,ä½¿ç”¨ await æ¥æ‰§è¡Œ
    return await asyncio.to_thread(func, *args, **kwargs)


async def run_with_async(func, *args, **kwargs):
    """
    é€šç”¨æ–¹æ³•ï¼šæ ¹æ®å‡½æ•°æ˜¯å¦ä¸ºåç¨‹è‡ªåŠ¨é€‰æ‹© await æˆ–ç›´æ¥è°ƒç”¨
    æ”¯æŒåœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­ç»Ÿä¸€å¤„ç†åŒæ­¥/å¼‚æ­¥å‡½æ•°è°ƒç”¨

    :param func: å¾…æ‰§è¡Œå‡½æ•°ï¼ˆåŒæ­¥æˆ–å¼‚æ­¥ï¼‰
    :param args: ä½ç½®å‚æ•°
    :param kwargs: å…³é”®å­—å‚æ•°
    :return: å‡½æ•°è¿”å›ç»“æœ
    """
    if inspect.iscoroutinefunction(func):
        return await func(*args, **kwargs)
    else:
        return await asyncio.to_thread(func, *args, **kwargs)  # ç”¨ asyncio.to_thread ä»¥é¿å…é˜»å¡


async def run_with_semaphore(func, *args, semaphore=None, **kwargs):
    if semaphore:
        async with semaphore:
            return await func(*args, **kwargs)
    else:
        return await func(*args, **kwargs)


_StringLikeT = Union[bytes, str, memoryview]


def list_or_args_keys(keys: Union[_StringLikeT, Iterable[_StringLikeT]],
                      args: tuple[_StringLikeT, ...] = None) -> list[_StringLikeT]:
    # å°† keys å’Œ args åˆå¹¶æˆä¸€ä¸ªæ–°çš„åˆ—è¡¨
    # returns a single new list combining keys and args
    try:
        iter(keys)
        # a string or bytes instance can be iterated, but indicates
        # keys wasn't passed as a list
        if isinstance(keys, (bytes, str)):
            keys = [keys]
        else:
            keys = list(keys)  # itertools.chain.from_iterable(keys)
    except TypeError:
        keys = [keys]
    if args:
        keys.extend(args)
    return keys


def get_function_parameters(func):
    signature = inspect.signature(func)
    parameters = signature.parameters
    return [param for param in parameters]


def remove_function_decorators(func) -> str:
    """
    è·å–å‡½æ•°æºç ï¼Œå»æ‰æ‰€æœ‰è£…é¥°å™¨
    """
    source = inspect.getsource(func)  # è·å–åŸå§‹ä»£ç  function_code
    tree = ast.parse(source)  # è§£æ AST

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):  # æ‰¾åˆ°å‡½æ•°å®šä¹‰
            node.decorator_list = []  # æ¸…ç©ºè£…é¥°å™¨åˆ—è¡¨

    return ast.unparse(tree)  # é‡æ–°è½¬æ¢æˆä»£ç 
    # import textwrap
    # textwrap.dedent(source)  # å¤„ç†ç¼©è¿›


def strip_kwargs_wrapper(func):
    """è¿”å›ä¸€ä¸ªå»æ‰ **kwargs çš„åŒ…è£…å‡½æ•°"""
    sig = inspect.signature(func)
    allowed_params = []
    # any(p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values())
    for p in sig.parameters.values():
        if p.kind == inspect.Parameter.VAR_KEYWORD:
            continue
        if p.annotation == inspect._empty:
            allowed_params.append(p)
        elif isinstance(p.annotation, type) and p.annotation.__module__.startswith("httpx"):
            continue  # è·³è¿‡ httpx ç­‰å¤æ‚ç±»å‹
        else:
            allowed_params.append(p)
    # åˆ›å»ºæ–°çš„å‡½æ•°ç­¾å
    new_sig = inspect.Signature(allowed_params)

    @wraps(func)
    def wrapper(*args, **kwargs):
        bound_args = new_sig.bind(*args, **kwargs)
        bound_args.apply_defaults()
        return func(*bound_args.args, **bound_args.kwargs)

    wrapper.__signature__ = new_sig  # æ›¿æ¢ç­¾åï¼ˆä»…ç”¨äºå·¥å…·æ³¨å†Œï¼Œä¸å½±å“å®é™…è°ƒç”¨ï¼‰
    return wrapper


def python_type_to_json_schema_type(python_type):
    origin = get_origin(python_type) or python_type

    # å¤„ç† Union/Optional
    if origin is Union:
        args = [a for a in get_args(python_type) if a is not type(None)]
        if len(args) == 1:
            return python_type_to_json_schema_type(args[0])
        else:
            return "string"  # fallback for complex Unions

    mapping = {
        str: "string",
        int: "integer",
        float: "number",
        bool: "boolean",
        list: "array",
        dict: "object",
    }
    return mapping.get(origin, "string")


def extract_function_metadatas(code):
    # ç”¨äºè§£æPythonå‡½æ•°å¹¶æå–å…ƒæ•°æ®,ä»æºä»£ç çº§åˆ«æå–è¯¦ç»†çš„ä»£ç ä¿¡æ¯
    # ä½¿ç”¨ASTæ¥è§£æPythonä»£ç ,å°† Python æºä»£ç ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰è½¬æ¢ä¸ºæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰
    tree = ast.parse(code)
    functions = []

    # éå†ASTèŠ‚ç‚¹ï¼Œæ‰¾åˆ°å‡½æ•°å®šä¹‰
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            function_name = node.name
            arguments = [arg.arg for arg in node.args.args]
            docstring = ast.get_docstring(node) or ""
            function_code = ast.unparse(node) if hasattr(ast, 'unparse') else ast.dump(node)
            # if generate_docstring and not docstring:
            #     docstring = generate_docstring(function_name, function_code)

            metadata = {
                'name': function_name,
                'args': arguments,
                'docstring': docstring,
                'code': function_code
            }
            functions.append(metadata)

    return functions


def extract_function_metadata(func) -> dict:
    # è·å–å‡½æ•°çš„åç§°ã€å‚æ•°ä»¥åŠdocstring
    func_name = func.__name__
    # print(f"[tools] æœªæ‰¾åˆ°ç¼“å­˜: {func_name},ç”Ÿæˆ metadata")
    # è·å–å‡½æ•°å‚æ•°åˆ—è¡¨
    signature = inspect.signature(func)
    docstring = func.__doc__

    parameters = {}
    required_params = []

    for param_name, param in signature.parameters.items():
        param_type = param.annotation if param.annotation != inspect.Parameter.empty else str
        json_type = python_type_to_json_schema_type(param_type)
        # å¦‚æœæœ‰ç±»å‹æç¤ºï¼Œæ¨æ–­å‚æ•°ç±»å‹str(param.annotation).replace("<class '", "").replace("'>", "")
        param_info = {
            "type": json_type,
            "description": f"Description of the {param_name} parameter."
        }

        if param.default == inspect.Parameter.empty:
            required_params.append(param_name)
        else:
            param_info["default"] = param.default
        parameters[param_name] = param_info

    # åˆå§‹metadataç»“æ„
    metadata = {
        "type": "function",
        "function": {
            "name": func_name,
            "description": docstring or "No description available.",
            "parameters": {
                "type": "object",
                "properties": parameters,
                "required": required_params
            },
            # "x-url":å·¥å…·çš„å®é™… API åœ°å€
        },
        # "func": func,
    }
    return metadata


def get_module_functions(module_name: str = None):
    module = importlib.import_module(module_name) if module_name else inspect.getmodule(inspect.currentframe())
    module_name = module.__name__
    funcs = inspect.getmembers(module, inspect.isfunction)
    local_funcs = [(name, func) for name, func in funcs if func.__module__ == module_name]
    # for name, _func in funcs:
    #     print(f"Function: {name}")
    return local_funcs


def functions_registry(functions_list: list, safe_path=True, module_name: str | list = None) -> dict:
    """
    æ ¹æ®å‡½æ•°åç§°åˆ—è¡¨,åˆ›å»ºå…¨å±€å‡½æ•°æ³¨å†Œè¡¨,æˆ–è€…æŒ‡å®šæ¨¡å—ä¸­åŠ¨æ€åŠ è½½
    1. ä»å½“å‰å…¨å±€ä½œç”¨åŸŸæŸ¥æ‰¾å‡½æ•°åï¼›
    2. æŒ‡å®š module_nameï¼Œæ‰¹é‡ä»è¯¥æ¨¡å—åŠ è½½ï¼›
    3. ä½¿ç”¨ 'module.path:func' æ ¼å¼ï¼Œå•ä¸ªåŠ¨æ€åŠ è½½ã€‚

    :param functions_list: éœ€è¦æ³¨å†Œçš„å‡½æ•°ååˆ—è¡¨
    :param safe_path: å–æ¶ˆä¸æ£€æŸ¥æ˜¯å¦å¯è°ƒç”¨ã€‚
    :param module_name: æ¨¡å—åç§°æˆ–è€…æ¨¡å—åç§°åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰ï¼Œé€‚åˆä»ä¸€ä¸ªæ¨¡å—æˆ–å¤šä¸ªæ¨¡å—ä¸­åŒ¹é…åŠ è½½å¤šä¸ªå‡½æ•°ã€‚
    :return: Dict[str, Callable[..., Any]]
    """
    if not safe_path:
        module = importlib.import_module(module_name) if module_name else None
        return {name: getattr(module, name) if module else globals().get(name) for name in functions_list}

    if isinstance(module_name, list):
        return function_registry_dynamic(functions_list, module_name)
    return functions_registry_safe(functions_list, module_name)
    # get_function_parameters


def functions_registry_safe(functions_list: list, module_name: str = None) -> dict:
    """
    æ ¹æ®å‡½æ•°åç§°åˆ—è¡¨,åˆ›å»ºå…¨å±€å‡½æ•°æ³¨å†Œè¡¨,æˆ–è€…æŒ‡å®šæ¨¡å—ä¸­åŠ¨æ€åŠ è½½,æ£€æŸ¥æ˜¯å¦å¯è°ƒç”¨
    1. ä»å½“å‰å…¨å±€ä½œç”¨åŸŸæŸ¥æ‰¾å‡½æ•°åï¼›
    2. æŒ‡å®š module_nameï¼Œæ‰¹é‡ä»è¯¥æ¨¡å—åŠ è½½ï¼›
    3. ä½¿ç”¨ 'module.path:func' æ ¼å¼ï¼Œå•ä¸ªåŠ¨æ€åŠ è½½ã€‚ä¼˜å…ˆåŒ¹é…

    :param functions_list: éœ€è¦æ³¨å†Œçš„å‡½æ•°ååˆ—è¡¨
    :param module_name: æ¨¡å—åç§°ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰ï¼Œé€‚åˆä»ä¸€ä¸ªæ¨¡å—ä¸­åŠ è½½å¤šä¸ªå‡½æ•°ã€‚
    :return: Dict[str, Callable[..., Any]]
    """
    # å¦‚æœæ²¡æœ‰æ¨¡å—åï¼Œå›é€€åˆ°å…¨å±€ä½œç”¨åŸŸ
    module = importlib.import_module(module_name) if module_name else globals()
    registry = {}
    for name in functions_list:
        try:
            if ":" in name:
                module_path, func_name = name.rsplit(":", 1)
                _module = importlib.import_module(module_path)
                func_obj = getattr(_module, func_name, None)
                name = func_name
            else:
                func_obj = getattr(module, name, None)

            if not callable(func_obj):
                raise ValueError(f"å‡½æ•° {name} ä¸å­˜åœ¨æˆ–ä¸æ˜¯å¯è°ƒç”¨å¯¹è±¡,æœªåœ¨å½“å‰ä½œç”¨åŸŸä¸­æ‰¾åˆ°,å¯èƒ½æœªå¯¼å…¥æˆ–æ¨¡å—æœªæŒ‡å®šã€‚")

            registry[name] = func_obj

        except ModuleNotFoundError:
            registry[name] = None
            print(f"Module '{module_name}','{name}' not found.")
            # importlib.reload(module_path)

        except Exception as e:
            registry[name] = None
            print(f"[âš ï¸] åŠ è½½å‡½æ•°å¤±è´¥: {name} â†’ {type(e).__name__}: {e}")

    return registry


def function_registry_dynamic(functions_list: list, module_names: list):
    """
    åŠ¨æ€åŠ è½½æ¨¡å—å¹¶æ³¨å†Œå‡½æ•°
    :param functions_list: éœ€è¦æ³¨å†Œçš„å‡½æ•°ååˆ—è¡¨
    :param module_names: æ¨¡å—åç§°åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰
    :return: å‡½æ•°æ³¨å†Œè¡¨
    """
    registry = {}
    for module_name in module_names:
        try:
            module = importlib.import_module(module_name)  # åŠ¨æ€åŠ è½½æ¨¡å—
            for name in functions_list:
                if name in registry:  # é¿å…é‡å¤è¦†ç›–ï¼Œåªæ³¨å†Œç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„
                    continue
                func = getattr(module, name, None)
                if func is not None and callable(func):
                    registry[name] = func
        except ModuleNotFoundError:
            print(f"Module '{module_name}' not found.")
    return registry


def parse_tool_text(text):
    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ¥åŒ¹é… <tags>, <tool_call>, <content> åŠå…¶å†…å®¹
    tags_pattern = r'<tags>(.*?)</tags>'
    tool_call_pattern = r'<tool_call>(.*?)</tool_call>'
    content_pattern = r'<content>(.*?)</content>'
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾åŒ¹é…çš„å†…å®¹
    tags_match = re.search(tags_pattern, text, re.DOTALL)
    tool_call_match = re.search(tool_call_pattern, text, re.DOTALL)
    content_match = re.search(content_pattern, text, re.DOTALL)
    # æå–åŒ¹é…çš„å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    tags = tags_match.group(1).strip() if tags_match else ""
    tool_call = tool_call_match.group(1).strip() if tool_call_match else ""
    content = content_match.group(1).strip() if content_match else ""
    # å°†æå–çš„å†…å®¹å­˜å‚¨åœ¨å­—å…¸ä¸­
    result = {
        "tags": tags,
        "tool_call": tool_call,
        "content": content
    }
    return result


def openai_tools_to_mcp(tools: list[dict]):
    def decorator(mcp_server):
        for tool in tools:
            if tool["type"] != "function":
                continue

            func_info = tool["function"]

            def make_handler(func_name, func_desc):
                async def handler(params):
                    # è¿™é‡Œå¯ä»¥æ·»åŠ é€šç”¨å¤„ç†é€»è¾‘
                    print(f"Handling {func_name} with params: {params}")
                    # å®é™…å¤„ç†å‡½æ•°åº”è¯¥åœ¨åˆ«å¤„å®šä¹‰
                    return await globals()[f"handle_{func_name}"](params)

                handler.__name__ = func_name
                handler.__doc__ = func_desc
                return handler

            mcp_server.add_handler(
                action=func_info["name"],
                handler=make_handler(func_info["name"], func_info["description"])
            )
        return mcp_server

    return decorator


def deduplicate_functions_by_name(tools: list[dict]) -> list[dict]:
    seen = set()
    tools_metadata = []
    for tool in tools:
        name = tool.get("function", {}).get("name") or tool.get("name")
        if name and name not in seen:
            seen.add(name)
            tools_metadata.append(tool)
    return tools_metadata


def get_tools_params(tools: list[dict]) -> list[tuple[str, dict]]:
    return [(tool["type"], tool.get(tool["type"], {}))
            for tool in tools
            if isinstance(tool, dict) and tool.get("type") and tool["type"] != "function"]


def description_tools(tools: list[dict]) -> list[dict]:
    return [{'name': tool.get("function", {}).get("name"),
             'description': tool.get("function", {}).get("description")}
            for tool in tools if isinstance(tool, dict)]


def filter_directory(base_path: str, config: dict) -> list:
    """
    æ ¹æ®é…ç½®è¿‡æ»¤ç›®å½•ï¼Œè¿”å›åŒ¹é…çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆç›¸å¯¹äº base_pathï¼‰

    config å­—æ®µè¯´æ˜ï¼š
      - include: list of glob patterns to include at root level
      - recursive_include: dict of {dir_name: [patterns]}ï¼Œè¡¨ç¤ºåœ¨æŒ‡å®šå­ç›®å½•ä¸‹é€’å½’åŒ¹é…æ¨¡å¼
      - prune: list of directory names toå®Œå…¨è·³è¿‡
      - exclude: list of glob patterns toæ’é™¤çš„æ–‡ä»¶æˆ–è·¯å¾„ï¼ˆç›¸å¯¹äº baseï¼‰
      - global_exclude: list of glob patterns to æ’é™¤çš„æ–‡ä»¶æˆ–ç›®å½•ï¼ˆç›¸å¯¹äº base, ä½œç”¨äºä»»æ„å±‚çº§ï¼‰
    """
    import fnmatch
    base = Path(base_path)
    matched = set()

    include = config.get("include", [])
    recursive_include = config.get("recursive_include", {})
    prune = set(config.get("prune", []))
    exclude = config.get("exclude", [])
    global_exclude = config.get("global_exclude", [])

    # Helper to check global exclude
    def is_glob_excluded(rel_path: Path) -> bool:
        for pattern in global_exclude:
            if fnmatch.fnmatch(str(rel_path), pattern):
                return True
        return False

    # 1. Root include: only files in base directory matching include patterns
    for pattern in include:
        for p in base.glob(pattern):
            rel = p.relative_to(base)
            if p.is_file() and not is_glob_excluded(rel):
                matched.add(str(rel))

    # 2. Recursive include: for each specified directory, walk under it
    for dir_name, patterns in recursive_include.items():
        dir_path = base / dir_name
        if not dir_path.exists() or not dir_path.is_dir():
            continue
        for root, dirs, files in os.walk(dir_path):
            # Prune dirs in-place
            dirs[:] = [d for d in dirs if d not in prune]
            for fname in files:
                rel = Path(root).relative_to(base) / fname
                # Check global exclude
                if is_glob_excluded(rel):
                    continue
                # Check exclude list
                skip = False
                for ex in exclude:
                    if fnmatch.fnmatch(str(rel), ex):
                        skip = True
                        break
                if skip:
                    continue
                # Check patterns
                for pat in patterns:
                    if fnmatch.fnmatch(fname, pat):
                        matched.add(str(rel))
                        break

    # Convert to sorted list
    return sorted(matched)


def get_file_info(directory):
    """
    è·å–æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶ä¿¡æ¯ï¼ŒåŒ…æ‹¬åç§°ã€ç±»å‹ã€å¤§å°ã€åˆ›å»ºæ—¶é—´ã€ä¿®æ”¹æ—¶é—´ï¼Œå¹¶è®¡ç®—å¤§å°ï¼ˆä»¥MBä¸ºå•ä½ï¼‰ã€‚
    get_file_info("E://Documents//Jupyter")
    """
    import pandas as pd
    file_info_list = []
    for root, dirs, files in os.walk(directory):
        # å…ˆè·å–æ–‡ä»¶å¤¹çš„ä¿¡æ¯
        for dir_name in dirs:
            dir_path = os.path.join(root, dir_name)
            # æ£€æŸ¥æ˜¯å¦ä¸ºç¬¦å·é“¾æ¥
            if os.path.islink(dir_path):
                link_target = os.readlink(dir_path)  # è·å–ç¬¦å·é“¾æ¥æŒ‡å‘çš„ç›®æ ‡
                if os.path.exists(link_target):  # æ£€æŸ¥ç›®æ ‡è·¯å¾„æ˜¯å¦å­˜åœ¨
                    # éå†ç¬¦å·é“¾æ¥ç›®æ ‡çš„å†…å®¹
                    for sub_root, sub_dirs, sub_files in os.walk(link_target):
                        for sub_file in sub_files:
                            sub_file_path = os.path.join(sub_root, sub_file)
                            sub_file_stat = os.stat(sub_file_path)
                            file_info_list.append({
                                "Name": sub_file,
                                "Type": os.path.splitext(sub_file)[1],  # æ–‡ä»¶æ‰©å±•å
                                "Size (Bytes)": sub_file_stat.st_size,  # æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
                                "Size (MB)": round(sub_file_stat.st_size / (1024 * 1024), 2),  # æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰
                                "Creation Time": pd.to_datetime(sub_file_stat.st_ctime, unit='s'),
                                "Modification Time": pd.to_datetime(sub_file_stat.st_mtime, unit='s'),
                                "Path": sub_file_path,
                                "Link Target": link_target  # è®°å½•ç¬¦å·é“¾æ¥ç›®æ ‡è·¯å¾„
                            })
            else:
                # å¦‚æœæ˜¯æ™®é€šæ–‡ä»¶å¤¹
                dir_stat = os.stat(dir_path)
                file_info_list.append({
                    "Name": dir_name,
                    "Type": "Directory",  # æ ‡è®°ä¸ºæ–‡ä»¶å¤¹
                    "Size (Bytes)": 0,  # æ–‡ä»¶å¤¹å¤§å°è®¾ä¸º0
                    "Size (MB)": 0,  # æ–‡ä»¶å¤¹å¤§å°è®¾ä¸º0
                    "Creation Time": pd.to_datetime(dir_stat.st_ctime, unit='s'),
                    "Modification Time": pd.to_datetime(dir_stat.st_mtime, unit='s'),
                    "Path": dir_path,
                    "Link Target": None  # æ²¡æœ‰ç¬¦å·é“¾æ¥ç›®æ ‡
                })

        # è·å–æ–‡ä»¶çš„ä¿¡æ¯
        for file in files:
            file_path = os.path.join(root, file)
            if os.path.islink(file_path):  # å¦‚æœæ˜¯ç¬¦å·é“¾æ¥
                link_target = os.readlink(file_path)  # è·å–ç¬¦å·é“¾æ¥ç›®æ ‡
                if os.path.exists(link_target):  # æ£€æŸ¥ç¬¦å·é“¾æ¥ç›®æ ‡æ˜¯å¦å­˜åœ¨
                    file_stat = os.stat(link_target)  # è·å–ç›®æ ‡æ–‡ä»¶çš„çŠ¶æ€ä¿¡æ¯
                    file_info_list.append({
                        "Name": file,
                        "Type": os.path.splitext(file)[1],
                        "Size (Bytes)": file_stat.st_size,
                        "Size (MB)": round(file_stat.st_size / (1024 * 1024), 2),
                        "Creation Time": pd.to_datetime(file_stat.st_ctime, unit='s'),
                        "Modification Time": pd.to_datetime(file_stat.st_mtime, unit='s'),
                        "Path": file_path,
                        "Link Target": link_target  # è®°å½•ç¬¦å·é“¾æ¥ç›®æ ‡è·¯å¾„
                    })
            else:  # æ™®é€šæ–‡ä»¶
                file_stat = os.stat(file_path)
                file_info_list.append({
                    "Name": file,
                    "Type": os.path.splitext(file)[1],
                    "Size (Bytes)": file_stat.st_size,
                    "Size (MB)": round(file_stat.st_size / (1024 * 1024), 2),
                    "Creation Time": pd.to_datetime(file_stat.st_ctime, unit='s'),
                    "Modification Time": pd.to_datetime(file_stat.st_mtime, unit='s'),
                    "Path": file_path,
                    "Link Target": None
                })

    return pd.DataFrame(file_info_list)


def batch_convert_encoding(input_dir, output_dir, from_encoding='gb2312', to_encoding='utf-8'):
    """
    æ‰¹é‡å°†æŒ‡å®šç›®å½•ä¸­çš„æ–‡ä»¶ä»ä¸€ç§ç¼–ç è½¬æ¢ä¸ºå¦ä¸€ç§ç¼–ç ã€‚

    :param input_dir: è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
    :param output_dir: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    :param from_encoding: åŸå§‹æ–‡ä»¶ç¼–ç ï¼ˆé»˜è®¤ gb2312ï¼‰
    :param to_encoding: è½¬æ¢åçš„æ–‡ä»¶ç¼–ç ï¼ˆé»˜è®¤ utf-8ï¼‰
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for root, dirs, files in os.walk(input_dir):
        for file in files:
            input_file_path = os.path.join(root, file)
            relative_path = os.path.relpath(input_file_path, input_dir)
            output_file_path = os.path.join(output_dir, relative_path)

            try:
                # åˆ›å»ºè¾“å‡ºç›®å½•
                os.makedirs(os.path.dirname(output_file_path), exist_ok=True)

                # è½¬æ¢æ–‡ä»¶ç¼–ç 
                with open(input_file_path, 'r', encoding=from_encoding) as f_in:
                    content = f_in.read()
                with open(output_file_path, 'w', encoding=to_encoding) as f_out:
                    f_out.write(content)

                print(f"è½¬æ¢æˆåŠŸ: {input_file_path} -> {output_file_path}")
            except Exception as e:
                print(f"è½¬æ¢å¤±è´¥: {input_file_path} é”™è¯¯ä¿¡æ¯: {e}")


def get_memory_info():
    """ä» /proc/self/status è·å–å†…å­˜ä½¿ç”¨ï¼ˆLinux Onlyï¼‰"""
    try:
        with open("/proc/self/status") as f:
            for line in f:
                if line.startswith("VmRSS:"):
                    mem_kb = int(line.split()[1])
                    break
            return mem_kb / 1024  # è½¬ä¸º MB
    except Exception:
        # psutil.virtual_memory().total / (1024 ** 3)
        return -1


def get_cpu_time():
    """ä» /proc/self/stat è·å– CPU æ—¶é—´ï¼ˆå•ä½ï¼šç§’ï¼‰"""
    try:
        with open("/proc/self/stat") as f:
            values = f.read().split()
            utime = int(values[13])
            stime = int(values[14])
            ticks_per_sec = os.sysconf(os.sysconf_names["SC_CLK_TCK"])
            return (utime + stime) / ticks_per_sec
    except Exception:
        return -1


def get_open_fds_count() -> int:
    try:
        base = f"/proc/{os.getpid()}/fd"
        return len(os.listdir(base))
    except Exception:
        return -1


def count_http_connections(port=8000):
    if platform.system() != "Linux":
        # print("Warning: count_http_connections is only supported on Linux.")
        return -1
    count = 0
    with open("/proc/net/tcp", "r") as f:
        lines = f.readlines()[1:]
        for line in lines:
            parts = line.strip().split()
            local_address = parts[1]
            local_port_hex = local_address.split(":")[1]
            if int(local_port_hex, 16) == port:
                count += 1
    return count


def is_port_open(host, port):
    try:
        with socket.create_connection((host, port), timeout=3):
            return True
    except (OSError, ConnectionRefusedError):
        return False


def get_worker_identity():
    """è·å–å½“å‰ Worker çš„å”¯ä¸€æ ‡è¯†"""
    #  ä½¿ç”¨è¿›ç¨‹ ID + ä¸»æœºå + å¯åŠ¨æ—¶é—´
    pid = os.getpid()
    hostname = socket.gethostname()
    # start_time = os.times().elapsed  # è¿›ç¨‹å¯åŠ¨åçš„æ—¶é—´
    worker_info = f"{pid}-{hostname}"

    worker_id = os.environ.get("GUNICORN_WORKER_ID", None)  # ä½¿ç”¨ Gunicorn ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœä½¿ç”¨ Gunicornï¼‰
    if worker_id is not None:
        # is_main_worker = worker_id == "0"
        return worker_id, worker_info
    unique_id = hashlib.sha256(worker_info.encode()).hexdigest()[:16]  # ä½¿ç”¨å”¯ä¸€å“ˆå¸Œæ ‡è¯†
    return f"worker-{unique_id}", worker_info


def memory_monitor(threshold_percent: float = 60, desc: bool = False):
    """ç›‘æ§è¿›ç¨‹å†…å­˜ä½¿ç”¨ç‡ï¼Œè¶…å‡ºé˜ˆå€¼åˆ™è§¦å‘åƒåœ¾å›æ”¶"""
    try:
        # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
        process = psutil.Process()
        memory_percent = process.memory_percent()
        memory_mb = process.memory_info().rss / 1024 / 1024
        if desc:
            print(f"[MemoryMonitor] CPU ä½¿ç”¨ç‡: {psutil.cpu_percent(interval=1)}%")
            print(f"[MemoryMonitor] ç³»ç»Ÿå†…å­˜: {psutil.virtual_memory()}")
            if tracemalloc.is_tracing():
                snapshot = tracemalloc.take_snapshot()  # è·å–å†…å­˜åˆ†é…ç»Ÿè®¡
                top_stats = snapshot.statistics('lineno')[:10]
                for stat in top_stats[:10]:
                    print(stat)

        # å½“å†…å­˜ä½¿ç”¨è¶…è¿‡é˜ˆå€¼æ—¶è§¦å‘åƒåœ¾å›æ”¶
        if memory_percent > threshold_percent:  # 70% å†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼
            print(f"[MemoryMonitor] å½“å‰å†…å­˜ä½¿ç”¨ç‡: {memory_percent:.2f}%, RSS: {memory_mb:.2f}MB - è§¦å‘åƒåœ¾å›æ”¶")
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            after = process.memory_info().rss / 1024 / 1024
            print(f"[MemoryMonitor] åƒåœ¾å›æ”¶åå†…å­˜: {after:.2f}MBï¼Œé‡Šæ”¾ï¼š{memory_mb - after:.2f}MB")

    except Exception as e:
        print(f"[MemoryMonitor] error: {e}")


async def get_log_lines(log_path: str | Path = "app.log", lines: int = 100):
    log_file = Path(log_path)
    if not log_file.exists():
        return {"error": f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}"}

    try:
        lines_list = deque(maxlen=lines)
        async with aiofiles.open(log_file, "r", encoding="utf-8", errors="ignore") as f:
            # f.readlines(),await f.read()
            async for line in f:
                lines_list.append(line.strip())

        return {"logs": list(lines_list), "count": len(lines_list)}

    except Exception as e:
        return {"error": f"è¯»å–æ—¥å¿—å¤±è´¥: {str(e)}"}


def get_job_list(scheduler):  # AsyncIOScheduler
    job_list = [
        {'id': job.id,
         'name': job.name,
         'func': str(job.func),
         # 'args': job.args,
         # 'kwargs': job.kwargs,
         'next_run_time': job.next_run_time.isoformat() if job.next_run_time else None,
         'trigger': str(job.trigger)
         } for job in scheduler.get_jobs()
    ]
    return job_list


# ä¿å­˜æ‰€æœ‰æ¨¡å‹åˆ°æ–‡ä»¶
def save_models(models: dict, model_dir='data/models/'):
    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„
    :param models: ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰éœ€è¦ä¿å­˜çš„æ¨¡å‹
    :param model_dir: ä¿å­˜æ¨¡å‹çš„è·¯å¾„
    """
    os.makedirs(model_dir, exist_ok=True)
    for model_name, model in models.items():
        if model is not None:
            joblib.dump(model, f'{model_dir}/{model_name}.pkl')
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³ {model_dir}")


# åŠ è½½æ¨¡å‹
def load_models(model_names: list, model_dir='data/models/'):
    """
    åŠ è½½ä¿å­˜çš„æ¨¡å‹
    :param model_names: æ¨¡å‹åç§°åˆ—è¡¨
    :param model_dir: ä¿å­˜æ¨¡å‹çš„è·¯å¾„
    :return: åŠ è½½çš„æ¨¡å‹å­—å…¸
    """
    models = {}
    for model_name in model_names:
        model_path = f'{model_dir}/{model_name}.pkl'
        if os.path.exists(model_path):
            models[model_name] = joblib.load(model_path)
    print(f"æ¨¡å‹å·²ä» {model_dir} åŠ è½½")
    return models


def save_dictjson(dictionary, file_path, encoding='utf-8', ascii=False):
    with open(file_path, 'w', encoding=encoding) as file:
        json.dump(dictionary, file, ensure_ascii=ascii)


def load_dictjson(file_path, encoding='utf-8'):
    with open(file_path, 'r', encoding=encoding) as file:
        dictionary = json.load(file)
    return dictionary


def load_datasets(path):
    samples = []
    with open(path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            samples.append(json.loads(line.strip()))
    return samples


def pickle_serialize(obj):
    try:
        json.dumps(obj)  # æµ‹è¯•æ˜¯å¦å¯JSONåºåˆ—åŒ–
        return obj
    except (TypeError, ValueError):  # å°†ä¸å¯JSONåºåˆ—åŒ–çš„éƒ¨åˆ†è½¬ä¸ºpickleçš„base64å­—ç¬¦ä¸²
        try:
            data = pickle.dumps(obj)  # è¿”å› bytes ç±»å‹
            return {'__pickle__': base64.b64encode(data).decode('utf-8')}  # è½¬ä¸º ASCII-safe å­—èŠ‚ä¸²
        except Exception as e:
            print(f"Object cannot be pickled: {e}")
            return obj


def pickle_deserialize(obj):
    if isinstance(obj, dict):
        if '__pickle__' in obj:
            try:
                return pickle.loads(base64.b64decode(obj['__pickle__'].encode('utf-8')))  # encoding='bytes' é¿å…è‡ªåŠ¨å¯¼å…¥æ¨¡å—
            except Exception as e:
                raise ValueError(f"Failed to decode pickle: {e}")
    return obj


if __name__ == "__main__":
    def main():
        with open("utils.py", 'r', encoding='utf-8') as file:
            code = file.read()
        functions = extract_function_metadatas(code)
        for func in functions:
            print(func)

        # funcs = get_module_functions('utils.utils')
        # print([i[0] for i in funcs])

        filtered_files = filter_directory(r"/", config={
            "include": ["*.py", "deploy.bat", "requirements.txt", "CI"],
            "recursive_include": {
                "agents": ["*"],
                "script": ["*"],
                "docker": ["*"],
                "data": ["*.yaml", "*.pkl"]
            },
            "prune": [".venv", ".git", "docker_build", "script/data"],
            "exclude": ["script/nlp.py", "script/data/*"],
            "global_exclude": ["__pycache__/*", "*.py[cod]", "*.log"]
        })
        print(json.dumps(filtered_files, indent=2, ensure_ascii=False))


    main()
