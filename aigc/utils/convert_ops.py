import ast
import markdown2
import yaml
from functools import partial
from .base import *


# convert/extract/format/fix/clean


def fix_unbalanced_brackets(name):
    left_count = name.count('(')
    right_count = name.count(')')

    # å¦‚æœå·¦æ‹¬å·æ¯”å³æ‹¬å·å¤šï¼Œè¡¥é½å³æ‹¬å·
    if left_count > right_count:
        name += ')' * (left_count - right_count)
    # å¦‚æœå³æ‹¬å·æ¯”å·¦æ‹¬å·å¤šï¼Œè¡¥é½å·¦æ‹¬å·
    elif right_count > left_count:
        name = '(' * (right_count - left_count) + name

    return name


def fix_indentation(code: str) -> str:
    """
    è°ƒæ•´ç¼©è¿›,ä¿®å¤ä»£ç ç¼©è¿›ï¼Œç¡®ä¿æœ€å°çš„ç¼©è¿›è¢«ç§»é™¤ï¼Œä»¥é¿å…ç¼©è¿›é”™è¯¯
    å¿½ç•¥ç©ºè¡Œï¼Œæå–æ¯è¡Œå·¦ä¾§ç©ºæ ¼æ•°ï¼›
    æ‰¾å‡ºæ‰€æœ‰éç©ºè¡Œä¸­çš„ æœ€å°ç¼©è¿›é‡ï¼ˆmin_indentï¼‰ï¼›
    å¯¹æ¯è¡Œå»æ‰è¿™ä¸ªç¼©è¿›é‡ï¼ˆä¿æŒç›¸å¯¹ç¼©è¿›ç»“æ„ï¼‰ï¼›
    æœ€åæ‹¼æ¥ä¸ºæ–°çš„ä»£ç å­—ç¬¦ä¸²ã€‚
    """
    lines = code.replace("\t", "    ").splitlines()
    non_empty_lines = [line for line in lines if line.strip()]
    if not non_empty_lines:
        return code

    min_indent = min(len(line) - len(line.lstrip()) for line in lines if line.strip())
    fixed_lines = [(line[min_indent:] if len(line) >= min_indent else line).rstrip() for line in lines]
    return "\n".join(fixed_lines)


def fix_invalid_backslashes(match):
    char = match.group(1)
    if char in '"\\/bfnrtu':  # JSON é‡Œåˆæ³•çš„è½¬ä¹‰å­—ç¬¦åªæœ‰è¿™äº›ï¼š " \ bfnrtu
        return '\\' + char  # åˆæ³•ä¿ç•™
    else:
        return '\\\\' + char  # éæ³•çš„è¡¥æˆ \\ + å­—ç¬¦


def extract_python_code(text) -> list[str]:
    """
    æå– Markdown ä»£ç å—ä¸­çš„ Python ä»£ç ï¼ŒåŒæ—¶æ”¯æŒç¼©è¿›ä»£ç å—
    """
    code_blocks = re.findall(r'```(?:python)?(.*?)```', text, re.DOTALL)
    if not code_blocks:
        # æŸ¥æ‰¾ç¼©è¿›ä»£ç å—ï¼Œå³æ¯è¡Œå‰ 4 ä¸ªç©ºæ ¼çš„ä»£ç ï¼Œ æ—  ``` åŒ…å›´çš„ä»£ç å—
        code_blocks = re.findall(r'((?: {4}.*\n)+)', text)

    return [fix_indentation(block) for block in code_blocks]  # [block.strip()]


def extract_any_code(markdown_string: str):
    # Regex pattern to match Python code blocks,åŒ¹é… Pythonä¸å…¶ä»–ä»£ç å—
    pattern = r"```[\w\s]*python\n([\s\S]*?)```|```([\s\S]*?)```"
    # Find all matches in the markdown string
    matches = re.findall(pattern, markdown_string, re.IGNORECASE)
    # Extract the Python code from the matches
    code_blocks = []
    for match in matches:
        code = match[0] or match[1]  # å¦‚æœæ˜¯ Python ä»£ç å—ï¼Œå– ```python ä¹‹åçš„ä»£ç ,å¦‚æœæ˜¯å…¶ä»–ä»£ç å—ï¼Œå–ä»£ç å†…å®¹
        code_blocks.append(code.strip())

    return code_blocks


def extract_json_str_md(json_code: str) -> str:
    """
    æ¨¡å‹è¿”å›çš„å†…å®¹ï¼Œå…¶ä¸­ JSON æ•°æ®é€šå¸¸è¢«åŒ…è£¹åœ¨ Markdown çš„ä»£ç å—æ ‡è®°ä¸­ï¼ˆå³ä»¥ json å¼€å§‹ï¼Œä»¥ ç»“æŸï¼‰
    å¦‚æœæœªæ‰¾åˆ°èµ·å§‹æˆ–ç»“æŸæ ‡è®°ï¼Œå°è¯•ç›´æ¥è§£ææ•´ä¸ªå­—ç¬¦ä¸²ä¸º JSON
    :param json_code:
    :return:
    """
    start = json_code.find("```json")
    # ä»startå¼€å§‹æ‰¾åˆ°ä¸‹ä¸€ä¸ª```ç»“æŸ
    end = json_code.find("```", start + 1)
    if start == -1 or end == -1:
        try:
            json.loads(json_code)
            return json_code
        except Exception as e:
            print("Error:", e)
        return ""
    return json_code[start + 7:end]


def extract_json_str(text: str) -> str:
    """
    å°è¯•ä»è¾“å…¥å­—ç¬¦ä¸²ä¸­æå– JSON æ•°ç»„æˆ–å¯¹è±¡å­—ç¬¦ä¸²ã€‚
    - æ”¯æŒå»é™¤ markdown çš„ ```json ä»£ç å—åŒ…è£…
    - ä¼˜å…ˆæå– JSON æ•°ç»„ï¼ˆ[...]ï¼‰ï¼Œå…¶æ¬¡ä¸ºå¯¹è±¡ï¼ˆ{...}ï¼‰
    - è‹¥æ— æ˜ç¡®ç»“æ„ï¼Œåˆ™å°è¯•è§£ææ•´ä½“å†…å®¹æ˜¯å¦ä¸ºåˆæ³• JSON
    """

    # å»é™¤ markdown åŒ…è£¹
    text = re.sub(r"```json\s*", "", text)
    text = re.sub(r"```\s*", "", text)

    # æŸ¥æ‰¾ JSON æ•°ç»„
    start_index = text.find('[')
    end_index = text.rfind(']')
    if start_index != -1 and end_index != -1 and end_index > start_index:
        return text[start_index:end_index + 1]

    # æŸ¥æ‰¾ JSON å¯¹è±¡
    start_index = text.find('{')
    end_index = text.rfind('}')
    if start_index != -1 and end_index != -1 and end_index > start_index:
        return text[start_index:end_index + 1]

    # å°è¯•æ•´ä½“æ˜¯å¦ä¸ºåˆæ³• JSON
    try:
        json.loads(text)
        return text
    except json.JSONDecodeError as e:
        print(f"æå–å¤±è´¥ï¼Œè¾“å…¥éåˆæ³• JSON: {e}")
        return ""


def parse_json(inputs: dict | str) -> dict:
    #  Markdown ä¸­å®Œæ•´åŒ…è£¹çš„ JSON å—ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼ˆMarkdown JSON å—ã€æ™®é€š JSON å­—ç¬¦ä¸²ã€å­—å…¸ç­‰ï¼‰,æ”¯æŒå·²ç»æ˜¯å­—å…¸çš„è¾“å…¥
    if not isinstance(inputs, dict):
        try:
            match = re.search(r'^\s*(```json\n)?(.*)\n```\s*$', inputs, re.S)
            if match:
                inputs = match.group(2).strip()
            inputs = json.loads(inputs)
        except json.JSONDecodeError as exc:
            raise Exception(f'invalid json format: {inputs}') from exc

    return inputs


def extract_sql_code(text):
    sql_blocks = re.findall(r'```(?:sql)?(.*?)```', text, re.DOTALL)
    if not sql_blocks:
        sql_blocks = re.findall(r'((?:SELECT|INSERT|UPDATE|DELETE).*?;)', text, re.DOTALL)
    return [block.strip() for block in sql_blocks]


def extract_html_code(text):
    html_blocks = re.findall(r'```(?:html)?(.*?)```', text, re.DOTALL)
    if not html_blocks:
        html_blocks = re.findall(r'(<html.*?</html>)', text, re.DOTALL | re.IGNORECASE)
    return [block.strip() for block in html_blocks]


def extract_cpp_code(text):
    cpp_blocks = re.findall(r'```(?:cpp|c\+\+)?(.*?)```', text, re.DOTALL)
    return [block.strip() for block in cpp_blocks]


def extract_java_code(text):
    java_blocks = re.findall(r'```(?:java)?(.*?)```', text, re.DOTALL)
    return [block.strip() for block in java_blocks]


def extract_bash_code(text):
    bash_blocks = re.findall(r'```(?:bash|sh)?(.*?)```', text, re.DOTALL)
    return [block.strip() for block in bash_blocks]


def extract_markdown_blocks(text: str) -> list[str]:
    """
    è¿”å›å†…å®¹å—åˆ—è¡¨ï¼Œå»é™¤åŒ…è£¹çš„ markdown æ ‡è®°
    """
    blocks = re.findall(r"```markdown\n(.*?)```", text, flags=re.DOTALL)
    return [block.strip() for block in blocks]


def extract_table_code(text):
    # æå–æ•´ä¸ªè¡¨æ ¼å—
    table_blocks = re.findall(r'```(?:table)?(.*?)```', text, re.DOTALL)
    if not table_blocks:
        table_blocks = re.findall(r'((?:\|.*?\|)+)', text)  # ç®€å•åŒ¹é… Markdown è¡¨æ ¼ï¼Œå¦‚ | A | B |
    return [block.strip() for block in table_blocks]


def extract_table_blocks(text) -> list[str]:
    """
    æå– Markdown æ ¼å¼çš„è¡¨æ ¼æ•°æ®ï¼Œè¿”å›å®Œæ•´çš„è¡¨æ ¼å—åˆ—è¡¨
    table_blocks: List[str]ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€æ®µå®Œæ•´çš„è¡¨æ ¼ï¼ˆå«å¤šè¡Œï¼‰
    """
    table_pattern = re.compile(r'(?:^\|[^\n]*\|\s*$\n?)+', re.MULTILINE)
    table_blocks = table_pattern.findall(text)
    if table_blocks:
        return [block.strip() for block in table_blocks if block.strip()]

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…ï¼ˆå¤„ç†ä¸è§„èŒƒçš„è¡¨æ ¼,é€è¡Œæ‰¾ä»¥ | å¼€å¤´å¹¶åŒ…å« | çš„è¡Œï¼‰
    table_blocks = []
    current_table = []

    for line in text.splitlines():
        if re.match(r'^\|.*\|$', line.strip()):
            current_table.append(line.strip())
        elif current_table:
            table_blocks.append("\n".join(current_table))
            current_table = []

    if current_table:
        table_blocks.append("\n".join(current_table))

    return [block.strip() for block in table_blocks if block.strip()]


def extract_table_segments(raw_text) -> list[tuple[str, str]]:
    """
    ä»é•¿æ–‡æœ¬ä¸­æå–æ‰€æœ‰è¿ç»­çš„â€œ|...|â€è¡¨æ ¼å—ï¼Œä½œä¸ºä¸€ä¸ªæ•´ä½“æ®µè½è¿”å›ï¼Œ
    å¹¶è¿”å›å»é™¤äº†è¿™äº›è¡¨æ ¼å—åçš„çº¯æ­£æ–‡ã€‚
    :param raw_text: åŸå§‹å¤šè¡ŒåˆåŒæ–‡æœ¬
    :return: [('table',table_blocks), ('text',remaining_text)]
      - table_blocks: strï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€æ®µå®Œæ•´çš„è¡¨æ ¼ï¼ˆå«å¤šè¡Œï¼‰
      - remaining_text: strï¼Œæ²¡æœ‰è¡¨æ ¼å—çš„æ­£æ–‡
    """
    # ordered
    # 1. æ­£åˆ™åŒ¹é…è¿ç»­å¤šè¡Œâ€œ|â€¦|â€è¡¨æ ¼å—
    table_pattern = re.compile(r'(?:^\|[^\n]*\|\s*$\n?)+', re.MULTILINE)

    segments = []
    last_end = 0
    # 2. éå†æ‰€æœ‰è¡¨æ ¼å—
    for m in table_pattern.finditer(raw_text):
        start, end = m.span()
        # 2a. å…ˆæŠŠè¡¨æ ¼å—å‰é¢çš„æ­£æ–‡ç‰‡æ®µæ”¶é›†ä¸‹æ¥
        if start > last_end:
            text_segment = raw_text[last_end:start]
            segments.append(('text', text_segment))
        # 2b. å†æŠŠè¿™ä¸ªè¡¨æ ¼å—æœ¬èº«æ”¶é›†ä¸‹æ¥
        table_block = m.group()
        segments.append(('table', table_block))
        last_end = end

    # 2c. æœ€åæ”¶é›†è¡¨æ ¼å—åå‰©ä½™çš„æ­£æ–‡
    if last_end < len(raw_text):
        segments.append(('text', raw_text[last_end:]))

    return segments


def extract_markdown_table(text: str, convert: bool = True) -> list[dict]:
    """
    ä»Markdownæ ¼å¼æ–‡æœ¬ä¸­æå–è¡¨æ ¼å¹¶è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    ä¿ç•™åŸå§‹å­—ç¬¦ä¸²æ ¼å¼ï¼Œåªåšå¿…è¦çš„æ¸…ç†
    """
    block_match = re.search(r'(?:^\s*\|.*\|\s*$\n?)+', text, re.MULTILINE)  # åŒ¹é…Markdownè¡¨æ ¼æ¨¡å¼
    if not block_match:
        return []

    lines = block_match.group(0).strip().splitlines()
    if len(lines) < 2:
        return []

    def clean_cell(content: str) -> str | None:
        """æ¸…ç†å•å…ƒæ ¼å†…å®¹ï¼Œåªç§»é™¤åŸºæœ¬çš„Markdownæ ¼å¼"""
        if content is None:
            return None

        content = re.sub(r'\*\*([^*]+)\*\*', r'\1', content)  # ç§»é™¤ç®€å•çš„åŠ ç²—æ ‡è®°ï¼ˆ**text**ï¼‰ï¼Œä½†ä¿ç•™å†…å®¹
        content = re.sub(r'\*([^*]+)\*', r'\1', content)  # ç§»é™¤ç®€å•çš„æ–œä½“æ ‡è®°ï¼ˆ*text*ï¼‰ï¼Œä½†ä¿ç•™å†…å®¹
        content = re.sub(r'`([^`]*)`', r'\1', content)  # ç§»é™¤ `code`
        return content.strip() or None  # ç§»é™¤è¡Œé¦–è¡Œå°¾å¤šä½™ç©ºæ ¼

    # æ¸…ç†å’Œæå–è¡¨å¤´
    headers = [clean_cell(col.strip() or f"col{idx}") for idx, col in enumerate(lines[0].strip('|').split('|'))]
    sep_re = re.compile(r'^\s*\|?[-:\s|]+\|?\s*$')
    data_lines = [ln for ln in lines[1:] if not sep_re.match(ln)]  # æ•°æ®è¡Œä»ç¬¬ä¸€è¡Œä¹‹åï¼Œè·³è¿‡æ‰€æœ‰åˆ†éš”çº¿

    # å¤„ç†æ•°æ®è¡Œ
    result = []
    for ln in data_lines:
        cells = [clean_cell(col.strip()) for col in ln.strip('|').split('|')]
        cells += [None] * (len(headers) - len(cells))
        cells = cells[:len(headers)]  # æˆªæ–­å¤šä½™åˆ—
        row = {hdr: convert_num_value(val) if convert and val is not None else val
               for hdr, val in zip(headers, cells)}  # åˆ›å»ºè¡Œæ•°æ®
        result.append(row)

    return result


def parse_table_block(block: str) -> list[list[str]]:
    """
    å°†ä¸€ä¸ªè¿ç»­çš„è¡¨æ ¼å—ï¼ˆå¤šè¡Œä»¥ | å¼€å¤´å’Œç»“å°¾ï¼‰æ‹†æˆè¡Œå’Œå­—æ®µåˆ—è¡¨ã€‚
    :param block: str, å½¢å¦‚:
      "| åˆ—A | åˆ—B |\n| --- | --- |\n| 1  | x   |\n| 2  | y   |\n"
    :return: List[List[str]]ï¼Œå¦‚ [["åˆ—A","åˆ—B"], ["1","x"], ["2","y"]]
    """
    rows = []
    for line in block.strip().split('\n'):
        line = line.strip()
        if not line or not line.startswith('|') or line.count('|') < 2:
            continue
        # æŒ‰ | åˆ†ï¼Œä¸¢æ‰ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªç©ºå­—ç¬¦ä¸²
        cells = [cell.strip() for cell in line.split('|')[1:-1]]
        rows.append(cells)
    return rows


def extract_web_content(html):
    # æå–<title>å†…å®¹
    title_match = re.search(r"<title.*?>(.*?)</title>", html, re.IGNORECASE | re.DOTALL)
    title = title_match.group(1).strip() if title_match else ""

    # æå–<body>å†…å®¹ï¼Œå»é™¤è„šæœ¬ã€æ ·å¼ç­‰æ ‡ç­¾
    body_match = re.search(r"<body.*?>(.*?)</body>", html, re.IGNORECASE | re.DOTALL)
    body_content = body_match.group(1).strip() if body_match else ""

    # ç§»é™¤<script>å’Œ<style>æ ‡ç­¾åŠå…¶å†…å®¹
    body_content = re.sub(r"<(script|style).*?>.*?</\1>", "", body_content, flags=re.IGNORECASE | re.DOTALL)

    # ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾ï¼Œåªä¿ç•™æ–‡æœ¬
    text_content = re.sub(r"<[^>]+>", "", body_content)
    text_content = re.sub(r"\s+", " ", text_content).strip()

    return {"title": title, "content": text_content}


def extract_yaml_data(text):
    """æå– Markdown ä¸­çš„ YAML æ•°æ®"""
    yaml_blocks = re.findall(r'```yaml\n(.*?)\n```', text, re.DOTALL)
    parsed_data = []

    for block in yaml_blocks:
        try:
            parsed_data.append(yaml.safe_load(block))  # è§£æ YAML
        except yaml.YAMLError:
            parsed_data.append(None)  # è§£æå¤±è´¥åˆ™è¿”å› None

    return parsed_data


def extract_transcription_segments(text: str) -> str:
    core = text.strip()  # ç¬¬äºŒéƒ¨åˆ†å°±æ˜¯æˆ‘ä»¬è¦çš„è¯†åˆ«å†…å®¹
    lines = []
    last_line = None
    for line in core.splitlines():
        line = line.strip()  # å¯é€‰ï¼šå»æ‰å¤šä½™ç©ºè¡Œ
        if not line:
            continue
        # å»æ‰æ¯è¡Œçš„ "[xxs - xxs]"
        line = re.sub(r"^\[[^\]]+\]\s*", "", line)
        # åˆå¹¶å’Œä¸Šä¸€è¡Œé‡å¤çš„å†…å®¹
        if line == last_line:
            continue
        lines.append(line)
        last_line = line

    return "\n".join(lines)


def extract_list_data(text) -> list[str]:
    list_blocks = re.findall(r'```(?:list)?(.*?)```', text, re.DOTALL)
    if not list_blocks:
        list_blocks = re.findall(r'(\n\s*[-*].*?(\n\s{2,}.*?)*\n)', text)  # çº¯æ–‡æœ¬åˆ—è¡¨
    return [block.strip() for block in list_blocks]


def extract_json_data(text) -> list[str]:
    # æå– JSON æ ¼å¼çš„ä»£ç å—
    json_blocks = re.findall(r'```(?:json)?(.*?)```', text, re.DOTALL)
    return [block.strip() for block in json_blocks]


def extract_jsons(input_str) -> list[dict]:
    """
    å¤„ç†åŒ…å«å¤šä¸ª JSON å¯¹è±¡çš„æ–‡æœ¬æ•°æ®,æˆåŠŸè§£æäº† JSON å¯¹è±¡ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰è§£æç»“æœçš„åˆ—è¡¨
    :param input_str:
    :return: list[dict]
    """
    # 1,None,-1
    matches = re.findall(r'\{.*?\}', input_str, re.DOTALL)  # regex.findall(r'\{(?:[^{}]|(?R))*\}', input_str)
    if not matches:
        return None
    json_objects = []  # [var.strip() for var in matches if '{' not in var]
    for match in matches:
        try:
            json_objects.append(json.loads(match))
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e} - Skipping this fragment: {match}")

    return json_objects


def clean_any_string(raw_text):
    """
    æ–‡æœ¬æ¸…æ´—å‡½æ•°ï¼Œæ”¯æŒå¤šç§æ–‡æœ¬æ ¼å¼é¢„å¤„ç†ï¼ŒåŒ…æ‹¬é‚®ä»¶ã€URLã€æ—¶é—´ã€æ—¥æœŸã€å¼‚å¸¸ç©ºæ ¼ç­‰ã€‚
    """
    pure_text = raw_text.replace('\n', " ")
    # æ›¿æ¢ä¸å¯è§å­—ç¬¦ï¼ˆå¦‚ \xa0ï¼‰ä¸ºæ™®é€šç©ºæ ¼
    pure_text = pure_text.replace('\xa0', ' ').strip()
    pure_text = re.sub(r'[-â€“â€”]', ' ', pure_text)

    pure_text = re.sub(r"\d+/\d+/\d+", "", pure_text)  # å‰”é™¤æ—¥æœŸ
    pure_text = re.sub(r"[0-2]?[0-9]:[0-6][0-9]", "", pure_text)  # å‰”é™¤æ—¶é—´
    pure_text = re.sub(r"\S+@\S+", "", pure_text)  # å»é™¤ç”µå­é‚®ä»¶
    # #URLï¼Œä¸ºäº†é˜²æ­¢å¯¹ä¸­æ–‡çš„è¿‡æ»¤ï¼Œæ‰€ä»¥ä½¿ç”¨[a-zA-Z0-9]è€Œä¸æ˜¯\w
    url_regex = re.compile(r"""
        (https?://)?             # åè®®éƒ¨åˆ†å¯é€‰
        ([a-zA-Z0-9-]+\.)+       # åŸŸåéƒ¨åˆ†
        [a-zA-Z]{2,}             # é¡¶çº§åŸŸå
        (/[a-zA-Z0-9-]*)*        # è·¯å¾„éƒ¨åˆ†å¯é€‰
    """, re.VERBOSE | re.IGNORECASE)
    pure_text = url_regex.sub(r"", pure_text)
    # pure_text = re.sub("[^\u4e00-\u9fa5]","",pure_text)  #  å»é™¤æ‰€æœ‰éæ±‰å­—å†…å®¹ï¼ˆè‹±æ–‡æ•°å­—ï¼‰
    # pure_text = re.sub(r"\s+", " ", pure_text).strip()  # å¤šä½™ç©ºæ ¼åˆå¹¶
    # å»æ‰å¤šä½™ç©ºæ ¼å’Œè¿ç»­ç©ºæ ¼
    pure_text = re.sub(r'[^\S\r\n]+', ' ', pure_text)  # r'\s+'
    return pure_text.strip()


def clean_json_string(json_str):
    # 1. å»é™¤ // æ³¨é‡Š
    json_str = re.sub(r'//.*', '', json_str)
    # 2. ä¿®å¤éæ³•åæ–œæ ï¼šæŠŠéæ³•çš„ \x è½¬ä¸º x
    json_str = re.sub(r'\\(.)', fix_invalid_backslashes, json_str)
    # 3. æ›¿æ¢ HTML æ ‡ç­¾ã€ä¼ªæ ‡ç­¾ã€éæ³•æ¢è¡Œç¬¦
    json_str = json_str.replace('<br>', '\n')  # æ›¿æ¢ HTML æ ‡ç­¾æˆ–ä¼ªæ ‡ç­¾
    json_str = json_str.replace('<', 'ã€Š').replace('>', 'ã€‹')  # ä¿®å¤ <ucam.xxx> é€ æˆçš„é”™è¯¯
    json_str = json_str.replace('\\"', '"')  # ä¿®å¤è¢«è½¬ä¹‰çš„åŒå¼•å·ï¼ˆå¦‚ \\"ï¼‰

    json_str = re.sub(r'"(reason|suggest)":\s*"([^"]+?)(?=\n\s*")', lambda m: f'"{m.group(1)}": "{m.group(2).strip()}"',
                      json_str)  # å°è¯•è¡¥å…¨ "reason": "... \n  "suggest"

    return json_str


def clean_escaped_string(text: str, force_str: bool = True, decode_unicode: bool = False) -> str:
    """
    å°è¯•å»é™¤å¤–å±‚å¼•å·ï¼Œå¹¶åè½¬ä¹‰ï¼Œè¿”å›å­—ç¬¦ä¸²ã€‚
    force_str=True æ—¶ï¼Œéå­—ç¬¦ä¸²ç»“æœä¹Ÿä¼šè½¬æˆ strã€‚
    decode_unicode=True æ—¶ï¼Œå°è¯•æŠŠ \\uXXXX å½¢å¼è§£ç æˆçœŸå®å­—ç¬¦ã€‚
    """
    try:
        result = ast.literal_eval(text)  # è‡ªåŠ¨å¤„ç† \" \\n ç­‰
        if force_str and not isinstance(result, str):
            result = str(result)
        return result
    except:
        # fallback å¤„ç†
        text = text.strip()
        if re.fullmatch(r'''["'].*["']''', text) and text[0] == text[-1]:
            text = text[1:-1]
        if decode_unicode:
            try:
                return text.encode('utf-8').decode('unicode_escape')
            except:
                pass

    return text


def extract_json_from_string(input_str):
    # ä»ä¸€ä¸ªæ™®é€šå­—ç¬¦ä¸²ä¸­æå– JSON ç»“æ„ï¼Œä½†å¯èƒ½ä¸å¤„ç†åµŒå¥—çš„ JSON
    parsed = extract_json_struct(input_str)
    if parsed:
        return parsed

    match = re.search(r'\{.*}', input_str, re.DOTALL)
    if match:
        json_str = clean_json_string(match.group(0))
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e},{input_str}")

    return extract_json_array(input_str)


def extract_links(text):
    # æå– Markdown æ ¼å¼çš„é“¾æ¥ [é“¾æ¥æ–‡å­—](é“¾æ¥åœ°å€)
    pattern = r'\[([^\]]+)\]\((https?://[^\s)]+)\)'
    links = re.findall(pattern, text)
    return [{'text': link[0], 'url': link[1]} for link in links]


def extract_headers(text):
    # æå– ## æˆ– ### ç­‰æ ‡é¢˜
    headers = re.findall(r'^(#{1,6})\s+(.*)', text, re.MULTILINE)
    return [{'level': len(header[0]), 'text': header[1].strip()} for header in headers]


def extract_imports(text):
    bold_texts = re.findall(r'\*\*(.*?)\*\*', text)  # æå– Markdown æ ¼å¼çš„ **ç²—ä½“**
    italic_texts = re.findall(r'__(.*?)__|\*(.*?)\*', text)  # æå– Markdown æ ¼å¼çš„ __æ–œä½“__ æˆ– *æ–œä½“*
    italic = [italic[0] or italic[1] for italic in italic_texts]
    return {"bold": bold_texts, "italic": italic, "header": extract_headers(text)}


def extract_tagged_content(text, tag="answer"):
    """
    æå–æŒ‡å®šæ ‡ç­¾æœ€åä¸€ä¸ªåŒ¹é…
    Extracts the value from the last occurrence of a specified tag in the text.

    Args:
        text (str): The input text containing the tagged content.
        tag (str): The tag to extract content from (default is 'answer').

    Returns:
        str or None: The extracted content, or None if no valid content is found.
    """
    pattern = rf"<{tag}>(.*?)</{tag}>"  # æ­£åˆ™åŒ¹é… <tag>...</tag>
    matches = re.findall(pattern, text, re.DOTALL)  # è·å–æ‰€æœ‰åŒ¹é…é¡¹"<answer> </answer>""

    if matches:
        last_match = matches[-1].strip()  # è·å–æœ€åä¸€ä¸ªåŒ¹é…çš„å†…å®¹å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
        return None if last_match == "..." else last_match
    return None


# reasoning
def extract_tagged_split(text, tag="think"):
    """
    Splits the text into two parts: the content inside the specified tag
    and the remaining text outside the tag.

    Args:
        text (str): The input text containing the tagged content.
        tag (str): The tag to extract content from (default is 'think',reasoning).

    Returns:
        list: A list containing [tag_content, remaining_text].
    """
    pattern = re.compile(rf"<{tag}>(.*?)</{tag}>\s*(.*)", re.DOTALL)
    match = pattern.search(text)

    if match:
        think_content = match.group(1).strip()  # æå– <think> å†…çš„å†…å®¹,
        output_content = match.group(2).strip()  # æå–æœ€ç»ˆè¾“å‡ºå†…å®¹
        return [think_content, output_content]

    return [None, text]


def ordinal_generator():
    ordinals = ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©']
    for ordinal in ordinals:
        yield ordinal


def remove_markdown_block(text: str) -> str:
    """
    å¦‚æœæ–‡æœ¬ä»¥ ```markdown å¼€å¤´å¹¶ä»¥ ``` ç»“å°¾ï¼Œåˆ™ç§»é™¤è¿™ä¸¤ä¸ªæ ‡è®°ï¼Œè¿”å›ä¸­é—´å†…å®¹ã€‚
    å¦åˆ™è¿”å›åŸå§‹æ–‡æœ¬ã€‚
    """
    match = re.match(r"^```markdown\s*\n(.*?)\n?```$", text.strip(), re.DOTALL)
    if match:
        return match.group(1).strip()
    return text


def remove_markdown(text):
    # å»é™¤ Markdown çš„å¸¸è§æ ‡è®°
    """
    **ç²—ä½“æ–‡æœ¬**
    _æ–œä½“æ–‡æœ¬_
    ![å›¾ç‰‡æè¿°](image_url)
    [é“¾æ¥æ–‡æœ¬](url)
    ### æ ‡é¢˜æ–‡æœ¬
    > å¼•ç”¨å—
    * æ— åºåˆ—è¡¨é¡¹
    1. æœ‰åºåˆ—è¡¨é¡¹
    ~~åˆ é™¤çº¿æ–‡æœ¬~~
    __ä¸‹åˆ’çº¿æ–‡æœ¬__
    """
    text = remove_markdown_block(text)
    text = re.sub(r'(`{1,3})(.*?)\1', r'\2', text, flags=re.DOTALL)  # å»é™¤åå¼•å·ä»£ç å—
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # å»é™¤ç²—ä½“
    text = re.sub(r'\*(.*?)\*', r'\1', text)  # å»é™¤æ–œä½“
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)  # å»é™¤å›¾ç‰‡
    # text = re.sub(r'\[.*?\]\((.*?)\)', r'\1', text)  # å»é™¤é“¾æ¥ï¼Œä½†ä¿ç•™ URL
    text = re.sub(r'\[(.*?)\]\((.*?)\)', r'\1', text)  # å»é™¤é“¾æ¥å’Œ URL
    text = re.sub(r'#{1,6}\s*(.*)', r'\1', text, flags=re.MULTILINE)  # å»é™¤æ ‡é¢˜
    text = re.sub(r'>\s*(.*)', r'\1', text, flags=re.MULTILINE)  # å»é™¤å¼•ç”¨å—
    text = re.sub(r'(\*|-|\+)\s+(.*)', r'\2', text)  # å»é™¤æ— åºåˆ—è¡¨ç¬¦å·
    text = re.sub(r'\d+\.\s+(.*)', r'\1', text)  # å»é™¤æœ‰åºåˆ—è¡¨ç¬¦å·
    text = re.sub(r'~~(.*?)~~', r'\1', text)  # å»é™¤åˆ é™¤çº¿
    text = re.sub(r'_{2}(.*?)_{2}', r'\1', text)  # å»é™¤ä¸‹åˆ’çº¿æ ‡è®°

    text = re.sub(r'\n{2,}', '\n', text)  # å°†å¤šä½™çš„ç©ºè¡Œæ›¿æ¢ä¸ºå•ä¸ªæ¢è¡Œç¬¦,å‹ç¼©ç©ºè¡Œ
    return text.strip()


def format_for_wechat(text):
    formatted_text = text.split("</think>")[-1]  # text:extract_tagged_split(text, tag="think")[1]
    formatted_text = re.sub(r'\*\*(.*?)\*\*', r'âœ¦\1âœ¦', formatted_text)  # **ç²—ä½“** è½¬æ¢ä¸º âœ¦ç²—ä½“âœ¦æ ·å¼
    formatted_text = re.sub(r'!!(.*?)!!', r'â—\1â—', formatted_text)  # !!é«˜äº®!! è½¬æ¢ä¸º â—ç¬¦å·åŒ…å›´
    # formatted_text = re.sub(r'__(.*?)__', r'â€»\1â€»', formatted_text)  # __æ–œä½“__ è½¬æ¢ä¸ºæ˜Ÿå·åŒ…å›´çš„æ ·å¼
    formatted_text = re.sub(r'~~(.*?)~~', r'_\1_', formatted_text)  # ~~ä¸‹åˆ’çº¿~~ è½¬æ¢ä¸ºä¸‹åˆ’çº¿åŒ…å›´
    formatted_text = re.sub(r'\^\^(.*?)\^\^', r'||\1||', formatted_text)  # ^^é‡è¦^^ è½¬æ¢ä¸º ||é‡è¦|| åŒ…å›´
    formatted_text = re.sub(r'######\s+(.*?)(\n|$)', r'[\1]\n', formatted_text)  # ###### å…­çº§æ ‡é¢˜
    formatted_text = re.sub(r'#####\s+(.*?)(\n|$)', r'ã€Š\1ã€‹\n', formatted_text)  # ##### äº”çº§æ ‡é¢˜
    formatted_text = re.sub(r'####\s+(.*?)(\n|$)', r'ã€\1ã€‘\n', formatted_text)  # #### æ ‡é¢˜è½¬æ¢
    formatted_text = re.sub(r'###\s+(.*?)(\n|$)', r'=== \1 ===\n', formatted_text)  # ### ä¸‰çº§æ ‡é¢˜
    formatted_text = re.sub(r'##\s+(.*?)(\n|$)', r'â€” \1 â€”\n', formatted_text)  # ## äºŒçº§æ ‡é¢˜
    formatted_text = re.sub(r'#\s+(.*?)(\n|$)', r'â€» \1 â€»\n', formatted_text)  # # ä¸€çº§æ ‡é¢˜
    # formatted_text = re.sub(r'```([^`]+)```',
    #                         lambda m: '\n'.join([f'ï½œ {line}' for line in m.group(1).splitlines()]) + '\n',
    #                         formatted_text)
    # formatted_text = re.sub(r'`([^`]+)`', r'ã€Œ\1ã€', formatted_text)  # `ä»£ç ` è½¬æ¢ä¸ºã€Œä»£ç ã€æ ·å¼
    # formatted_text = re.sub(r'>\s?(.*)', r'ğŸ’¬ \1', formatted_text)  # > å¼•ç”¨æ–‡æœ¬ï¼Œè½¬æ¢ä¸ºèŠå¤©ç¬¦å·åŒ…å›´
    # formatted_text = re.sub(r'^\s*[-*+]\s+', 'â€¢ ', formatted_text, flags=re.MULTILINE)  # æ— åºåˆ—è¡¨é¡¹
    # formatted_text = re.sub(r'^\s*\d+\.\s+',f"{next(ordinal_iter)} ", formatted_text, flags=re.MULTILINE)  # æœ‰åºåˆ—è¡¨é¡¹
    formatted_text = re.sub(r'\n---+\n', '\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n', formatted_text, flags=re.MULTILINE)  # æ›¿æ¢æ°´å¹³çº¿ r'^---+$'
    formatted_text = re.sub(r'\?{4}', 'âœ¨', formatted_text)
    formatted_text = re.sub(r'\n{2,}', '\n\n', formatted_text)  # è½¬æ¢æ¢è¡Œä»¥é¿å…å¤šä½™ç©ºè¡Œ

    return formatted_text.strip()


def format_to_spans(text: str, codes: bool = True, tables: bool = True) -> str:
    """æŠŠè‡ªå®šä¹‰æ ‡è®°è¯­æ³•è½¬æ¢ä¸º HTML span"""
    import html
    def parse_table(md: str) -> str:
        lines = [l for l in md.splitlines() if l.strip()]
        if not lines or "|" not in lines[0]:
            return md  # ä¸æ˜¯è¡¨æ ¼
        html_table = ["<table>"]
        for i, line in enumerate(lines):
            cols = [c.strip() for c in line.split("|") if c.strip()]
            if not cols:
                continue
            tag = "th" if i == 0 else "td"
            html_table.append("<tr>" + "".join(f"<{tag}>{html.escape(c)}</{tag}>" for c in cols) + "</tr>")
        html_table.append("</table>")
        return "\n".join(html_table)

    def format_text(t: str) -> str:
        # --- æˆ– *** æˆ– ___ ä½œä¸ºæ°´å¹³åˆ†å‰²çº¿
        t = re.sub(r'^\s*(---|\*\*\*|___)\s*$', '<hr>', t, flags=re.M)
        # #### æˆ–æ›´å¤šçš„æ ‡é¢˜ -> small-caps
        t = re.sub(r'^(#{4,7})\s+(.*?)(\n|$)', r'<span class="bold important small-caps">\2</span>\n',
                   t, flags=re.M)
        # ### æ ‡é¢˜ -> large-text uppercase
        t = re.sub(r'###\s(.*?)(\n|$)', r'<span class="bold large-text uppercase">\1</span>\n', t)
        # æ–‡æœ¬æ ‡è®°æ›¿æ¢
        t = re.sub(r'\*\*(.*?)\*\*', r'<span class="bold">\1</span>', t)  # **ç²—ä½“**
        t = re.sub(r'!!(.*?)!!', r'<span class="highlight">\1</span>', t)  # !!é«˜äº®!!
        t = re.sub(r'__(.*?)__', r'<span class="italic">\1</span>', t)  # __æ–œä½“__
        t = re.sub(r'~~(.*?)~~', r'<span class="underline">\1</span>', t)  # ~~ä¸‹åˆ’çº¿~~
        t = re.sub(r'\^\^(.*?)\^\^', r'<span class="important">\1</span>', t)  # ^^é‡è¦^^

        # t = re.sub(r'\n+', '\n', t).strip()
        t = re.sub(r'\n{2,}', '</p><p>', t)  # å¤„ç†å¤šä½™ç©ºè¡Œ -> æ®µè½
        t = t.replace("\n", "<br>")  # æŠŠæ¢è¡Œè½¬æˆ <br>ï¼ˆé¿å…æµè§ˆå™¨å¿½ç•¥ï¼‰
        # # åŒ…è£¹åœ¨ <p> ä¸­ï¼Œä¿è¯ HTML ç»“æ„å®Œæ•´
        # if not t.startswith("<p>"):
        #     t = f"<p>{t}</p>"
        return t

    def repl_codeblock(match):
        code_content = html.escape(match.group(1))
        return f'<pre><code>{code_content}</code></pre>'

    if codes:
        # å¤„ç† ```code```ï¼ŒHTML è½¬ä¹‰å†…éƒ¨
        text = re.sub(r'```(.*?)```', repl_codeblock, text, flags=re.S)
    if tables:
        text = parse_table(text)  # å…ˆå¤„ç†è¡¨æ ¼

    return format_text(text)


def normalize_markdown(text: str) -> str:
    # ç¡®ä¿æ ‡é¢˜åè‡³å°‘æœ‰ä¸€ä¸ªç©ºè¡Œ
    def repl(m):
        title = m.group(1)
        rest = m.group(2)
        if rest.startswith("\n\n"):
            return title + rest
        else:
            return title + "\n\n" + rest

    text = text.replace("\\n", "\n")
    pattern = re.compile(r'^(#{1,6} .+?)(\n.*)', re.MULTILINE | re.DOTALL)  # ç¡®ä¿æ ‡é¢˜åè‡³å°‘æœ‰ä¸€ä¸ªç©ºè¡Œ
    return pattern.sub(repl, text)


def format_for_html(text: str, html: bool = False):
    # Markdown æ ¼å¼çš„æ–‡æœ¬è½¬æ¢ä¸º HTML çš„å­—ç¬¦ä¸²,æ¸²æŸ“ Markdown æ–‡ç« 
    # markdown.markdown(text, extensions=['tables', 'codehilite'])
    # from IPython.display import Markdown, display
    # display(Markdown(f"`{export_command}`"))
    style = """
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Noto Sans", Arial, sans-serif; }
        table { border-collapse: collapse; width: 100%; max-width: 100%; overflow-x: auto; display: block; }
        th, td { border: 1px solid #999; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        pre { background-color: #1e1e1e; color: #d4d4d4; padding: 8px; border-radius: 5px; overflow-x: auto; font-size: 0.9em;}
        code { font-family: "Courier New", monospace; background-color: #1e1e1e; color: #d4d4d4; padding: 2px 4px; border-radius: 3px;}
    </style>
    """
    text = normalize_markdown(text)
    html_content = markdown2.markdown(text, extras=["tables", "fenced-code-blocks", "break-on-newline"])
    return f"<html><head>{style}</head><body>{html_content}</body></html>" if html else html_content


def safe_convert_html(text):
    '''è¿‡æ»¤å±é™©æ ‡ç­¾ï¼šé˜²æ­¢ XSS æ”»å‡»'''
    from markdown import Markdown
    from bs4 import BeautifulSoup
    html = Markdown(extensions=['tables']).convert(text)
    soup = BeautifulSoup(html, 'html.parser')
    # ç§»é™¤ script ç­‰å±é™©æ ‡ç­¾
    for tag in soup.find_all(['script', 'iframe']):
        tag.decompose()
    return str(soup)


def format_table_md(records: list[dict]) -> str:
    seen = []
    for r in records:
        for k in r.keys():
            if k not in seen:
                seen.append(k)
    headers = seen  # list(records[0].keys())

    lines = ["| " + " | ".join(headers) + " |",  # header_line
             "| " + " | ".join(["---"] * len(headers)) + " |"]  # divider_line
    for row in records:  # rows
        lines.append("| " + " | ".join(str(row.get(h, "")) for h in headers) + " |")
    return "\n".join(lines)


def format_content_str(content, level=0, exclude_null=True) -> str:
    if exclude_null and content is None:
        return ""  # "*ï¼ˆç©ºå†…å®¹ï¼‰*"

    if isinstance(content, dict):
        if exclude_null and not content:
            return ""
        if all(isinstance(x, str) for x in content.keys()):
            heading_prefix = ("\n" + " " * level) if level > 0 else ''  # "#" * (level + 2)
            lines = [f"{heading_prefix}**{key}**:{format_content_str(val, level + 1, exclude_null)}"
                     for key, val in content.items()
                     if not exclude_null or val is not None]  # ç»Ÿä¸€èµ°é€’å½’å¤šè¡Œè¾“å‡º
            return "\n".join(lines)
        return json.dumps(content, ensure_ascii=False, indent=2)

    if isinstance(content, (list, tuple)):
        if exclude_null and not content:
            return ""  # "*ï¼ˆç©ºåˆ—è¡¨ï¼‰*"
        return "\n".join(format_content_str(x, level, exclude_null) for x in content)  # format_table/str/any

    if isinstance(content, (int, float, bool)):
        return str(content)

    return str(content).strip()


def format_summary_text(summary_data: dict | list[dict], title_map: dict = None, base_level=3) -> str:
    """
    å­—æ®µä¸­æ–‡æ ‡é¢˜æ˜ å°„
    å°†ç»“æ„åŒ– summary_data è½¬ä¸ºåˆ†èŠ‚å±•ç¤ºæ–‡æœ¬ï¼ˆmarkdown é£æ ¼ï¼‰
    - æ¸²æŸ“é¡ºåºä»¥ summary_data æœ¬èº«ä¸ºå‡†
    """
    title_map = title_map or {}
    numerals = "ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å"

    def render_one(data: dict, index: int = None):
        values = list(data.values())
        if all(isinstance(v, (int, float, bool)) for v in values) or all(
                isinstance(v, str) and len(v) < 50 for v in values):
            return f"```json\n{json.dumps(map_fields(data, title_map), ensure_ascii=False, indent=2)}```"

        num_iter = iter(numerals)
        sections = []
        for i, (key, content) in enumerate(data.items()):
            if not content:
                continue
            if title_map and key not in title_map:
                continue
            # è‡ªåŠ¨è½¬ä¸ºå­—ç¬¦ä¸²ï¼ˆæ”¯æŒå­—å…¸æˆ–è¡¨æ ¼ç»“æ„ï¼‰
            num = next(num_iter, str(i + 1))
            prefix = f"{'#' * (base_level + 1)} {num}ã€{title_map.get(key, key)}"
            if isinstance(content, list) and all(isinstance(x, dict) for x in content):
                content_str = format_table_md(content)
            else:
                content_str = remove_markdown_block(format_content_str(content))
            sections.append(f"{prefix}\n\n{content_str}\n")

        md_text = "\n\n---\n\n".join(sections)
        if index is not None:
            return f"{'#' * base_level}  ç¬¬ {index + 1} æ¡\n\n" + md_text
        return md_text

    if isinstance(summary_data, list):
        return "\n\n---\n\n".join(render_one(item, i) for i, item in enumerate(summary_data))
    return render_one(summary_data)


def split_summary_chunks(text: str) -> list[str]:
    """
     æ¸…æ´—å¤§æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬ï¼Œåˆ†æˆè‡ªç„¶æ®µ + æ¸…é™¤ bullet/markdown/ç¬¦å·å‰ç¼€
    """

    def clean_lines(line: str) -> str:
        return re.sub(r"^[-â€“â€¢\d\)\.\s]+", "", line).strip()  # æ¸…é™¤ bullet/æ•°å­—/å¤šä½™ç¬¦å·

    normalized = re.sub(r'\n{2,}', '\n\n', text.strip())
    return [clean_lines(chunk.replace("\n", " ")) for chunk in normalized.split("\n\n") if chunk.strip()]


def split_text_into_sentences(raw_text: str) -> list[str]:
    # é€å­—æ‰¾æ ‡ç‚¹åˆ†å‰²ï¼Œä½¿ç”¨å¸¸è§çš„æ ‡ç‚¹ç¬¦å·åˆ†å‰²æ–‡æœ¬ï¼Œç”Ÿæˆå¥å­åˆ—è¡¨
    sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', '.', '!', '?', ';']  # å¸¸è§ä¸­æ–‡/è‹±æ–‡æ ‡ç‚¹
    sentences = []
    current_sentence = ""

    for char in raw_text:
        current_sentence += char
        if current_sentence[-1] in sentence_endings:
            sentences.append(current_sentence.strip())
            current_sentence = ""

    # å¦‚æœæœ‰æ®‹ç•™çš„æ–‡æœ¬ï¼ŒåŠ å…¥å¥å­åˆ—è¡¨
    if current_sentence.strip():
        sentences.append(current_sentence.strip())

    return sentences


def remove_parentheses(entity):
    keys = {'ï¼»', '(', '[', 'ï¼ˆ'}
    symbol = {'ï¼½': 'ï¼»', ')': '(', ']': '[', 'ï¼‰': 'ï¼ˆ'}
    stack = []
    remove = []
    for index, s in enumerate(entity):
        if s in keys:
            stack.append((s, index))
        if s in symbol:
            if not stack: continue
            temp_v, temp_index = stack.pop()
            if entity[index - 1] == '\\':
                t = entity[temp_index - 1:index + 1]
                remove.append(t)
            else:
                remove.append(entity[temp_index:index + 1])

    for r in remove:
        entity = entity.replace(r, '')
    return entity


def split_sentences(text,
                    pattern=(r'[^ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d\r\n]*\b[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\ã€'  # ä¸­æ–‡åºå· "ä¸€ã€äºŒã€"
                             r'|[^ï¼ˆ(ï¼‰)]*\b[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ï¼‰)]'  # æ‹¬å·å†…çš„ä¸­æ–‡åºå· "(ä¸€)(äºŒ)"
                             r'|[^\d\r\n]*\b\d+\ã€'  # æ•°å­—åºå· "1ã€2ã€"
                             r'|[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]'  # å¥å·ã€æ„Ÿå¹å·ã€é—®å·
                             r'|[^\r\n]*\r?\n'  # æ¢è¡Œç¬¦ï¼ˆæ”¯æŒ Windows çš„ \r\n å’Œ Unix çš„ \nï¼‰
                             )
                    ) -> list[str]:
    """
    åˆ†å¥å‡½æ•°ï¼Œæ”¯æŒæŒ‰æ ‡ç‚¹ç¬¦å·å’Œç»“æ„åŒ–åºå·è¿›è¡Œåˆ†å¥ï¼Œåˆ†éš”ç¬¦ä¼šä¿ç•™åœ¨å‰ä¸€å¥ç»“å°¾ã€‚ç»“æ„åŒ–æ¯”è¾ƒæ¸…æ™°çš„åˆåŒã€åˆ¶åº¦æ–‡ä»¶ã€‚ç²—ç²’åº¦åˆ†å¥ï¼ˆä»¥è‡ªç„¶è¯­è¨€çš„æ ‡ç‚¹/åºå·ä¸ºä¸»ï¼‰
    :param text: è¾“å…¥çš„æ–‡æœ¬
    :param pattern: æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…åˆ†éš”ç¬¦
    :return: åˆ†å‰²åçš„å¥å­åˆ—è¡¨
    """
    if not pattern:
        pattern = r'(?=[ã€‚ï¼ï¼Ÿ])'
    sentences = re.findall(pattern, text)
    # re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
    return [s.strip() for s in sentences if s.strip()]


def split_sentences_clean(text, h_symbols=True, h_tables=True) -> list[str]:
    """
    åˆåŒã€è§„ç« ã€å¸¦å¤§é‡ç¼–å·ã€æ¡æ¬¾ã€è¡¨æ ¼çš„æ–‡æœ¬,åˆ†å¥å»ºæ¨¡ã€æ‘˜è¦ã€åˆ‡å—å¤„ç†
    ç¯‡ç« åˆ†å¥ï¼Œé¢å¤–æ”¯æŒï¼š
      - ç¬¬Xæ¡ï¼ˆä¸­å›½å¼æ¡æ¬¾ï¼‰
      - (ä¸€)ã€(1)ã€(a) ç­‰æ‹¬å·ç¼–å·
      - 1.1ã€2.3.4 ç­‰å¤šçº§å°æ•°ç¼–å·
    :param text: str, æ•´æ®µåŸå§‹æ–‡æœ¬
    :param h_symbols: bool, æ˜¯å¦å¤„ç†è¿ç»­ç¬¦å·å’Œæ¢è¡Œç¬¦æ ‡å‡†åŒ–
    :param h_tables: bool, æ˜¯å¦å¤„ç†è¡¨æ ¼ç¬¦å·â€œ|â€
    :return: list of sentences
    """
    # 1. ç»Ÿä¸€æ¢è¡Œç¬¦
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    text = re.sub(r'\n{2,}', '\n\n', text.strip())
    if h_symbols:
        # 2. åœ¨å„ç§åºå·åé¢åŠ ç©ºæ ¼ï¼Œé¿å…ä¸æ­£æ–‡ç²˜è¿
        # ï¼ˆ1ï¼‰ä¸­å›½å¼æ¡æ¬¾ï¼šç¬¬Xæ¡
        text = re.sub(r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ¡)', r'\1 ', text)
        # ï¼ˆ2ï¼‰æ‹¬å·ç¼–å·ï¼š(ä¸€)ã€(1)ã€(a)â€¦â€¦
        text = re.sub(r'(\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\dA-Za-z]+\))', r'\1 ', text)
        # ï¼ˆ3ï¼‰å¤šçº§å°æ•°ç¼–å·ï¼š1.1ã€2.3.4â€¦â€¦
        text = re.sub(r'(\d+(?:\.\d+)+)', r'\1 ', text)

        # 3. ç‰¹æ®Šå¤„ç†è¡¨æ ¼â€œ|åºå·.â€ã€â€œ|åºå·ã€â€
        text = re.sub(r'(\|\s*\d+[\.ã€])', r'\1 ', text)
        text = re.sub(r'(^|\n)\s*(\d+[\.ã€])', r'\1\2 ', text)

    if h_tables:
        # 4. æŠŠè¡¨æ ¼åˆ†éš”ç¬¦ â€˜|â€™ çœ‹ä½œå¥å·
        text = text.replace('|', 'ã€‚')

    # 5. åˆå¹¶è¿ç»­ä¸­æ–‡æ ‡ç‚¹
    text = re.sub(r'[ã€‚ï¼ï¼Ÿï¼›]{2,}', 'ã€‚', text)

    # 6. æŒ‰ä¸­æ–‡å¥å·ã€é—®å·ã€å¹å·ã€åˆ†å·åˆ‡å¥
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›])', text)

    # 7. å»ç©ºç™½ï¼Œè¿‡æ»¤å¤ªçŸ­çš„
    return [s.strip() for s in sentences if s.strip() and len(s.strip()) > 1]


def structure_aware_chunk(text, max_size: int = 1000) -> list[str]:
    """
    æŒ‰ç»“æ„ä¼˜å…ˆåˆ‡åˆ†æ–‡æœ¬ï¼Œä¿è¯æ¯å—å°½é‡ä¸è¶…è¿‡ max_sizeã€‚
    - æŒ‰åˆ†éš”ç¬¦é€’å½’åˆ‡åˆ†,ä»â€œè‡ªç„¶è¾¹ç•Œâ€åˆ°â€œç¡¬åˆ‡å‰²â€ï¼Œæ¸è¿›å¼åˆ†å‰²
    - å¦‚æœä»è¶…è¿‡ max_sizeï¼Œåˆ™ç»§ç»­ç»†åˆ†
    """

    # ä¼˜å…ˆçº§åˆ†éš”ç¬¦ï¼ˆæ ¹æ®ç»éªŒæ’åºï¼‰
    separators = [
        "\n\n",  # æ®µè½è¾¹ç•Œ - æœ€è‡ªç„¶
        "ã€‚\n",  # ä¸­æ–‡å¥å­ + æ¢è¡Œ
        ".\n",  # è‹±æ–‡å¥å­ + æ¢è¡Œ
        "ã€‚",  # ä¸­æ–‡å¥å·
        ".",  # è‹±æ–‡å¥å·
        "ï¼›",  # ä¸­æ–‡åˆ†å·
        ";",  # è‹±æ–‡åˆ†å·
        "ï¼Œ",  # ä¸­æ–‡é€—å·
        ",",  # è‹±æ–‡é€—å·ï¼ˆæœ€åçš„æ‰‹æ®µï¼‰
    ]

    def recursive_split(t: str, sep_index: int = 0) -> list[str]:
        if len(t) <= max_size:
            return [t]

        if sep_index >= len(separators):
            # æœ€åä¸€å±‚ç¡¬åˆ‡
            return [t[i:i + max_size] for i in range(0, len(t), max_size)]

        sep = separators[sep_index]
        if sep in t:
            parts = t.split(sep)
            result = []
            for part in parts:
                if not part.strip():
                    continue
                sub_chunks = recursive_split(part, sep_index + 1)
                # æŠŠåˆ†éš”ç¬¦åŠ å›åˆ°æ¯ä¸ª sub_chunk çš„ç»“å°¾ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
                for i, sc in enumerate(sub_chunks):
                    if i < len(sub_chunks) - 1 or part.endswith(sep):
                        result.append(sc + sep)
                    else:
                        result.append(sc)
            return result

        return recursive_split(t, sep_index + 1)

    chunks = recursive_split(text)
    return [c.strip() for c in chunks if c.strip()]


def cross_sentence_chunk(sentences: list[str], chunk_size=5, overlap_size=2, max_length=512,
                         model_name="gpt-3.5-turbo"):
    """
    æ»‘åŠ¨çª—å£åˆ†å— + æœ€å¤§é•¿åº¦æˆªæ–­ï¼ˆåªæµ‹é•¿åº¦ï¼Œä¸ç”¨ tokenizer.decodeï¼‰
    :param sentences: åˆ†å¥åçš„å¥å­åˆ—è¡¨
    :param chunk_size: æ¯å—åŒ…å«å‡ ä¸ªå¥å­
    :param overlap_size: ç›¸é‚»å—é‡å å‡ ä¸ªå¥å­
    :param max_length: æœ€å¤§é•¿åº¦ï¼ˆtokenæ•°æˆ–å­—ç¬¦æ•°ï¼‰
    :param  model_name: ç”¨äºè®¡ç®— token é•¿åº¦çš„åˆ†è¯å™¨
    :return: List[str] æ¯å—ä¸€ä¸ªå­—ç¬¦ä¸²
    """
    tokenizer = get_tokenizer(model_name)
    chunks = []
    step = max(chunk_size - overlap_size, 1)

    for i in range(0, len(sentences), step):
        window = sentences[i: i + chunk_size]
        text = " ".join(window)

        if tokenizer:
            token_count = len(tokenizer.encode(text))
            if token_count > max_length:
                # ç›´æ¥æŒ‰å­—ç¬¦æˆªæ–­ï¼Œå¯èƒ½å¥å­å‰²è£‚
                text = text[: max_length]
        else:
            # ç”¨å­—ç¬¦é•¿åº¦ä½œä¸º fallback
            if len(text) > max_length:
                text = text[: max_length]

        chunks.append(text)

    return chunks


def organize_segments_chunk(sentences: list[str], chunk_size=7, overlap_size=2, max_length=1024,
                            tokenizer=None) -> list[list[str]]:
    """
    äº¤å‰åˆ†å—å‡½æ•°ï¼Œå°†å¥å­åˆ—è¡¨æŒ‰å—åˆ’åˆ†ï¼Œå¹¶åœ¨å—ä¹‹é—´ä¿æŒä¸€å®šé‡å ï¼Œå¹¶æ ¹æ®max_lengthæ§åˆ¶æ¯ä¸ªæ®µè½çš„æœ€å¤§é•¿åº¦ã€‚
    :param sentences: åˆ†å¥åçš„å¥å­åˆ—è¡¨ split_sentences_clean
    :param chunk_size: æ¯ä¸ªå—çš„å¥å­æ•°é‡
    :param overlap_size: å—ä¹‹é—´çš„é‡å å¥å­æ•°
    :param max_length: æ¯ä¸ªå—çš„æœ€å¤§é•¿åº¦ï¼ˆtokenæ•°ï¼‰
    :param tokenizer: ç”¨äºè®¡ç®—tokené•¿åº¦çš„åˆ†è¯å™¨æ¨¡å‹ï¼ˆTokenizerï¼‰
    :return: äº¤å‰åˆ†å—åçš„å¥å­å—åˆ—è¡¨
    """
    # Step 1: æ„å»ºåŸºç¡€å—
    base_chunks = []
    current = []
    for sent in sentences:
        if lang_token_size(" ".join(current + [sent]), tokenizer=tokenizer) <= max_length:
            current.append(sent)
        else:
            if current:
                base_chunks.append(current)
            # å•å¥è¿‡é•¿ -> å†ç¡¬åˆ‡ï¼Œ ç”¨ tokenizer åªæµ‹é•¿åº¦ï¼Œä¸ decode
            if lang_token_size(sent, tokenizer=tokenizer) > max_length:
                sub_chunks = structure_aware_chunk(sent, max_size=max_length)  # å•å¥ä¹Ÿè¶…é•¿ï¼ŒæŒ‰æ ‡ç‚¹å¼ºåˆ¶æˆå—
                base_chunks.extend([[sc] for sc in sub_chunks])
                current = []
            else:
                current = [sent]
    if current:
        base_chunks.append(current)  # æ·»åŠ å½“å‰å—

    # Step 2: æ»‘åŠ¨çª—å£ï¼Œå¤„ç†æ»‘åŠ¨çª—å£é‡å ï¼Œç»„ç»‡å¤§ç‰‡æ®µ
    overlapped = []
    step = chunk_size - overlap_size
    for i in range(0, len(base_chunks), step):
        merged = []
        for j in range(i, min(i + chunk_size, len(base_chunks))):
            merged.extend(base_chunks[j])

        # ç¡®ä¿ merged ä¸è¶…è¿‡ max_length
        temp = []
        for s in merged:
            if lang_token_size(" ".join(temp + [s]), tokenizer=tokenizer) > max_length:
                overlapped.append(temp)
                temp = [s]
            else:
                temp.append(s)
        if temp:
            overlapped.append(temp)

    return overlapped


# å®ç°å°åˆ°å¤§åˆ†å—é€»è¾‘
def organize_segments(tokens: list[int | str], small_chunk_size: int = 175, large_chunk_size: int = 512,
                      overlap: int = 20):
    '''
    å°å—é€‚åˆç”¨äºæŸ¥è¯¢åŒ¹é…ï¼Œæé«˜æŸ¥è¯¢çš„ç²¾å‡†åº¦ã€‚
    å¤§å—åˆ’åˆ†ï¼Œå°†åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å¤šä¸ªå°å—åˆå¹¶ä¸ºè¾ƒå¤§çš„ç‰‡æ®µã€‚
    æ»‘åŠ¨çª—å£ï¼šä¸ºäº†ä¿æŒä¸Šä¸‹æ–‡å…³ç³»ï¼Œåœ¨å°å—å’Œå¤§å—ä¹‹é—´æ·»åŠ ä¸€å®šçš„é‡å åŒºåŸŸï¼Œç¡®ä¿è¾¹ç¼˜ä¿¡æ¯ä¸ä¸¢å¤±ã€‚è¿™æ ·ï¼ŒæŸ¥è¯¢ç»“æœèƒ½ä¿æŒæ›´é«˜çš„è¿è´¯æ€§ã€‚
    '''

    # å°å—åˆ†å‰²
    small_chunks = []
    for i in range(0, len(tokens), small_chunk_size - overlap):
        small_chunks.append(tokens[i:i + small_chunk_size])  # ''.join()

    # ç»„ç»‡å¤§ç‰‡æ®µ
    large_chunks = []
    for i in range(0, len(small_chunks), large_chunk_size // small_chunk_size):
        large_chunk = []
        for j in range(i, min(i + large_chunk_size // small_chunk_size, len(small_chunks))):
            large_chunk.extend(small_chunks[j])
        large_chunks.append(large_chunk[:large_chunk_size])

    return small_chunks, large_chunks


def extract_code_blocks(text, lag='python', **kwargs):
    # ä»æ–‡æœ¬ä¸­æå–ç‰¹å®šæ ¼å¼çš„ä»£ç å—ï¼Œæ”¯æŒä¸åŒçš„ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ Pythonã€SQLã€HTML ç­‰ï¼‰ä»¥åŠè¡¨æ ¼ã€JSONã€åˆ—è¡¨ç­‰æ•°æ®ç±»å‹
    funcs = {
        "sql": extract_sql_code,
        "html": extract_html_code,
        "python": extract_python_code,
        "cpp": extract_cpp_code,
        "java": extract_java_code,
        "bash": extract_bash_code,
        "code": extract_any_code,
        "method": extract_method_calls,

        "md": extract_markdown_blocks,
        "table": extract_table_code,
        "yaml": extract_yaml_data,
        "list": extract_list_data,
        "json": extract_json_data,
    }
    if lag in funcs:
        return funcs[lag](text)

    # æå– ``` åŒ…è£¹çš„ä»£ç å—
    code_blocks = re.findall(r'```(\w+)?\n(.*?)```', text, re.DOTALL)  # r'```(.*?)```'
    if lag:
        code_blocks = [block for block in code_blocks if block.lstrip().startswith(lag)]
        return code_blocks  # è¿‡æ»¤å‡ºæŒ‡å®šè¯­è¨€çš„ä»£ç å—

    # try:
    #     for k, f in funcs.items():
    #         print(k,f(text))
    # except Exception as e:
    #     print(k,e)

    return {k: f(text) for k, f in funcs.items()}


def extract_string(text, extract: str | list, **kwargs):
    if not extract or extract == 'raw':
        return None
    funcs = {
        "jsons": extract_jsons,
        "json": extract_json_struct,
        "json_array": extract_json_array,
        "json_object": extract_json_from_string,

        "header": extract_headers,
        "imports": extract_imports,
        "links": extract_links,
        'urls': extract_text_urls,

        "tables": extract_table_blocks,
        "table_segments": extract_table_segments,
        "table_data": extract_markdown_table,
        "answer": extract_tagged_content,
        "think": partial(extract_tagged_split, tag="think"),
        "reasoning": partial(extract_tagged_split, tag="reasoning"),
        'clean_any': clean_any_string,
        'sentence': split_sentences,
        'sentences_clean': split_sentences_clean,
        'split_chunks': split_summary_chunks,
        "wechat": format_for_wechat,
        'remark': remove_markdown,
        "html": format_for_html,
        "spans": format_to_spans,
        "web": extract_web_content,
    }

    def run_extract(e):
        if e in funcs:
            return funcs[e](text, **kwargs)  # str,list,dict,set
        extract_type = e.split('.')
        if extract_type[0] == 'code':
            return extract_code_blocks(text, lag=extract_type[1] if len(extract_type) > 1 else '', **kwargs)
        return None

    try:
        if isinstance(extract, str):
            result = run_extract(extract)
            if result:
                return result

            if extract not in funcs:
                return {k: f(text, **kwargs) for k, f in funcs.items()}  # "type": "all", dict

        elif isinstance(extract, list):
            results = {}
            for e in extract:
                val = run_extract(e)
                if val is not None:
                    results[e] = val
            if results:
                return results  # dict

    except Exception as e:
        print(e)

    return None
