import httpx
import json, os, io, time, uuid
from datetime import datetime
from typing import Optional, Type, Dict, List, Tuple, Any, Union, Literal
from contextlib import asynccontextmanager, contextmanager
from starlette.requests import Request
from starlette.responses import JSONResponse, Response, StreamingResponse
from redis.asyncio import Redis, StrictRedis, ConnectionPool
from neo4j import GraphDatabase, AsyncGraphDatabase
from dask.distributed import Client as DaskClient, LocalCluster
from qdrant_client import AsyncQdrantClient, QdrantClient
from openai import AsyncOpenAI, OpenAI
from fastmcp import FastMCP, Context as MCPContext, Client as MCPClient, settings
import oss2

# https://gofastmcp.com/servers/context

from .base import *
from .mysql_ops import OperationMysql
from .task_ops import HierarchicalTimeWheel
from utils import async_to_sync, generate_hash_key, is_port_open, chunks_iterable, get_file_type_wx
from config import Config, AI_Models, model_api_keys

# Config.load('config.yaml')
# if os.getenv('AIGC_DEBUG', '0').lower() in ('1', 'true', 'yes'):
#     Config.debug()

_httpx_clients: Dict[str, httpx.AsyncClient] = {}
_graph_driver: Optional[GraphDatabase] = None
# _graph_driver_lock = asyncio.Lock()  # é˜²æ­¢å¹¶å‘åˆå§‹åŒ–
_redis_clients: Dict[int, Optional[Redis]] = {}  # StrictRedis(host='localhost', port=6379, db=0)
_redis_pools: Dict[int, Optional[ConnectionPool]] = {}
_dask_cluster: Optional[LocalCluster | str] = None
_dask_client: Optional[DaskClient] = None
AI_Client: Dict[str, Optional[AsyncOpenAI]] = {}  # OpenAI
QD_Client = AsyncQdrantClient(host=Config.QDRANT_HOST, grpc_port=Config.QDRANT_GRPC_PORT,
                              prefer_grpc=True) if Config.QDRANT_GRPC_PORT else AsyncQdrantClient(url=Config.QDRANT_URL)
DB_Client = OperationMysql(async_mode=True, minsize=2)

AliyunBucket = oss2.Bucket(oss2.Auth(Config.ALIYUN_oss_AK_ID, Config.ALIYUN_oss_Secret_Key), Config.ALIYUN_oss_endpoint,
                           Config.ALIYUN_Bucket_Name)
logger = get_root_logging(file_name="app.log")  # logging.getLogger(__name__)
mcp = FastMCP(name="FastMCP Server")  # Create a server instance,main_mcp


# mcp_app = mcp.http_app(transport="streamable-http", path="/mcp")

# dependencies=["pandas", "matplotlib", "requests"]

def get_scheduler(redis_host: str = Config.REDIS_HOST, redis_port: int = Config.REDIS_PORT,
                  timezone: str = "Asia/Shanghai"):
    """
    åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªå…¨å±€ AsyncIOScheduler è°ƒåº¦å™¨å®ä¾‹ã€‚
    æ”¯æŒ Redis JobStore + å†…å­˜ JobStoreï¼Œé»˜è®¤å¼‚æ­¥æ‰§è¡Œå™¨ã€‚
    """
    from apscheduler.schedulers.asyncio import AsyncIOScheduler
    from apscheduler.executors.asyncio import AsyncIOExecutor
    from apscheduler.jobstores.memory import MemoryJobStore
    from apscheduler.jobstores.redis import RedisJobStore
    # from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
    # from apscheduler.schedulers.background import BackgroundScheduler
    # from apscheduler.executors.pool import ThreadPoolExecutor

    # executor = concurrent.futures.ThreadPoolExecutor(max_workers=5)
    # scheduler = BackgroundScheduler(jobstores={'default': SQLAlchemyJobStore(engine=engine), 'memory': MemoryJobStore()},
    #                                 executors={'default': ThreadPoolExecutor(4)}, timezone='Asia/Shanghai')  # è®¾ç½®çº¿ç¨‹æ± å¤§å°
    scheduler = AsyncIOScheduler(
        executors={"default": AsyncIOExecutor()},
        jobstores={
            "memory": MemoryJobStore(),
            "redis": RedisJobStore(
                jobs_key="apscheduler.jobs",
                run_times_key="apscheduler.run_times",
                host=redis_host, port=redis_port, db=0,
            ),
        },
        timezone=timezone,
    )  # å¼‚æ­¥è°ƒåº¦å™¨

    if not scheduler.running:
        scheduler.start()
        print(f"[scheduler] Started with timezone={timezone}")
    # scheduler.shutdown()
    return scheduler


def get_httpx_client(time_out: float = None, proxy: str = None) -> httpx.AsyncClient:
    # @asynccontextmanager
    key = proxy or "default"
    global _httpx_clients
    if key not in _httpx_clients or _httpx_clients[key].is_closed:
        transport = httpx.AsyncHTTPTransport(proxy=proxy or None)
        limits = httpx.Limits(max_connections=Config.MAX_HTTP_CONNECTIONS,
                              max_keepalive_connections=Config.MAX_KEEPALIVE_CONNECTIONS)
        timeout = httpx.Timeout(timeout=time_out or Config.HTTP_TIMEOUT_SEC, read=60.0, write=30.0, connect=5.0)
        _httpx_clients[key] = httpx.AsyncClient(transport=transport, limits=limits, timeout=timeout)
    # try:
    #     yield _httpx_clients[key] #Depends(get_httpx_client)
    # finally:
    #     # æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œå…³é—­å®¢æˆ·ç«¯ï¼Œå› ä¸ºå®ƒæ˜¯å•ä¾‹ï¼Œå…¨å±€ç”¨çš„
    #     pass

    return _httpx_clients[key]


async def shutdown_httpx():
    for key, _client in _httpx_clients.items():
        if _client and not _client.is_closed:
            await _client.aclose()


def get_redis(db: int = 0) -> Optional[Redis]:
    global _redis_clients, _redis_pools
    if db not in _redis_clients or _redis_clients[db] is None:
        pool = ConnectionPool(host=Config.REDIS_HOST, port=Config.REDIS_PORT, db=db,
                              decode_responses=True,  # è‡ªåŠ¨è§£ç ä¸ºå­—ç¬¦ä¸²
                              max_connections=Config.REDIS_MAX_CONCURRENT
                              )
        _redis_clients[db] = Redis(connection_pool=pool)
        _redis_pools[db] = pool

    return _redis_clients[db]


async def shutdown_redis():
    global _redis_clients, _redis_pools
    for key, _client in _redis_clients.items():
        if _client:
            await _client.aclose()
        _redis_clients[key] = None
    for key, _pool in _redis_pools.items():
        if _pool:
            await _pool.disconnect()
            _redis_pools[key] = None


async def check_redis_connection(redis: Redis):
    try:
        await redis.ping()
        print("âœ… Redis connected.")
        return True
    except ConnectionError as e:
        print(f"âŒ Redis connection failed: {e}")
    return False


async def get_redis_connection():
    redis = get_redis()
    if not await check_redis_connection(redis):
        return None
    return redis


async def get_redis_retry(redis: Redis, key: str, retry: int = 3, delay: float = 0.1):
    for attempt in range(retry):
        try:
            return await redis.get(key)
        except Exception as e:
            print(f"[Redis GET] attempt {attempt + 1} failed: {e}")
            await asyncio.sleep(delay)
    raise Exception(f"Redis GET failed after {retry} retries.")


async def get_redis_value(redis: Redis, key: str) -> Union[dict, set, list, str, int, float, None]:
    """
    Redis å€¼è·å–æ–¹æ³•ï¼Œæ”¯æŒé€šé…ç¬¦æŸ¥è¯¢å’Œ JSON è§£æ

    Args:
        key: è¦æŸ¥è¯¢çš„é”®åï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰
        redis: Redis å®¢æˆ·ç«¯å®ä¾‹

    Returns:
        æ ¹æ®å†…å®¹è¿”å›è§£æåçš„æ•°æ®
    """

    def parse_json(value: str) -> Any:
        if not isinstance(value, str):
            return value
        try:
            return json.loads(value)
        except (json.JSONDecodeError, TypeError):
            return value

    try:
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é€šé…ç¬¦
        if any(x in key for x in ("*", "?", "[")):
            keys = await redis.keys(key)
            if not keys:
                return None

            values = await redis.mget(*keys)
            result = {}
            for k, v in zip(keys, values):
                # é”®åè§£ç 
                key_str = k.decode("utf-8") if isinstance(k, bytes) else k
                if v is None:
                    result[key_str] = None
                else:  # å€¼å¤„ç†å’Œ JSON è§£æå°è¯•
                    result[key_str] = parse_json(v.decode("utf-8") if isinstance(v, bytes) else v)

            return result

        else:  # å•ä¸ªé”®æŸ¥è¯¢
            t = await redis.type(key)
            if t == "none":
                return None
            if t == "string":
                value = await redis.get(key)
                if value is None:
                    return None
                if isinstance(value, bytes):
                    value = value.decode('utf-8')
                return parse_json(value)
            if t == "hash":
                value = await redis.hgetall(key)
                return {k: parse_json(v) for k, v in value.items()}
            if t == "set":
                return await redis.smembers(key)
            if t == "list":
                items = await redis.lrange(key, 0, -1)
                return [parse_json(i) for i in items]
            print(key, t)
            # å…¶ä»–ç±»å‹ï¼Œå¦‚ zset, stream
            return None

    except (ConnectionError, TimeoutError) as e:
        # Connect call failed,redis.exceptions.ConnectionError
        raise Exception(f"Redis è¿æ¥å¤±è´¥,detail:{e}")
    except Exception as e:
        raise Exception(f"Redis æŸ¥è¯¢é”™è¯¯,detail:{e}")


async def scan_from_redis(redis: Redis, key: str = "funcmeta", batch_count: int = 0) -> list[dict]:
    """
    ä» Redis ä¸­è·å–åŒ¹é…çš„æ‰€æœ‰å…ƒæ•°æ®è®°å½•ï¼Œæ”¯æŒ scan æˆ– keys æ–¹å¼ã€‚

    Args:
        redis: Redis å®ä¾‹ã€‚
        key: Redis key å‰ç¼€ï¼ˆå¦‚ "funcmeta"ï¼‰ã€‚
        batch_count: æ¯æ‰¹ scan çš„æ•°é‡ï¼ˆå¤§äº 0 ä½¿ç”¨ scanï¼Œå¦åˆ™ç”¨ keysï¼‰ã€‚

    Returns:
        List[dict]: åŒ¹é…åˆ°çš„ JSON æ•°æ®åˆ—è¡¨ã€‚
    """
    match_pattern = f"{key}:*"

    data = []
    if batch_count > 0:
        cursor = b'0'
        while cursor:
            cursor, keys = await redis.scan(cursor=cursor, match=match_pattern, count=batch_count)
            if keys:
                values = await redis.mget(*keys)
                data.extend(json.loads(v) for v in values if v)
    else:
        keys = await redis.keys(match_pattern)
        if keys:
            cached_values = await redis.mget(*keys)
            data = [json.loads(v) for v in cached_values if v]  # set(cached_values
    return data


async def stream_to_redis(redis: Redis, batch: list, key: str = 'streams'):
    pipe = redis.pipeline()
    for stream_id, chunk in batch:
        # if not isinstance(chunk, dict):
        #     chunk = {"data": json.dumps(chunk, ensure_ascii=False)}
        stream_name = f"{key}:{stream_id % 3}"  # é€‰æ‹©åˆ†ç‰‡æµå
        await pipe.xadd(stream_name, fields=chunk, id="*", maxlen=10000, approximate=True)
    try:
        results = await pipe.execute()
        return len(results)
    except Exception as e:
        print(f"[Redis Stream Error] key={key}, batch_size={len(batch)}, error={e}")
        raise


async def sadd_to_redis(redis: Redis, key: str, values: list | set | tuple | str, ex: int = 3600) -> int:
    if not values:
        return 0
    if isinstance(values, str):
        values = [values]
    pipe = redis.pipeline()
    await pipe.sadd(key, *values)
    if ex > 0:
        await pipe.expire(key, ex)  # æ¯æ¬¡æ·»åŠ éƒ½é‡ç½®TTL
    results = await pipe.execute()
    return results[0] if results else 0  # r.smembers,scard/sismember


async def run_with_lock(func_call: Callable, *args, lock_timeout: int = 600, lock_key: str = None, redis=None,
                        **kwargs):
    redis = redis or get_redis()
    if not redis:
        return await func_call(*args, **kwargs)
    func_name = getattr(func_call, "__qualname__", getattr(func_call, "__name__", repr(func_call)))
    lock_key = lock_key or f'lock:{func_name}'
    lock_value = str(uuid.uuid4())  # str(time.time())ï¼Œæ¯ä¸ªworkerä½¿ç”¨å”¯ä¸€çš„lock_value
    lock_acquired = await redis.set(lock_key, lock_value, nx=True, ex=lock_timeout)
    if not lock_acquired:
        logger.info(f"âš ï¸ åˆ†å¸ƒå¼é”å·²è¢«å ç”¨ï¼Œè·³è¿‡ä»»åŠ¡: {func_name}")
        return None

    result = None
    try:
        logger.info(f"ğŸ”’ è·å–é”æˆåŠŸï¼Œå¼€å§‹æ‰§è¡Œä»»åŠ¡: {func_name}")
        result = await func_call(*args, **kwargs)
    except Exception as e:
        logger.error(f"âš ï¸ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {func_name} -> {e}")
    finally:
        # current_lock_value = await redis.get(redis_key)
        # if current_lock_value and current_lock_value == lock_value:
        #     await redis.delete(redis_key)
        # ä½¿ç”¨ Lua è„šæœ¬ä¿è¯åŸå­æ€§ï¼Œç¡®ä¿åªæœ‰é”æŒæœ‰è€…èƒ½é‡Šæ”¾ï¼Œåªæœ‰æœ€åˆè·å–é”çš„é‚£ä¸ªworkeræ‰èƒ½æˆåŠŸåˆ é™¤é”
        lua_script = """
           if redis.call("get", KEYS[1]) == ARGV[1] then
               return redis.call("del", KEYS[1])
           else
               return 0
           end
           """
        await redis.eval(lua_script, 1, lock_key, lock_value)

    return result


def distributed_lock(lock_timeout: int = 600, redis_key: Optional[str] = None):
    '''
    locked_operation = distributed_lock(lock_timeout=300)(my_task)    æ‰‹åŠ¨åº”ç”¨è£…é¥°å™¨,ä¸´æ—¶éœ€è¦åŠ é”çš„å‡½æ•°
    await locked_operation(123, {"name": "John"})
    @distributed_lock(300) é•¿æœŸä½¿ç”¨çš„ä»»åŠ¡å‡½æ•°
    :param lock_timeout:
    :param redis_key:
    :return:
    '''

    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            redis = get_redis()
            if not redis:
                return await func(*args, **kwargs)

            lock_key = redis_key or f"lock:{func.__qualname__}"

            # å°è¯•è·å–é”
            async with with_distributed_lock(lock_key, None, lock_timeout * 1000, redis) as lock_acquired:
                if not lock_acquired:
                    logger.info(f"âš ï¸ åˆ†å¸ƒå¼é”å·²è¢«å ç”¨ï¼Œè·³è¿‡ä»»åŠ¡: {func.__qualname__}")
                    return None

                logger.info(f"ğŸ”’ è·å–é”æˆåŠŸï¼Œå¼€å§‹æ‰§è¡Œä»»åŠ¡: {func.__qualname__}")
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    logger.error(f"âš ï¸ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {func.__qualname__} -> {e}")
                    raise

        return wrapper

    return decorator


@asynccontextmanager
async def with_distributed_lock(lock_key: str, lock_value: str = None, lock_timeout=10000, redis=None,
                                release: bool = True):
    """
    åˆ†å¸ƒå¼é”ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    lock_timeout # æ¯«ç§’
    ç”¨æ³•ï¼š
    async with with_distributed_lock( "my_lock",None,10000,redis_conn) as lock_acquired:
        if lock_acquired:
            # æ‰§è¡Œå—ä¿æŠ¤çš„æ“ä½œ
    """
    redis_conn = redis or get_redis()
    lock_identifier = lock_value or str(uuid.uuid4())
    acquired = await redis_conn.set(lock_key, lock_identifier, nx=True, px=lock_timeout)

    try:
        yield acquired
    finally:
        if acquired and release:
            # åŸå­æ€§é‡Šæ”¾é”
            script = """
            if redis.call("get", KEYS[1]) == ARGV[1] then
                return redis.call("del", KEYS[1])
            else
                return 0
            end
            """
            await redis_conn.eval(script, 1, lock_key, lock_identifier)


async def is_main_worker(worker_id: str = None, redis=None):
    async with with_distributed_lock("lock:main_worker", worker_id, lock_timeout=60 * 1000, redis=redis,
                                     release=False) as acquired:
        return acquired


def get_dask_client(cluster=None, n_workers: int = 3):
    global _dask_client, _dask_cluster
    if _dask_client:
        return _dask_client

    if cluster is None:
        if not _dask_cluster:
            if is_port_open("127.0.0.1", 8786):
                _dask_cluster = "tcp://127.0.0.1:8786"
                print("Dask Scheduler ç«¯å£è¢«å ç”¨ï¼Œè¿æ¥å·²æœ‰é›†ç¾¤")
            else:
                # å¯åŠ¨æœ¬åœ° Dask é›†ç¾¤,æœ¬æœºä¸Šå¯åŠ¨è‹¥å¹²ä¸ª worker è¿›ç¨‹,ä½¿ç”¨çº¿ç¨‹è€Œä¸æ˜¯è¿›ç¨‹ï¼ˆå’Œä¸€ä¸ª scheduler) http://127.0.0.1:8787
                _dask_cluster = LocalCluster(scheduler_port=8786, dashboard_address=":8787",
                                             n_workers=n_workers, threads_per_worker=1, processes=True)

        cluster = _dask_cluster

    try:
        _dask_client = DaskClient(cluster, timeout=3)  # åˆ›å»ºDaskå®¢æˆ·ç«¯, compression=None
        print(_dask_client.ncores())  # _dask_client.get_versions(check=True)
    except Exception as e:
        print(f"âŒ æ— æ³•åˆ›å»º Dask Client: {e}")
        # raise RuntimeError(f"âŒ æ— æ³•åˆ›å»º Dask Client: {e}")

    return _dask_client


def close_dask_client():
    global _dask_client, _dask_cluster
    # print("Closing Dask client or cluster...")
    if _dask_client:
        _dask_client.close()
        _dask_client = None
    if _dask_cluster:
        if isinstance(_dask_cluster, LocalCluster):
            _dask_cluster.close()
        _dask_cluster = None


def get_neo_driver():
    global _graph_driver
    if _graph_driver is None:
        _graph_driver = AsyncGraphDatabase.driver(uri=Config.NEO_URI,  # uri="bolt://localhost:7687"
                                                  auth=(Config.NEO_Username, Config.NEO_Password),
                                                  max_connection_lifetime=3600,  # å•è¿æ¥ç”Ÿå‘½å‘¨æœŸ
                                                  max_connection_pool_size=30,  # æœ€å¤§è¿æ¥æ± æ•°é‡
                                                  connection_timeout=30  # è¶…æ—¶
                                                  )
    return _graph_driver


def get_w3():
    try:
        from web3 import Web3

        w3 = Web3(
            Web3.HTTPProvider(f'https://mainnet.infura.io/v3/{Config.INFURA_PROJECT_ID}'))  # ("http://127.0.0.1:8545")
        return w3
    except ImportError:
        print("[Web3 Init] Web3 not installed.")
    except Exception as e:
        print(f"[Web3 Init] Failed to get web3: {e}")

    return None


def upload_file_to_oss(bucket, file_obj, object_name=None, expires: int = 604800, total_size: int = 0):
    """
      ä¸Šä¼ æ–‡ä»¶åˆ° OSS æ”¯æŒ `io` å¯¹è±¡ã€‚
      :param bucket: OSS bucket å®ä¾‹
      :param file_obj: æ–‡ä»¶å¯¹è±¡ï¼Œå¯ä»¥æ˜¯ `io.BytesIO` æˆ– `io.BufferedReader`
      :param object_name: OSS ä¸­çš„å¯¹è±¡å
      :param expires: ç­¾åæœ‰æ•ˆæœŸï¼Œé»˜è®¤ä¸€å‘¨ï¼ˆç§’ï¼‰
      :param total_size
    """
    if isinstance(file_obj, bytes):
        file_obj = io.BytesIO(file_obj)
    if not total_size:
        file_obj.seek(0, os.SEEK_END)
        total_size = file_obj.tell()  # os.path.getsize(file_path)
        file_obj.seek(0)
    if not object_name:
        if not hasattr(file_obj, "name"):
            file_obj.name = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.bin"
        object_name = f"upload/{file_obj.name}"

    if total_size > 1024 * 1024 * 16:
        part_size = oss2.determine_part_size(total_size, preferred_size=128 * 1024)
        upload_id = bucket.init_multipart_upload(object_name).upload_id
        parts = []
        part_number = 1
        offset = 0
        while offset < total_size:
            size_to_upload = min(part_size, total_size - offset)
            result = bucket.upload_part(object_name, upload_id, part_number,
                                        oss2.SizedFileAdapter(file_obj, size_to_upload))
            parts.append(oss2.models.PartInfo(part_number, result.etag, size=size_to_upload, part_crc=result.crc))
            offset += size_to_upload
            part_number += 1

        # å®Œæˆåˆ†ç‰‡ä¸Šä¼ 
        bucket.complete_multipart_upload(object_name, upload_id, parts)
    else:
        # OSS ä¸Šçš„å­˜å‚¨è·¯å¾„, æœ¬åœ°å›¾ç‰‡è·¯å¾„
        bucket.put_object(object_name, file_obj)
        # bucket.put_object_from_file(object_name, str(file_path))

    if 0 < expires <= 604800:  # å¦‚æœç­¾åsigned_URL
        url = bucket.sign_url("GET", object_name, expires=expires)
    else:  # ä½¿ç”¨åŠ é€ŸåŸŸå
        url = f"{Config.ALIYUN_Bucket_Domain}/{object_name}"
        # bucket.bucket_name
    # è·å–æ–‡ä»¶å¯¹è±¡
    # result = bucket.get_object(object_name)
    # result.read()è·å–æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹,result.headerså…ƒæ•°æ®ï¼ˆå¤´éƒ¨ä¿¡æ¯ï¼‰
    return url, object_name


# è·å–æ–‡ä»¶åˆ—è¡¨
def oss_list_files(bucket, prefix='upload/', max_keys: int = 100, max_pages: int = 1):
    """
    åˆ—å‡º OSS ä¸­çš„æ–‡ä»¶ã€‚
    :param bucket: oss2.Bucket å®ä¾‹
    :param prefix: æ–‡ä»¶åå‰ç¼€ï¼Œç”¨äºç­›é€‰
    :param max_keys: æ¯æ¬¡è¿”å›çš„æœ€å¤§æ•°é‡
    :param max_pages:
    :return: æ–‡ä»¶ååˆ—è¡¨
    """
    file_list = []
    if max_pages <= 1:
        for obj in oss2.ObjectIterator(bucket, prefix=prefix, max_keys=max_keys):
            file_list.append(obj.key)
    else:
        i = 0
        next_marker = ''
        while i < max_pages:
            result = bucket.list_objects(prefix=prefix, max_keys=max_keys, marker=next_marker)
            for obj in result.object_list:
                file_list.append(obj.key)
            if not result.is_truncated:  # å¦‚æœæ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œé€€å‡ºå¾ªç¯
                break
            next_marker = result.next_marker
            i += 1

    return file_list


class AsyncBatchAdd:
    """
    é€šç”¨çš„å¼‚æ­¥æ‰¹é‡å¤„ç†å™¨ï¼Œå¯ç”¨äºä»»ä½• SQLAlchemy ORM ç±»
    """

    def __init__(
            self,
            model_class: Type,
            batch_size: int = 100,
            batch_timeout: float = 3.0,
            queue_maxsize: int = 10000,
            get_session_func: Optional[Callable] = None
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨

        Args:
            model_class: SQLAlchemy ORM ç±»
            batch_size: æ¯æ‰¹å¤„ç†çš„è®°å½•æ•°é‡
            batch_timeout: æ‰¹å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            get_session_func: è·å–æ•°æ®åº“ä¼šè¯çš„å‡½æ•°,AsyncSessionLocal
        """
        self.model_class = model_class
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.get_session_func = get_session_func  # coro

        self._queue = asyncio.Queue(maxsize=queue_maxsize)
        self._task = None
        self._is_running = False
        self._last_insert_time = None

    @property
    def insert_time(self):
        return self._last_insert_time

    async def initialize(self):
        """åˆå§‹åŒ–å¤„ç†å™¨ï¼Œå¯åŠ¨åå°ä»»åŠ¡"""
        if not self._is_running:
            self._is_running = True
            self._task = asyncio.create_task(self._worker())
            logger.info(f"Initialized batch processor for {self.model_class.__name__}")

    async def shutdown(self):
        """å…³é—­å¤„ç†å™¨ï¼Œåœæ­¢åå°ä»»åŠ¡å¹¶å¤„ç†å‰©ä½™æ•°æ®"""
        if self._is_running:
            self._is_running = False
            if self._task:
                self._task.cancel()
                try:
                    await self._task
                except asyncio.CancelledError:
                    pass
                self._task = None
            logger.info(f"Shutdown batch processor for {self.model_class.__name__}")

    async def enqueue(self, data: Dict[str, Any]):
        """å°†æ•°æ®æ”¾å…¥é˜Ÿåˆ—ï¼ˆéé˜»å¡ï¼‰"""
        if not self._is_running:
            await self.initialize()
        await self._queue.put(data)

    def put_many_nowait(self, data_list: List[Dict]) -> int:
        """å°†å¤šæ¡æ•°æ®æ”¾å…¥é˜Ÿåˆ—ï¼ˆéé˜»å¡ï¼‰"""
        count = 0
        for data in data_list:
            try:
                self._queue.put_nowait(data)
                count += 1
            except asyncio.QueueFull:
                break
        return count

    async def _worker(self):
        """åå°æ‰¹é‡æ’å…¥å·¥ä½œå™¨"""
        batch = []
        self._last_insert_time = time.time()
        while self._is_running:
            try:
                # ç­‰å¾…æ–°æ¶ˆæ¯æˆ–è¶…æ—¶
                try:
                    item = await asyncio.wait_for(self._queue.get(), timeout=self.batch_timeout)
                    batch.append(item)
                    self._queue.task_done()
                except asyncio.TimeoutError:
                    pass  # è¶…æ—¶ï¼Œç»§ç»­å¤„ç†å½“å‰æ‰¹æ¬¡

                current_time = time.time()
                # å¦‚æœæ‰¹æ¬¡è¾¾åˆ°æŒ‡å®šå¤§å°æˆ–è¶…æ—¶ï¼Œæ‰§è¡Œæ’å…¥
                if (len(batch) >= self.batch_size or
                        (batch and current_time - self._last_insert_time >= self.batch_timeout)):
                    await self.process_batch(batch)
                    batch.clear()
                    self._last_insert_time = current_time

            except asyncio.CancelledError:
                if batch:  # ä»»åŠ¡è¢«å–æ¶ˆï¼Œå¤„ç†å‰©ä½™æ‰¹æ¬¡
                    await self.process_batch(batch)
                break
            except Exception as e:
                logger.error(f"Unexpected error in batch insert worker: {e}")

    async def process_batch(self, batch: List[Dict[str, Any]]):
        """å¤„ç†ä¸€æ‰¹æ•°æ®ï¼Œæ’å…¥åˆ°æ•°æ®åº“"""
        if not batch:
            return

        if not self.get_session_func:
            logger.error("No session function provided for batch processor")
            return

        try:
            async with self.get_session_func() as session:
                session.add_all([self.model_class(**data) for data in batch])
                await session.commit()
                logger.info(f"Successfully inserted {len(batch)} records for {self.model_class.__name__}")
        except Exception as e:
            logger.error(f"Error inserting batch for {self.model_class.__name__}: {e}")
            # await session.rollback()
            # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ é‡è¯•é€»è¾‘æˆ–é”™è¯¯å¤„ç†

    async def process_one(self, data):
        """ç›´æ¥æ’å…¥ï¼ˆä¸ä½¿ç”¨é˜Ÿåˆ—ï¼‰"""
        async with self.get_session_func() as session:
            try:
                session.add(self.model_class(**data))
                await session.commit()
                return True
            except Exception as e:
                await session.rollback()
                logger.error(f"Error insert for {self.model_class.__name__}: {e}")
        return False

    async def execute_batch(self, stmt, values_list: List[tuple | dict]):
        """
           æ‰¹é‡æ’å…¥æˆ–æ›´æ–°å†å²è®°å½•ï¼ˆæ”¯æŒ executemanyï¼‰ï¼Œä½¿ç”¨ MySQL çš„ ON DUPLICATE KEY UPDATEã€‚
        """
        total = 0
        try:
            async with self.get_session_func() as session:
                for chunk in chunks_iterable(values_list, self.batch_size):
                    await session.execute(stmt, chunk)
                    total += len(chunk)
                await session.commit()
            logger.info(f"Inserted/Updated {total} records")
            return total

        except Exception as e:
            logger.error(f"Error during history upsert: {e}\n{stmt}")
        return total

    @asynccontextmanager
    async def context(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºå®‰å…¨åœ°åˆå§‹åŒ–å’Œå…³é—­å¤„ç†å™¨"""
        await self.initialize()
        try:
            yield self
        finally:
            await self.shutdown()


async def send_to_wechat(user_name: str, context: str = None, link: str = None, object_name: str = None):
    url = f"{Config.WECHAT_URL}/sendToChat"
    headers = {'accept': 'application/json', 'Content-Type': 'application/json'}
    body = {'user': user_name, 'context': context, 'url': link,
            'object_name': object_name, 'file_type': get_file_type_wx(object_name)}

    try:
        cx = get_httpx_client(time_out=Config.HTTP_TIMEOUT_SEC)
        response = await cx.post(url, json=body, headers=headers)
        response.raise_for_status()
        return response.json()

    except Exception as e:
        logger.error(f'send_to_wechat{body}')
        logger.error(f"Error occurred while sending message: {e}")
        # with httpx.Client(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        #     response = cx.post(url, json=body, headers=headers)
        #     response.raise_for_status()
        # return response.json()

    return None


@async_error_logger(1)
async def get_data_for_model(model: dict):
    """è·å–æ¯ä¸ªæ¨¡å‹çš„æ•°æ®"""
    model_name = model.get('name')
    client = AI_Client.get(model_name)

    if client:
        try:
            models = await client.models.list()
            return [m.model_dump() for m in models.data]
        except Exception as e:
            print(f"OpenAI error occurred:{e},name:{model_name}")
    else:
        url = model.get('model_url') or model['base_url'] + '/models'
        headers = {}
        api_key = model.get('api_key')
        if api_key:
            headers["Authorization"] = f'Bearer {api_key}'
            if model['type'] == 'anthropic':
                headers = {"x-api-key": api_key, "anthropic-version": "2023-06-01"}

        cx = get_httpx_client(proxy=Config.HTTP_Proxy if model.get('proxy') else None)
        response = await cx.get(url, headers=headers, timeout=model.get('timeout', Config.LLM_TIMEOUT_SEC))
        response.raise_for_status()
        models = response.json()
        if models:
            return models.get('data')

    return None


class ModelList:
    ai_models: list[dict] = []
    models = []
    owners = []
    _redis = None
    _worker_id: str = None
    _list_key = "model_list"
    _data_key = "model_data_list"
    _hash_key = "model_data_hash"

    @classmethod
    def extract(cls) -> tuple[list, list]:
        """
        æå– AI_Models ä¸­çš„ name ä»¥åŠ search_field ä¸­çš„æ‰€æœ‰å€¼ï¼Œå¹¶å­˜å…¥ä¸€ä¸ªå¤§åˆ—è¡¨ã€‚

        è¿”å›ï¼š
        - List[str]: åŒ…å«æ‰€æœ‰æ¨¡å‹åç§°åŠå…¶å­æ¨¡å‹çš„åˆ—è¡¨
        """
        extracted_data = extract_ai_model("model", cls.ai_models)
        owners = [item[0] for item in extracted_data]
        flattened_list = [i for item in extracted_data for i in [item[0]] + item[1]]
        # duplicates = {item: count for item, count in Counter(flattened_list).items() if count > 1}
        # print("æ¨¡å‹æ•°é‡:", len(flattened_list), "é‡å¤æ¨¡å‹:", duplicates)
        # list(itertools.chain(*[sublist[1] for sublist in extracted_data])) #å»é‡å¹¶ä¿æŒé¡ºåº
        return list(dict.fromkeys(flattened_list)), owners

    @classmethod
    async def set(cls, redis=None, worker_id: str = None, ai_models: list = AI_Models):
        """æ›´æ–° MODEL_LIST,å¹¶ä¿å­˜åˆ° Redis"""
        cls.ai_models = ai_models or AI_Models
        cls.models, cls.owners = cls.extract()
        if cls._redis is None:
            cls._redis = redis or get_redis()
        if worker_id:
            cls._worker_id = worker_id

        await cls.to_redis(cls._list_key, cls.models)

        await cls.set_datas()

    @classmethod
    async def to_redis(cls, key: str, value, **kwargs):
        """
        åˆ†å¸ƒå¼å†™å…¥ï¼šè°å…ˆæŠ¢åˆ°é”è°å†™å…¥ï¼Œå¤±è´¥è‡ªåŠ¨é‡Šæ”¾é”ï¼ŒæˆåŠŸä¸é‡Šæ”¾
        kwargs: ä¼ é€’ç»™ redis.set çš„å…¶ä»–å‚æ•°ï¼Œå¦‚ ex/px/nx/xx ç­‰
        """
        if not cls._redis:
            return False

        lock_key = f"lock:{key}"
        async with with_distributed_lock(lock_key, cls._worker_id, 60000, redis=cls._redis, release=False) as acquired:
            if not acquired:  # é”å·²è¢«å ç”¨ï¼Œç›´æ¥è¿”å›å¤±è´¥
                return False

            try:
                await cls._redis.set(key, json.dumps(value, ensure_ascii=False), **kwargs)
                return True
            except Exception as e:
                logger.error(f"[Redis SET Error] key={key}, error={e}")
                if cls._worker_id:  # æœ‰æš—å·ä¸é‡Šæ”¾ï¼Œå†™å…¥å¤±è´¥æ‰é‡Šæ”¾é”
                    current_lock_value = await cls._redis.get(lock_key)
                    if current_lock_value and current_lock_value == cls._worker_id:
                        await cls._redis.delete(lock_key)

        # await run_with_lock(cls._redis.set, cls._list_key, json.dumps(cls.models, ensure_ascii=False),
        #                     lock_key=f"lock:{cls._list_key}", lock_timeout=60)
        return False

    @classmethod
    async def get(cls, updated=True):
        if cls._redis:
            data = await cls._redis.get(cls._list_key)
            if data:
                return json.loads(data)
        if not cls.models and updated:
            await cls.set()
            print("model_list updated:", cls.models)

        return cls.models

    @classmethod
    def contains(cls, value):
        models = cls.models or async_to_sync(cls.get)
        if ':' in value:
            owner, name = value.split(':')
            return owner in models or name in models

        return value in models

    @classmethod
    async def set_model_data(cls, model: dict, hash_data: dict = None) -> tuple:
        name = model.get('name')
        key = f"{cls._data_key}:{name}"
        hash_key = generate_hash_key(model.get('model', []))
        old_hash_key = (hash_data or {}).get(name)

        if old_hash_key == hash_key:  # å¦‚æœæ—§çš„ hash ç›¸åŒï¼Œåˆ™æ— éœ€æ›´æ–°
            data_raw = await cls._redis.get(key) if cls._redis else None
            if data_raw:
                model['data'] = json.loads(data_raw)
                if not model.get('model'):
                    model['model'] = [d['id'] for d in model['data']]
                return name, hash_key

        data = await get_data_for_model(model)  # å¦åˆ™é‡æ–°æ‹‰å–æ•°æ®å¹¶ç¼“å­˜
        if data:
            model['data'] = data
            if not model.get('model'):
                model['model'] = [d['id'] for d in data]
            await cls.to_redis(key, data)
            print('model:', model.get('name'), 'data:', data)
        return name, hash_key

    @classmethod
    async def set_datas(cls):
        # key_type = await cls._redis.type(cls._hash_key)
        # if key_type != "hash":
        #     await cls._redis.delete(cls._hash_key)
        hash_data = await cls._redis.hgetall(cls._hash_key) if cls._redis else {}
        tasks = [cls.set_model_data(model, hash_data) for model in cls.ai_models
                 if model.get('supported_list') and model.get('api_key')]

        results = await asyncio.gather(*tasks, return_exceptions=True)
        pipe = cls._redis.pipeline()
        for r in results:  # è¿‡æ»¤å‡ºæˆåŠŸçš„ç»“æœï¼Œå¹¶æ›´æ–° hash_data
            if isinstance(r, Exception):
                print(f"[set_model_datas] error {r}")
                continue
            name, hash_key = r
            if not (name and hash_key):
                continue
            if hash_data.get(name) != hash_key:
                pipe.hset(cls._hash_key, name, hash_key)
        await pipe.execute()

    @classmethod
    def save(cls, file_path='models.json'):
        with open(file_path, 'w', encoding='utf-8') as file:
            json.dump(cls.ai_models, file, ensure_ascii=False)
        print(json.dumps(cls.ai_models, indent=4))

    @staticmethod
    def extract_models(model_type: str = "model", ai_models: list = AI_Models) -> list:
        extracted_data = extract_ai_model(model_type, ai_models)
        models = [f"{owner}:{val}" if val else owner for owner, values in extracted_data for val in values]
        return list(dict.fromkeys(models))

    @staticmethod
    def get_model_data(model: str, ai_models: list = AI_Models):
        try:
            model_info, model_id = find_ai_model(model, 0, 'model', ai_models)
            model_data = next((item for item in model_info.get('data', []) if item['id'] == model_id), {})
            data = {
                "id": model_id,
                "object": "model",
                "created": 0,
                "owned_by": model_info['name'],
                "type": "chat",
                "context_length": 32768,
                "permission": [
                    {
                        "id": f"modelperm-{model_info['name']}:{model_id}",
                        "object": "model_permission",
                    }
                ],
                "supported_parameters": ["max_tokens", "stop", "temperature", "tool_choice", "tools",
                                         "top_k", "top_p"],
            }
            for k, v in model_data.items():
                if not v:  # null
                    continue
                if k == 'id':
                    continue
                if k == "owned_by":
                    if v == "system":
                        continue
                    data[k] += f'-{v}'
                data[k] = v
        except ValueError as e:
            data = {'error': str(e)}
        return data

    @staticmethod
    def get_models(ai_models: list = AI_Models):
        extracted_data = extract_ai_model("model", ai_models)
        models_data = []
        for owner, models in extracted_data:
            owner_data = next((item.get('data', []) for item in ai_models if item['name'] == owner), [])
            for i, model_id in enumerate(models):
                data = {
                    "id": f"{owner}:{model_id}",  # å”¯ä¸€æ¨¡å‹ID,  ç”¨äºæŒ‡å®šæ¨¡å‹è¿›è¡Œè¯·æ±‚ fine-tuned-model
                    "object": "model",
                    "type": "chat",
                    "created": 1740386673,
                    "owned_by": owner,  # æ‹¥æœ‰è¯¥æ¨¡å‹çš„ç»„ç»‡
                    "root": model_id,  # æ ¹ç‰ˆæœ¬ï¼Œä¸ ID ç›¸åŒ
                    "parent": None,  # å¦‚æœæ²¡æœ‰çˆ¶æ¨¡å‹ï¼Œåˆ™ä¸º None
                    "context_length": 32768,  # max_context_length: 65536,131072,163840,256000
                    # "max_model_len": 4096,#GPUå†…å­˜é™åˆ¶è€Œéœ€è¦è°ƒæ•´æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦
                    "permission": [
                        {
                            "id": f"modelperm-{owner}:{model_id}",
                            "object": "model_permission",
                        }
                    ],
                    "supported_parameters": ["max_tokens", "stop", "temperature", "tool_choice", "tools",
                                             "top_k", "top_p"],
                }  # åŸºç¡€ç»“æ„
                model_data = next((item for item in owner_data if item['id'] == model_id), {})
                for k, v in model_data.items():  # è¦†ç›–æ¨¡å‹ä¿¡æ¯
                    if k not in {"id", "owned_by"} and v:
                        data[k] = v
                models_data.append(data)

        return {"object": "list", "data": models_data}


async def init_ai_clients(ai_models: list = AI_Models) -> dict:
    limits = httpx.Limits(max_connections=max(Config.MAX_HTTP_CONNECTIONS, Config.MAX_KEEPALIVE_CONNECTIONS),
                          max_keepalive_connections=Config.MAX_KEEPALIVE_CONNECTIONS)
    transport = httpx.AsyncHTTPTransport(proxy=Config.HTTP_Proxy)
    # proxies = {"http://": Config.HTTP_Proxy, "https://": Config.HTTP_Proxy}
    # http_client = DefaultHttpxClient(proxy="http://my.test.proxy.example.com", transport=httpx.HTTPTransport(local_address="0.0.0.0"))
    for model in ai_models:
        model_name = model.get('name')
        api_key = model_api_keys(model_name)
        if api_key:
            model['api_key'] = api_key
            if model_name not in AI_Client and model.get('supported_openai'):  # model_name in SUPPORTED_OPENAI_MODELS
                http_client = None
                time_out = model.get('timeout', Config.LLM_TIMEOUT_SEC)
                if model.get('proxy'):  # proxies=proxies
                    timeout = httpx.Timeout(time_out, read=time_out, write=100.0, connect=10.0)
                    http_client = httpx.AsyncClient(transport=transport, limits=limits, timeout=timeout)

                AI_Client[model_name]: AsyncOpenAI = AsyncOpenAI(api_key=api_key, base_url=model['base_url'],
                                                                 http_client=http_client,
                                                                 max_retries=Config.MAX_RETRY_COUNT)
                if http_client is None:
                    AI_Client[model_name] = AI_Client[model_name].with_options(timeout=time_out,
                                                                               max_retries=Config.MAX_RETRY_COUNT)
    return AI_Client


def find_ai_model(name: str, model_id: int = 0, search_field: str = 'model',
                  ai_models: list = AI_Models) -> Tuple[dict, str]:
    """
    åœ¨ AI_Models ä¸­æŸ¥æ‰¾æ¨¡å‹ã€‚å¦‚æœæ‰¾åˆ°åç§°åŒ¹é…çš„æ¨¡å‹ï¼Œè¿”å›æ¨¡å‹åŠå…¶ç±»å‹æˆ–å…·ä½“çš„å­æ¨¡å‹åç§°ã€‚

    å‚æ•°:
    - name: è¦æŸ¥æ‰¾çš„æ¨¡å‹åç§°
    - model_id: å¯é€‰å‚æ•°ï¼ŒæŒ‡å®šè¿”å›çš„å­æ¨¡å‹ç´¢å¼•ï¼Œé»˜è®¤ä¸º 0
    - search_field: è¦åœ¨å…¶ä¸­æŸ¥æ‰¾åç§°çš„å­—æ®µï¼ˆé»˜è®¤ä¸º 'model'ï¼‰
     è¿”å›:
    - Tuple[Dict[str, Any], Union[str, None]]: æ¨¡å‹åŠå…¶å¯¹åº”çš„å­æ¨¡å‹åç§°ï¼ˆæˆ– Noneï¼‰

    å¼‚å¸¸:
    - ValueError: å¦‚æœæœªæ‰¾åˆ°æ¨¡å‹
    """

    if ':' in name:
        parts = name.split(':', 1)
        owner, model_name = parts[0], parts[1]
        model = next((item for item in ai_models if item['name'] == owner), None)
        if model:
            if model_name in model.get(search_field, []):
                return model, model_name
            if model_name in model.get('model_map', {}):
                return model, model['model_map'][model_name]

    model = next(
        (item for item in ai_models if item['name'] == name or name in item.get(search_field, [])),
        None
    )
    if model:
        model_items = model.get(search_field, [])

        if isinstance(model_items, (list, tuple)):
            if name in model_items:
                return model, name
            if model_items:
                model_id %= len(model_items)
                return model, model_items[model_id]
        elif isinstance(model_items, dict):
            if name in model_items:
                return model, model_items[name]
            # å¦‚æœæä¾›äº†åºå·ï¼Œè¿”å›åºå·å¯¹åº”çš„å€¼
            keys = list(model_items.keys())
            model_id = model_id if abs(model_id) < len(keys) else 0
            return model, model_items[keys[model_id]]
        elif name in model.get('model_map', {}):
            return model, model['model_map'][name]

        return model, name if model_items == name else ''

    raise ValueError(f"Model with name {name} not found.")
    # HTTPException(status_code=400, detail=f"Model with name {name} not found.")


def extract_ai_model(search_field: str = "model", ai_models: list = AI_Models):
    """
    æå– AI_Models ä¸­çš„ name ä»¥åŠ search_field ä¸­çš„æ‰€æœ‰å€¼ï¼ˆåˆ—è¡¨æˆ–å­—å…¸ keyï¼‰ã€‚

    è¿”å›ï¼š
    - List[Tuple[str, List[str]]]: æ¯ä¸ªæ¨¡å‹çš„åç§°åŠå…¶å¯¹åº”çš„æ¨¡å‹åˆ—è¡¨
    """
    extracted_data = []

    for model in ai_models:
        name = model["name"]
        field_value = model.get(search_field, [])
        if model.get('supported_openai', True) and not model.get('api_key'):
            continue

        if isinstance(field_value, list):
            extracted_data.append((name, list(dict.fromkeys(field_value))))
        elif isinstance(field_value, dict):
            extracted_data.append((name, list(field_value.keys())))
        else:
            extracted_data.append((name, [field_value]))

        model_map = model.get('model_map', {})
        if model_map:
            extracted_data.append((name, list(model_map.keys())))

    return extracted_data


async def run_mcp_task(transport: Literal["stdio", "streamable-http", "sse"] = "streamable-http", port=7007, **kwargs):
    # subprocess.Popen(["python", "-m", "mcp", "--port", "7007"])
    shutdown_event = asyncio.Event()

    async def _run():
        await mcp.run_async(transport=transport, port=port, host="127.0.0.1", path="/mcp", **kwargs)
        await shutdown_event.wait()  # ç­‰å¾…å…³é—­ä¿¡å·

    task = asyncio.create_task(_run())
    return task, shutdown_event


def create_openai_mcp(base_url="http://127.0.0.1:7000", timeout=Config.HTTP_TIMEOUT_SEC,
                      instructions: str | None = None) -> FastMCP:
    from fastmcp.server.openapi import RouteMap, MCPType
    api_client = httpx.AsyncClient(base_url=base_url, headers={"Authorization": "Bearer YOUR_TOKEN"})
    # import requests
    # resp = requests.get(f"{base_url}/openapi.json")
    # print("TEXT:", resp.text)
    # openapi_spec = resp.json()
    from utils import load_dictjson

    # openapi_spec = httpx.get(f"{base_url}/openapi.json").json()  # Load your OpenAPI spec
    openapi_spec = load_dictjson('../openapi.json', encoding='utf-8')
    print(openapi_spec)
    DEFAULT_ROUTE_MAPPINGS = [
        # custom mapping logic goes here
        # ... your specific route maps ...
        RouteMap(methods=["GET"], pattern=r".*\{.*\}.*", mcp_type=MCPType.RESOURCE_TEMPLATE),
        RouteMap(methods=["GET"], pattern=r".*", mcp_type=MCPType.RESOURCE),
        RouteMap(pattern=r"^/admin/.*", mcp_type=MCPType.EXCLUDE),
        # exclude all remaining routes
        RouteMap(mcp_type=MCPType.EXCLUDE),
    ]
    api_mcp = FastMCP.from_openapi(
        openapi_spec=openapi_spec,
        client=api_client,
        timeout=timeout,  # 30 second timeout for all requests
        route_maps=DEFAULT_ROUTE_MAPPINGS,
        instructions=instructions,
    )
    return api_mcp


# mcp.mount(openai_mcp("http://47.110.156.41:7000"), prefix="openapi")

async def call_mcp_tool(config: dict[str, Any] | str, name: str, **kwargs):
    async with MCPClient(config) as client:
        # Access tools and resources with server prefixes
        # answer = await client.call_tool("assistant_answer_question", {"query": "What is MCP?"})
        return await client.call_tool(name, **kwargs)


@mcp.custom_route("/", methods=["GET"])
async def health_check(request: Request) -> Response:
    return JSONResponse({"status": "ok"})


@mcp.resource("config://version")
def get_version():
    return Config.Version


if __name__ == "__main__":
    import nest_asyncio

    # nest_asyncio.apply()
    import threading


    # client = AI_Client['deepseek']
    # print(dir(client.chat.completions))# 'create', 'with_raw_response', 'with_streaming_response'
    # print(dir(client.completions))
    # print(dir(client.embeddings))
    # print(dir(client.files)) #'content', 'create', 'delete', 'list', 'retrieve', 'retrieve_content', 'wait_for_processing'

    # mcp.run(transport="sse", log_level="debug")
    # from fastmcp.server.proxy import FastMCPProxy
    # mcp.mount(FastMCPProxy("http://other-host:8001/mcp"), prefix="remote")

    # mcp.run(transport="streamable-http", host="127.0.0.1", port=8000, path="/mcp")
    # redis_client = Redis(host='47.110.156.41', port=7007, db=0, decode_responses=True)
    # import uvicorn
    # uvicorn.run(mcp, host="0.0.0.0", port=7007)

    async def main():
        # è¿›ç¨‹é—´é€šä¿¡ï¼Œé€‚ç”¨äºå‘½ä»¤è¡Œæˆ–è„šæœ¬å·¥å…·æ‰§è¡Œ
        # await asyncio.to_thread(mcp.run, transport="stdio", **kwargs)
        async with MCPClient("http://127.0.0.1:7000/mcp/mcp") as client:
            # ... use the client
            tools = await client.list_tools()
            print(f"Available tools: {tools}")
            result = await client.call_tool("add", {"a": 5, "b": 3})
            print(f"Result: {result}")
            resources = await client.list_resources()
            # Read a resource from the server
            data = await client.read_resource(resources[0].uri)
            print(f"Result: {data[0].text}")

        # map_task = await run_mcp_task("stdio")
        mcp_task, exit_event = await run_mcp_task(port=7007)
        # kk = await redis_client.get('dd')
        # print(kk)
        # async with MCPClient("utils.py") as client:
        #     tools = await client.list_tools()
        #     print(f"Available tools: {tools}")
        #     result = await client.call_tool("add", {"a": 5, "b": 3})
        #     print(f"Result: {result.text}")

        # Connect via in-memory transport
        async with MCPClient(mcp) as client:
            tools = await client.list_tools()
            print(f"Available tools: {tools}")
            result = await client.call_tool("add", {"a": 5, "b": 3})
            print(f"Result: {result}")
            resources = await client.list_resources()
            results = await client.read_resource(resources[0].uri)
            print(f"Result: {results[0].text}")

        # Connect via SSE
        # async with MCPClient("http://localhost:8000/sse") as client:
        #     # ... use the client
        #     tools = await client.list_tools()
        #     print(f"Available tools: {tools}")

        # å‘å‡ºé€€å‡ºä¿¡å·å¹¶å–æ¶ˆä»»åŠ¡
        exit_event.set()
        # mcp_task.cancel()
        # await mcp_task await asyncio.shield(mcp_task)
        try:
            await mcp_task
        except asyncio.CancelledError:
            print("åå°ä»»åŠ¡å·²å–æ¶ˆ")


    Config.load('../config.yaml')
    aliyun_bucket = oss2.Bucket(oss2.Auth(Config.ALIYUN_oss_AK_ID, Config.ALIYUN_oss_Secret_Key),
                                Config.ALIYUN_oss_endpoint,
                                Config.ALIYUN_Bucket_Name)
    files = oss_list_files(aliyun_bucket, prefix='upload/', max_keys=100, max_pages=1)
    print(files)
    Config.debug()


    async def test_r():
        redis = get_redis()
        result = await get_redis_value(redis, 'model_data_list:zzz')  # tokenflux,aihubmix
        print([item.get('id') for item in result])
        await shutdown_redis()

    # asyncio.run(main())

    # asyncio.run(test_r())
