import igraph as ig
import json, time, random
import asyncio
from redis.asyncio import Redis, StrictRedis, ConnectionPool
from typing import Dict, List, Tuple, Union, Iterable, Callable, Optional, Any
from enum import IntEnum, Enum
from collections import defaultdict
from dataclasses import asdict, dataclass, is_dataclass, field
from threading import Thread
import uuid
import zmq, zmq.asyncio
from neo4j import GraphDatabase, AsyncGraphDatabase

from config import Config
from structs import dataclass2dict

# Config.debug()

_graph_driver: Optional[GraphDatabase] = None
# _graph_driver_lock = asyncio.Lock()  # é˜²æ­¢å¹¶å‘åˆå§‹åŒ–
_redis_client: Optional[Redis] = None  # StrictRedis(host='localhost', port=6379, db=0)
_redis_pool: Optional[ConnectionPool] = None


def get_redis() -> Optional[Redis]:
    global _redis_client, _redis_pool
    if _redis_client is None:
        _redis_pool = ConnectionPool(host=Config.REDIS_HOST, port=Config.REDIS_PORT, db=0,
                                     decode_responses=True,  # è‡ªåŠ¨è§£ç ä¸ºå­—ç¬¦ä¸²
                                     max_connections=50
                                     # connection_pool=pool
                                     )
        _redis_client = Redis(connection_pool=_redis_pool)

    return _redis_client


async def shutdown_redis():
    global _redis_client, _redis_pool
    if _redis_client:
        await _redis_client.close()
        _redis_client = None

    if _redis_pool:
        await _redis_pool.disconnect()
        _redis_pool = None


async def check_redis_connection(redis):
    try:
        await redis.ping()
        print("Redis connected.")
        return True
    except ConnectionError as e:
        print(f"âŒ Redis connection failed: {e}")
    return False


async def get_redis_connection():
    redis = get_redis()
    if not await check_redis_connection(redis):
        return None
    return redis


def get_neo_driver():
    global _graph_driver
    if _graph_driver is None:
        _graph_driver = AsyncGraphDatabase.driver(uri=Config.NEO_URI, auth=(Config.NEO_Username, Config.NEO_Password),
                                                  max_connection_lifetime=3600,  # å•è¿æ¥ç”Ÿå‘½å‘¨æœŸ
                                                  max_connection_pool_size=50,  # æœ€å¤§è¿æ¥æ± æ•°é‡
                                                  connection_timeout=30  # è¶…æ—¶
                                                  )
    return _graph_driver


async def get_neo_nodes(cypher):
    driver = get_neo_driver()
    async with driver.session() as session:
        result = await session.run(cypher)  # "CREATE (n:Person {name: 'Alice'})"
        return [record async for record in result]


async def merge_neo_node(tx, node_id: str, props: dict, label: str = 'Node', id_field: str = "id"):
    """
    èŠ‚ç‚¹ MERGE å’Œå±æ€§æ›´æ–°å‡½æ•°
    Args:
        tx: Neo4j äº‹åŠ¡
        node_id: ä¸»é”®å€¼
        props: å…¶ä»–å±æ€§å­—å…¸
        label: èŠ‚ç‚¹æ ‡ç­¾ï¼ˆGraphQLé‡Œçš„ Node ç±»å‹ï¼‰
        id_field: å±æ€§å­—æ®µç”¨äºè¯†åˆ«èŠ‚ç‚¹ï¼Œé»˜è®¤ä¸º id
    """

    def serialize(v):
        if isinstance(v, (dict, list, tuple)):
            return json.dumps(v)
        return v

    safe_props = {k: serialize(v) for k, v in props.items()}
    props_str = ", ".join(f"{k}: ${k}" for k in safe_props)
    query = (
        f"MERGE (n:{label} {{{id_field}: $node_id}}) "
        f"SET n += {{{props_str}}}"
    )
    await tx.run(query, node_id=node_id, **safe_props)


async def merge_neo_relationship(tx, src_id, tgt_id, props: dict = None, rel_type: str = 'RELATED',
                                 label_src: str = 'Node', label_tgt: str = 'Node', id_field: str = "id"):
    '''
    rel_type:å…³ç³»ç±»å‹å "RELATED"ä»»æ„æ³›èŠ‚ç‚¹,"DEPENDS_ON"ä¾èµ–
    MATCH (a:Task {{id: $source_id}}), (b:Task {{id: $target_id}})
    MERGE (a)-[r:DEPENDS_ON]->(b)
    SET r += {{{props_str}}}
    '''
    if props:
        def serialize(v):
            if isinstance(v, (dict, list, tuple)):
                return json.dumps(v)
            return v

        safe_props = {k: serialize(v) for k, v in props.items()}
        props_str = ", ".join(f"{k}: ${k}" for k in safe_props)
        query = (
            f"MATCH (a:{label_src} {{{id_field}: $src_id}}), (b:{label_tgt} {{{id_field}: $tgt_id}})"
            f"MERGE (a)-[r:{rel_type}]->(b) "
            f"SET r += {{{props_str}}}"
        )
        await tx.run(query, src_id=src_id, tgt_id=tgt_id, **safe_props)
    else:
        # æ— å±æ€§å…³ç³»
        query = (
            f"MATCH (a:{label_src} {{{id_field}: $src_id}}),(b:{label_tgt} {{{id_field}: $tgt_id}}) "
            f"MERGE (a)-[r:{rel_type}]->(b)"
        )
        await tx.run(query, src_id=src_id, tgt_id=tgt_id)


async def scan_from_redis(redis, key: str = "funcmeta", user: str = None, batch_count: int = 0):
    """
    ä» Redis ä¸­è·å–åŒ¹é…çš„æ‰€æœ‰å…ƒæ•°æ®è®°å½•ï¼Œæ”¯æŒ scan æˆ– keys æ–¹å¼ã€‚

    Args:
        redis: Redis å®ä¾‹ã€‚
        key: Redis key å‰ç¼€ï¼ˆå¦‚ "funcmeta"ï¼‰ã€‚
        user: ç”¨æˆ· IDï¼ˆå¯é€‰ï¼‰ï¼Œè‹¥æŒ‡å®šåˆ™æ‹¼æ¥ä¸º funcmeta:{user}:*
        batch_count: æ¯æ‰¹ scan çš„æ•°é‡ï¼ˆå¤§äº 0 ä½¿ç”¨ scanï¼Œå¦åˆ™ç”¨ keysï¼‰ã€‚

    Returns:
        List[dict]: åŒ¹é…åˆ°çš„ JSON æ•°æ®åˆ—è¡¨ã€‚
    """
    if user:
        match_pattern = f"{key}:{user}:*"
    else:
        match_pattern = f"{key}:*"

    data = []
    if batch_count > 0:
        cursor = b'0'
        while cursor:
            cursor, keys = await redis.scan(cursor=cursor, match=match_pattern, count=batch_count)
            if keys:
                values = await redis.mget(*keys)
                data.extend(json.loads(v) for v in values if v)
    else:
        keys = await redis.keys(match_pattern)
        if keys:
            cached_values = await redis.mget(*keys)
            data = [json.loads(v) for v in cached_values if v]  # set(cached_values
    return data


async def do_job_by_lock(func_call: Callable, redis_key: str = None, lock_timeout: int = 600, **kwargs):
    redis = get_redis()
    if not redis:
        await func_call(**kwargs)
        return

    if not redis_key:
        redis_key = f'lock:{func_call.__name__}'
    lock_value = str(uuid.uuid4())  # str(time.time())ï¼Œæ¯ä¸ªworkerä½¿ç”¨å”¯ä¸€çš„lock_value
    lock_acquired = await redis.set(redis_key, lock_value, nx=True, ex=lock_timeout)
    if not lock_acquired:
        print(f"âš ï¸ åˆ†å¸ƒå¼é”å·²è¢«å ç”¨ï¼Œè·³è¿‡ä»»åŠ¡: {func_call.__name__}")
        return

    try:
        print(f"ğŸ”’ è·å–é”æˆåŠŸï¼Œå¼€å§‹æ‰§è¡Œä»»åŠ¡: {func_call.__name__}")
        await func_call(**kwargs)
    except Exception as e:
        print(f"âš ï¸ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {func_call.__name__} -> {e}")
    finally:
        # current_lock_value = await redis.get(redis_key)
        # if current_lock_value and current_lock_value == lock_value:
        #     await redis.delete(redis_key)
        # ä½¿ç”¨ Lua è„šæœ¬ä¿è¯åŸå­æ€§ï¼Œç¡®ä¿åªæœ‰é”æŒæœ‰è€…èƒ½é‡Šæ”¾ï¼Œåªæœ‰æœ€åˆè·å–é”çš„é‚£ä¸ªworkeræ‰èƒ½æˆåŠŸåˆ é™¤é”
        lua_script = """
           if redis.call("get", KEYS[1]) == ARGV[1] then
               return redis.call("del", KEYS[1])
           else
               return 0
           end
           """
        await redis.eval(lua_script, 1, redis_key, lock_value)


async def consumer_worker(queue: asyncio.Queue[tuple[Any, int]], process_task: Callable, max_retries: int = 0,
                          delay: float = 1.0, **kwargs):
    """
    å¯åŠ¨ worker:
        workers_task_background = [asyncio.create_task(consumer_worker(queue, process_task)) for _ in range(4)],
        #await asyncio.gather(*tasks)

    asyncio.Queue æ˜¯å†…å­˜å¯¹è±¡ï¼Œåªå­˜åœ¨äºå½“å‰è¿›ç¨‹çš„äº‹ä»¶å¾ªç¯ä¸­
    :param queue:asyncio.Queue()
    :param process_task:æ³¨å…¥å¤„ç†å‡½æ•° async def f(task: tuple, **kwargs),è¿”å›å€¼ true,false,å¤±è´¥å¯ä»¥é€‰æ‹© await queue.put(...retry+1):ä¼šç­‰å¾…é˜Ÿåˆ—æœ‰ç©ºé—´å†æ”¾å…¥ /.put_nowait(x)
    :param max_retries,0ä¸é‡è¯•
    :param delay: é‡è¯•å‰å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    :return:
    """
    while True:
        task = await queue.get()
        if task is None:
            print("[Info] Received shutdown signal")
            queue.task_done()
            break
        try:
            success = await process_task(task, **kwargs)
            if not success and max_retries > 0:
                if isinstance(task, (list, tuple)) and isinstance(task[-1], int):
                    retry_count = task[-1]
                    task_data = task[:-1]
                    if retry_count < max_retries:
                        await asyncio.sleep(delay)
                        new_task = (*task_data, retry_count + 1)  # é‡å»ºä»»åŠ¡ï¼Œé‡è¯•æ¬¡æ•°+1
                        await queue.put(new_task)
                        print(f"[Task Retry] {task_data} (attempt {retry_count + 1})")
                    else:
                        print(f"[Task Failed] {task_data}")

        except asyncio.CancelledError:
            print("[Cancel] Worker shutting down...")
            break
        except Exception as e:
            print(f"[Error] Unexpected error processing task: {e}")
        finally:
            queue.task_done()  # å¿…é¡»è°ƒç”¨ï¼Œæ ‡è®°ä»»åŠ¡å®Œæˆ


async def stop_worker(queue: asyncio.Queue, worker_tasks: list):
    '''ä¼˜é›…åœæ­¢æ‰€æœ‰ worker'''
    try:
        await queue.join()  # ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º

        print("All tasks processed. Stopping consumers...")
        for _ in worker_tasks:
            await queue.put(None)  # å‘é€åœæ­¢ä¿¡å·
    except Exception as e:
        print(f"[Tasks Error] {e}, attempting to cancel workers...")
        for c in worker_tasks:
            c.cancel()

    finally:
        # ç»Ÿä¸€å›æ”¶æ‰€æœ‰ä»»åŠ¡
        await asyncio.gather(*worker_tasks, return_exceptions=True)


# å¼‚æ­¥ç”Ÿäº§è€…
async def producer_push(messages: list, queue_key: str = "message_queue", status_key: str = "task_status", redis=None,
                        sleep: float = 0):
    """
    å¼‚æ­¥ä»»åŠ¡ç”Ÿäº§è€…ï¼šå°†ä»»åŠ¡å†™å…¥ Redis é˜Ÿåˆ—å’ŒçŠ¶æ€è®°å½•å“ˆå¸Œè¡¨ã€‚

    Args:
        messages (list): å¾…å†™å…¥çš„æ¶ˆæ¯ï¼ˆå­—ç¬¦ä¸²æˆ– dict åˆ—è¡¨ï¼‰ã€‚
        queue_key (str): Redis é˜Ÿåˆ—åç§°ï¼ˆç”¨äº BLPOP æ¶ˆè´¹ï¼‰ã€‚
        status_key (str): Redis å“ˆå¸Œè¡¨åç§°ï¼Œç”¨äºè®°å½•ä»»åŠ¡çŠ¶æ€ã€‚
        redis: Redis å®ä¾‹ï¼ˆè‹¥ä¸ºç©ºåˆ™è‡ªåŠ¨åˆ›å»ºï¼‰ã€‚
        sleep (float): æ¯æ¡æ¶ˆæ¯å†™å…¥é—´éš”æ—¶é—´ï¼Œå•ä½ç§’ã€‚
    """
    # rpush, hset, ç­‰å†™æ“ä½œ
    if redis is None:
        redis = get_redis()

    # await redis.rpush(queue_key, *messages)  # å¼‚æ­¥æ”¾å…¥é˜Ÿåˆ—
    for message in messages:
        if isinstance(message, (dict, list, tuple)):
            msg_str = json.dumps(message, ensure_ascii=False)
        else:
            msg_str = str(message)
        await redis.rpush(queue_key, msg_str)
        await redis.hset(status_key, msg_str, "pending")  # å¦‚æœå¤šä¸ªä»»åŠ¡æœ‰é‡å¤æ–‡æœ¬ï¼ŒçŠ¶æ€ä¼šè¢«è¦†ç›–
        print(f"Produced: {message}")
        if sleep > 0:
            await asyncio.sleep(sleep)


# å¼‚æ­¥æ¶ˆè´¹è€…
async def consumer_redis(process_task: Callable, queue_key: str | list[str] = "message_queue",
                         status_key: str = "task_status", redis=None, max_errors: int = 0, timeout: Optional[int] = 0,
                         **kwargs):
    """
    Redis å¼‚æ­¥æ¶ˆè´¹è€…
    å¼‚æ­¥å¤„ç†ä»»åŠ¡
        task = asyncio.create_task(consumer_redis(process_task,queue_key,args..., **kwargs))
        task.cancel()
        await task  # æ•è·å¼‚å¸¸åè‡ªç„¶é€€å‡º
    æµå¼è·å–ä»»åŠ¡
        async for task in consumer_redis(None,queue_key):
            await handler(task)
            await asyncio.sleep(1)

    Args:
        process_task (Callable): å¤„ç†å‡½æ•°ï¼Œæ¥æ”¶ task_data, è¿”å› bool è¡¨ç¤ºæ˜¯å¦æˆåŠŸ
        queue_key (str): Redis åˆ—è¡¨é˜Ÿåˆ—é”®åï¼ˆç”¨äº BLPOPï¼‰
        status_key (str): Redis å“ˆå¸Œï¼Œç”¨äºè®°å½•æ¯æ¡ä»»åŠ¡çŠ¶æ€
        redis (Redis): Redis å®ä¾‹
        max_errors (int): è¿ç»­å¤±è´¥æœ€å¤§æ¬¡æ•°ï¼Œè¶…è¿‡åé€€å‡ºï¼ˆ0 è¡¨ç¤ºä¸é™ï¼‰
        timeout (int): é˜»å¡è¶…æ—¶ç§’æ•°
    """
    if redis is None:
        redis = get_redis()
    error_count = 0
    while True:
        try:
            # å¼‚æ­¥é˜»å¡æ¶ˆè´¹,æ”¯æŒå¤šé˜Ÿåˆ—ç›‘å¬ Left Pop, timeout fallback
            message = await redis.blpop([queue_key] if isinstance(queue_key, str) else queue_key, timeout=timeout)
            if not message:
                print("[Info] Queue empty, exiting...")
                break

            q, item = message  # ç¬¬ä¸€ä¸ªå…ƒç´  channel_name,ç›‘å¬çš„é˜Ÿåˆ—å,'item' æ˜¯ bytes ç±»å‹
            try:
                task_data = json.loads(item.decode('utf-8'))
                print(f"[Task] Consumed: {task_data}")  # message[1].decode()
            except (json.JSONDecodeError, UnicodeDecodeError) as e:
                print(f"[Error] Invalid task format: {item}, error: {e}")
                await redis.hset(status_key, item, f"invalid_format: {str(e)}")
                continue

            if process_task is None:
                yield task_data
            else:
                try:
                    success = await process_task(task_data, **kwargs)
                    status = "completed" if success else "failed"
                    # task_id = task_data.get("id", hash(item))
                    await redis.hset(status_key, item, status)
                    error_count = 0
                except asyncio.CancelledError:
                    print("[Info] Consumer received shutdown signal")
                    break
                except Exception as e:
                    error_count += 1
                    print(f"[Error] Task processing error: {e}")
                    await redis.hset(status_key, item, f"error: {str(e)}")
                    await asyncio.sleep(1)  # é˜²æ­¢å´©æºƒå¾ªç¯

                    if error_count >= max_errors > 0:
                        print(f"[Fatal] Exceeded max_errors ({max_errors}), exiting consumer.")
                        break

        except Exception as e:
            print(f"[Critical] Redis error in consumer loop: {e}")
            await asyncio.sleep(2)

        # finally:
        #     await redis.close()
    print("[Exit] Consumer loop ended.")


# asyncio.create_task(

class AsyncAbortController:
    def __init__(self):
        self._abort_event = asyncio.Event()  # çº¿ç¨‹å®‰å…¨ threading.Event()

    def should_abort(self) -> bool:
        """æŸ¥è¯¢æ˜¯å¦å·²è§¦å‘ç»ˆæ­¢ä¿¡å·,å®æ—¶æ£€æŸ¥æ˜¯å¦ç»ˆæ­¢"""
        return self._abort_event.is_set()

    async def wait_abort(self):
        """ç­‰å¾…ç»ˆæ­¢ä¿¡å·ï¼ˆå¯ç”¨äºå¹¶å‘ awaitï¼‰,å¯ await ç­‰å¾…ä¸­æ–­"""
        await self._abort_event.wait()

    def abort(self):
        """å¤–éƒ¨è§¦å‘ç»ˆæ­¢ä¿¡å·,è§¦å‘ç»ˆæ­¢,æ˜¯å¦æå‰ç»ˆæ­¢,å¯ä¾›å¤–éƒ¨è§¦å‘"""
        self._abort_event.set()

    def reset(self):
        """æ¸…é™¤ç»ˆæ­¢ä¿¡å·ï¼Œä¸ºä¸‹ä¸€è½®ä»»åŠ¡åšå‡†å¤‡,é‡æ–°å¯åŠ¨å‰å¤ä½,å¯å¤šè½®å¤ç”¨"""
        self._abort_event.clear()


class TaskStatus(Enum):
    # "pending" | "ready" | "running" | "done" | "failed"
    PENDING = "pending"  # ç­‰å¾…æ¡ä»¶æ»¡è¶³
    READY = "ready"  # æ¡ä»¶æ»¡è¶³ï¼Œå¯ä»¥æ‰§è¡Œ
    IN_PROGRESS = "running"  # processing

    COMPLETED = "done"
    FAILED = "failed"
    RECEIVED = "received"


@dataclass
class TaskNode:
    name: str  # task_id ä»»åŠ¡åæˆ–åˆ«å
    description: Optional[str] = None
    # type: Optional[str] = None  # 'api', 'llm', 'script'
    action: Optional[str] = None  # execute ä»»åŠ¡çš„æ‰§è¡Œé€»è¾‘ï¼ˆå¯è°ƒç”¨å¯¹è±¡å‡½æ•°ã€è„šæœ¬æˆ–å¼•ç”¨çš„æ“ä½œç±»å‹),å¯æ‰§è¡ŒåŠ¨ä½œï¼ˆå¦‚è„šæœ¬ã€æ³¨å†Œåã€å‡½æ•°åï¼‰
    event: Any = None  # è§¦å‘æ ‡å¿—ï¼ˆä¸å¤„ç†ä¾èµ–é€»è¾‘ï¼‰äº‹ä»¶æ˜¯æ ‡è¯†ç¬¦ï¼Œç”¨äºä»»åŠ¡ä¹‹é—´çš„è§¦å‘,æŒ‡ç¤ºè§¦å‘çš„äº‹ä»¶ç±»å‹å’Œé™„åŠ æ•°æ®

    status: TaskStatus = TaskStatus.PENDING
    progress: int = 0  # æ‰§è¡Œè¿›åº¦ï¼Œé€‚åˆé•¿ä»»åŠ¡ã€æµä»»åŠ¡åœºæ™¯
    priority: int = 10  # æ‰§è¡Œé¡ºåºæ§åˆ¶
    command: Any = field(default_factory=dict)  # èŠ‚ç‚¹æ‰§è¡Œå‡½æ•°åŠ¨æ€è·³è½¬,goto,é™æ€è¾¹èµ° TaskEdge
    tags: List[str] = field(default_factory=list)  # åˆ†ç±»/æœç´¢ï¼Œç´¢å¼•ã€åˆ†ç»„ã€è¿‡æ»¤

    start_time: float = field(default_factory=time.time)
    end_time: float = 0

    data: Any = field(default_factory=dict)  # æ‰§è¡Œè¾“å…¥
    result: Any = field(default_factory=list)
    count: int = 0  # ç»“æœæ•°é‡

    @staticmethod
    def default_node_attrs(attributes: dict):
        default_attrs = {
            "status": "pending",
            "start_time": lambda: time.time(),
            "event": None,
            "action": None,
            **attributes
        }
        return {k: v() if callable(v) else v for k, v in default_attrs.items()}

    @property
    def duration(self) -> float:
        return (self.end_time - self.start_time) if self.end_time > self.start_time else 0.0

    async def to_redis(self, redis_client, ex=3600, key_prefix='task'):
        key = f"{key_prefix}:{self.name}"
        payload = dataclass2dict(self)
        payload.pop("data", None)
        await redis_client.setex(key, ex, json.dumps(payload, ensure_ascii=False))

    @classmethod
    async def from_redis(cls, redis_client, task_id: str, ex: int = 0, key_prefix='task'):
        key = f"{key_prefix}:{task_id}"
        value = await redis_client.get(key)
        if value:
            if ex > 0:
                await redis_client.expire(key, ex)
            data = json.loads(value)
            if "status" in data and data["status"] is not None:
                data["status"] = TaskStatus[data["status"]]
            return cls(**data)
        return None


class TaskManager:
    Task_queue: dict[str, TaskNode] = {}  # queue.Queue(maxsize=Config.MAX_TASKS)
    Task_lock = asyncio.Lock()
    key_prefix = 'task'

    @classmethod
    async def add_task(cls, task_id: str, task: TaskNode, redis_client=None, ex=3600):
        if redis_client:
            await task.to_redis(redis_client, ex=ex, key_prefix=cls.key_prefix)

        async with cls.Task_lock:
            cls.Task_queue[task_id or task.name] = task

    @classmethod
    async def remove_task(cls, task_id: str):
        async with cls.Task_lock:
            cls.Task_queue.pop(task_id, None)

    @classmethod
    async def get_task(cls, task_id: str, redis_client=None, ex: int = 0) -> TaskNode | None:
        async with cls.Task_lock:
            if task_id in cls.Task_queue:
                return cls.Task_queue[task_id]

        if redis_client:
            task = await TaskNode.from_redis(redis_client, task_id, ex=ex, key_prefix=cls.key_prefix)
            if task:
                async with cls.Task_lock:
                    cls.Task_queue[task_id] = task
            return task

        return None

    @classmethod
    async def get_task_status(cls, task_id: str, redis_client=None) -> TaskStatus | None:
        task = await cls.get_task(task_id, redis_client)
        if task:
            return task.status
        return None

    @classmethod
    async def set_task_status(cls, task: TaskNode, status: TaskStatus, progress: int = 10, redis_client=None):
        async with cls.Task_lock:
            task.status = status
            task.progress = progress
            cls.Task_queue[task.name] = task

        if redis_client:
            await task.to_redis(redis_client, key_prefix=cls.key_prefix)

    @classmethod
    async def update_task_result(cls, task_id: str, result, status: TaskStatus = TaskStatus.COMPLETED,
                                 redis_client=None):
        task: TaskNode = await cls.get_task(task_id, redis_client)
        if not task:
            print(f"[update_task_result] Task {task_id} not found.")
            return None

        async with cls.Task_lock:
            task.result = result
            task.count = len(result) if isinstance(result, (list, set, tuple)) else 1 if result else 0
            task.status = status
            task.end_time = time.time()
            if task.status in (TaskStatus.COMPLETED, TaskStatus.RECEIVED):
                task.data = None
                task.progress = 100
            elif task.status == TaskStatus.FAILED:
                print(f"Task failed: {dataclass2dict(task)}")

        if redis_client:
            await task.to_redis(redis_client, key_prefix=cls.key_prefix)

        return task.result

    @classmethod
    async def clean_old_tasks(cls, timeout_received=3600, timeout=86400):
        current_time = time.time()
        task_ids_to_delete = []

        for _id, task in cls.Task_queue.items():
            if task.end_time and (current_time - task.end_time) > timeout_received:
                if task.status == TaskStatus.RECEIVED:
                    task_ids_to_delete.append(_id)
                    print(f"Task {_id} has been marked for cleanup. Status: RECEIVED")
            elif (current_time - task.start_time) > timeout:
                task_ids_to_delete.append(_id)
                print(f"Task {_id} has been marked for cleanup. Timeout exceeded")

        if task_ids_to_delete:
            async with cls.Task_lock:
                for _id in task_ids_to_delete:
                    cls.Task_queue.pop(_id, None)


@dataclass
class TaskEdge:
    """å®šä¹‰ä»»åŠ¡ä¾èµ–å…³ç³»çš„æœ‰å‘è¾¹"""
    source: str  # ä¾èµ–çš„èµ·å§‹ä»»åŠ¡ID
    target: str  # è¢«è§¦å‘çš„ä»»åŠ¡ID
    # è¾¹ä¸Šçš„è§¦å‘æ¡ä»¶ï¼ˆä¸æºä»»åŠ¡çš„çŠ¶æ€ç›¸å…³),["done",{"deadline": time.time() + 60}]
    condition: Union[str, Dict[str, Any]]  # è§¦å‘æ¡ä»¶ï¼Œå¦‚ "done" æˆ– {"deadline": timestamp}

    # ä½¿ç”¨fieldæä¾›é»˜è®¤å€¼ä»¥é¿å…å¯å˜é»˜è®¤å€¼é—®é¢˜,absolute,relative,[None, {"relative": 5}]
    trigger_time: Optional[Dict[str, Union[int, float]]] = field(
        default=None,
        metadata={
            "description": "æ—¶é—´è§¦å‘é…ç½®ï¼Œå¦‚ {'relative': 5}(ç§’) æˆ– {'absolute': 1680000000}(æ—¶é—´æˆ³)"
        }
    )
    # ä»»åŠ¡è§¦å‘äº‹ä»¶,Noneæ— ä¾èµ–
    trigger_event: Optional[str] = field(
        default=None,
        metadata={"description": "äº‹ä»¶åç§°ï¼Œå¦‚ 'file_uploaded'"}
    )
    # å¤æ‚æ¡ä»¶,å‡½æ•°æˆ–å¤æ‚çš„é€»è¾‘åˆ¤æ–­
    rule: Optional[Callable[..., bool]] = field(
        default=None,
        metadata={"description": "è‡ªå®šä¹‰æ¡ä»¶å‡½æ•°ï¼Œæ¥å—ä¸Šä¸‹æ–‡è¿”å›å¸ƒå°”å€¼"}
    )

    def __post_init__(self):
        """æ•°æ®æ ¡éªŒå’Œè½¬æ¢"""
        self._validate_condition()
        self._normalize_trigger_time()

    def as_dict(self) -> dict:
        return dataclass2dict(self)

    @classmethod
    def from_dict(cls, data: dict, rule_function: Callable = None):
        if rule_function:
            if "rule" in data and isinstance(data["rule"], str):
                data["rule"] = rule_function(data["rule"])  # ä»æ³¨å†Œè¡¨æˆ–åŠ¨æ€å¯¼å…¥æ¢å¤å‡½æ•°
        return cls(**data)

    @classmethod
    def get(cls, key):
        return getattr(cls, key, None)

    def _validate_condition(self):
        """éªŒè¯conditionå­—æ®µæ ¼å¼"""
        if isinstance(self.condition, str):
            if self.condition not in ("done", "failed", "running"):
                raise ValueError(f"Invalid condition string: {self.condition}")
        elif isinstance(self.condition, dict):
            deadline = self.condition.get("deadline")
            if deadline is not None and not isinstance(deadline, (int, float)):
                raise TypeError("Deadline must be numeric")

    def _normalize_trigger_time(self):
        """å°†ç›¸å¯¹æ—¶é—´è½¬æ¢ä¸ºç»å¯¹æ—¶é—´æˆ³"""
        if self.trigger_time and "relative" in self.trigger_time:
            self.trigger_time = {
                "absolute": time.time() + self.trigger_time["relative"]
            }

    def should_trigger(self, context: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ»¡è¶³è§¦å‘æ¡ä»¶"""
        # 1. æ£€æŸ¥æ—¶é—´æ¡ä»¶
        if self.trigger_time and "absolute" in self.trigger_time:
            if time.time() < self.trigger_time["absolute"]:
                return False

        # 2. æ£€æŸ¥åŸºç¡€æ¡ä»¶
        if isinstance(self.condition, str):
            if context.get("status") != self.condition:
                return False
        elif isinstance(self.condition, dict):
            if "deadline" in self.condition:
                if time.time() < self.condition["deadline"]:
                    return False

        # 3. æ£€æŸ¥è‡ªå®šä¹‰è§„åˆ™
        if self.rule and not self.rule(**context):
            return False

        return True


class TaskScheduler:
    Task_graph = ig.Graph(directed=True)  # åˆ›å»ºæœ‰å‘å›¾

    def __init__(self, graph: Optional[ig.Graph] = None, driver: Optional[GraphDatabase] = None):
        self.graph = self.__class__.Task_graph.copy() if graph is None else graph  # å±€éƒ¨å‰¯æœ¬
        for attr in ["name", "status", "action", "start_time", "event", "priority"]:
            if attr not in self.graph.vs.attributes():  # "name" in Task_graph.vs.attributes()
                self.graph.vs[attr] = [None] * self.graph.vcount()

        self.driver = driver or get_neo_driver()

    def set_task_node(self, task_id: str, attributes: Dict[str, Any]) -> None:
        """æ·»åŠ æˆ–æ›´æ–°ä»»åŠ¡èŠ‚ç‚¹,æ·»åŠ èŠ‚ç‚¹åˆ°å›¾ä¸­"""
        # nodes = self.graph.vs["name"]
        try:
            node = self.graph.vs.find(name=task_id)
            node.update_attributes(**attributes)
            # if task_id in nodes:
            # # self.graph.vs[node.index].update_attributes(**attributes)
            # for k, v in attributes.items():
            #     node[k] = v
        except ValueError:
            default_attrs = TaskNode.default_node_attrs(attributes)
            self.graph.add_vertex(name=task_id, **default_attrs)

    async def set_node(self, task_node: TaskNode, **kwargs):
        """æ·»åŠ æˆ–æ›´æ–°ä»»åŠ¡èŠ‚ç‚¹ï¼Œä½¿ç”¨ TaskNode å¯¹è±¡,å‘ Neo4j æ·»åŠ æˆ–æ›´æ–°ä¸€ä¸ªä»»åŠ¡èŠ‚ç‚¹"""
        task_id = task_node.name
        attributes = dataclass2dict(task_node)  # asdict(task_node)
        attributes.update(kwargs)
        attributes.pop("data", None)
        try:
            node = self.graph.vs.find(name=task_id)
            node.update_attributes(**attributes)
        except ValueError:
            self.graph.add_vertex(name=task_id, **attributes)

        async with self.driver.session() as session:
            await session.execute_write(merge_neo_node, task_id, attributes, 'Task')
        return task_id

    def update_task_status(self, task_id: str, new_status: TaskStatus | str) -> None:
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        if isinstance(new_status, str):
            new_status = TaskStatus(new_status)
        node = self.graph.vs.find(name=task_id)
        node["status"] = new_status  # self.graph.vs[node.index]["status"] = new_status
        if new_status == TaskStatus.IN_PROGRESS:  # "running"
            node["start_time"] = time.time()

    @staticmethod
    async def update_task_status_neo(driver, task_id: str, new_status: TaskStatus | str):
        """
        å°†ä»»åŠ¡çŠ¶æ€æ›´æ–°åŒæ­¥åˆ° Neo4j èŠ‚ç‚¹ï¼ˆNode æ ‡ç­¾ï¼‰

        Args:
            driver: Neo4j å¼‚æ­¥é©±åŠ¨
            task_id (str): èŠ‚ç‚¹ IDï¼ˆå³ nameï¼‰
            new_status (TaskStatus | str): æ–°çŠ¶æ€
        """
        if isinstance(new_status, TaskStatus):
            new_status = new_status.value  # å¦‚æœæ˜¯ Enumï¼Œå–å‡ºå­—ç¬¦ä¸²

        start_time = time.time() if new_status == TaskStatus.IN_PROGRESS else None

        async def _update(tx, task_id, new_status, start_time):
            query = """
            MATCH (n:Task {id: $task_id})
            SET n.status = $new_status
            """ + (", n.start_time = $start_time" if start_time is not None else "")
            await tx.run(query, task_id=task_id, new_status=new_status, start_time=start_time)

        async with driver.session() as session:
            await session.execute_write(_update, task_id, new_status, start_time)

    def set_task_dependency(self, edge: TaskEdge | dict, **kwargs):
        """æ·»åŠ ä»»åŠ¡ä¾èµ–å…³ç³»"""
        if isinstance(edge, dict):
            edge = TaskEdge.from_dict(edge)  # TaskEdge(**edge)
        if edge.source not in self.graph.vs["name"] or edge.target not in self.graph.vs["name"]:
            raise ValueError("Source or target task does not exist")

        rel_props = {**edge.__dict__, **kwargs}
        rel_props.pop("source", None)
        rel_props.pop("target", None)

        if self.graph.are_adjacent(edge.source, edge.target):  # æ˜¯å¦ç›´æ¥ç›¸è¿ï¼ˆæœ‰æ— ä¸€æ¡è¾¹ï¼‰
            eid = self.graph.get_eid(edge.source, edge.target)
            self.graph.es[eid].update_attributes(**rel_props)
        else:
            self.graph.add_edge(edge.source, edge.target, **rel_props)
        return rel_props

    async def set_edge(self, task_edge: TaskEdge, **kwargs):
        """
        å‘ Neo4j æ·»åŠ æˆ–æ›´æ–°ä¸€æ¡ä»»åŠ¡ä¾èµ–è¾¹ï¼ˆä»»åŠ¡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼‰

        Args:
            task_edge (TaskEdge): åŒ…å« sourceã€target å’Œå…¶ä»–è¾¹å±æ€§
            kwargs: å¯é€‰é™„åŠ å±æ€§ï¼ˆå°†åˆå¹¶è¿› edge å±æ€§ä¸­ï¼‰
        """
        rel_props = self.set_task_dependency(task_edge, **kwargs)
        async with self.driver.session() as session:
            await session.execute_write(merge_neo_relationship, task_edge.source, task_edge.target, rel_props,
                                        'DEPENDS_ON', 'Task', 'Task')

    def graph_edge_to_task(self, edge: ig.Edge) -> TaskEdge:
        edge_dict = edge.attributes()
        # edge_dict.pop("source", None)
        # edge_dict.pop("target", None)
        return TaskEdge(source=self.graph.vs[edge.source]["name"], target=self.graph.vs[edge.target]["name"],
                        **edge_dict)

    def set_task_edges(self, edges: List[tuple], attributes: List[Dict[str, Any]]):
        # æ‰¹é‡è®¾ç½®è¾¹ç»“æ„ï¼ˆæº+ç›®æ ‡+å±æ€§ï¼‰
        for (source, target), attr in zip(edges, attributes):
            attrs = {k: v for k, v in attr.items() if k in TaskEdge.__annotations__}
            other_attrs = {k: v for k, v in attr.items() if k not in TaskEdge.__annotations__}
            edge = TaskEdge(source=source, target=target, **attrs)  # from_dict()
            self.set_task_dependency(edge, **other_attrs)

    def export_adjacency_list(self) -> dict:
        adjacency_list = defaultdict(list)

        for edge in self.graph.es:
            source = self.graph.vs[edge.source]["name"]
            target = self.graph.vs[edge.target]["name"]

            adjacency_list[source].append({
                "name": target,
                "attrs": edge.attributes()
            })

        return dict(adjacency_list)

    def export_nodes(self) -> dict:
        return {v["name"]: v.attributes() for v in self.graph.vs}
        # [self.graph.vs[n]["name"] for n in self.graph.successors(v.index)]

    def _check_condition(self, edge: TaskEdge | ig.Edge | Dict[str, Any]) -> int:
        """æ£€æŸ¥è¾¹è§¦å‘æ¡ä»¶æ˜¯å¦æ»¡è¶³"""
        if isinstance(edge, ig.Edge):
            edge = self.graph_edge_to_task(edge)
        elif isinstance(edge, dict):
            edge = TaskEdge.from_dict(edge)  # TaskEdge(**edge)

        source = self.graph.vs.find(name=edge.source)
        target = self.graph.vs.find(name=edge.target)

        # åŸºç¡€çŠ¶æ€æ£€æŸ¥
        if target["status"] != TaskStatus.PENDING:
            return 0

        # æ¡ä»¶ç±»å‹å¤„ç†,çŠ¶æ€æ¡ä»¶
        condition = edge.condition
        if condition is None:
            return 0

        if isinstance(condition, str):
            try:
                condition = TaskStatus(condition)
            except:
                print(condition)

        event_ready = 0
        if isinstance(condition, TaskStatus):
            if source["status"] != condition:
                return 0
            # ä»»åŠ¡çŠ¶æ€å˜åŒ–åè§¦å‘äº‹ä»¶é©±åŠ¨çš„ä»»åŠ¡è¾¹æˆ–è€…ä»»åŠ¡Aå®Œæˆæ—¶è§¦å‘å¤šä¸ªäº‹ä»¶
            if edge.trigger_event and source.get("event") != edge.trigger_event:
                return 0
            # æ£€æŸ¥æ—¶é—´æ¡ä»¶>
            if edge.trigger_time:
                if "absolute" in edge.trigger_time and time.time() < edge.trigger_time["absolute"]:
                    return 0
                if "relative" in edge.trigger_time and time.time() < source["start_time"] + edge.trigger_time[
                    "relative"]:
                    return 0
            event_ready = 1

        elif isinstance(condition, dict) and "deadline" in condition:
            if time.time() > condition["deadline"]:
                return 0
            event_ready = 1 << 1  # æ—¶é—´æ¡ä»¶<
        elif callable(condition):
            if not condition():
                return 0
            event_ready = 1 << 2  # è‡ªå®šä¹‰æ¡ä»¶
        # è‡ªå®šä¹‰è§„åˆ™
        if edge.rule:
            if not edge.rule():
                return 0

        return event_ready

    def check_and_trigger_tasks(self) -> List[str]:
        """æ£€æŸ¥å¹¶è§¦å‘ç¬¦åˆæ¡ä»¶çš„ä»»åŠ¡"""
        triggered = []
        for edge in self.graph.es:  # éå†å…¨éƒ¨è¾¹
            if self._check_condition(edge) > 0:
                target = self.graph.vs.find(name=edge.target)
                target["status"] = TaskStatus.READY
                triggered.append(target["name"])
                print(f"Task {target['name']} triggered by {edge.source}->{edge.target}")
        return triggered

    @staticmethod
    def _simulate_task_execution(task):
        # æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œ
        attrs = task.attributes()
        action = attrs.get("action")
        if isinstance(action, str):
            print(f"Simulating execution for {task['name']}:{action}")
        elif callable(action):
            return action()
        time.sleep(1)
        return True

    def execute_ready_tasks(self, executor: Callable[[Any], bool] = None) -> bool:
        """æ‰§è¡Œæ‰€æœ‰å°±ç»ªä»»åŠ¡"""
        # æŸ¥æ‰¾çŠ¶æ€ä¸º ready çš„ä»»åŠ¡å¹¶æ‰§è¡Œ
        ready_tasks = [v for v in self.graph.vs if v["status"] == TaskStatus.READY]
        if not ready_tasks:
            return False  # æ— å¯æ‰§è¡Œä»»åŠ¡æ—¶é€€å‡º

        for task in ready_tasks:
            print(f"Executing Ready task {task['name']}...")
            task["status"] = TaskStatus.IN_PROGRESS  # "running"
            try:
                exec_func = executor or self._simulate_task_execution
                result = exec_func(task)
                # ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ
                task["status"] = TaskStatus.COMPLETED if result else TaskStatus.FAILED  # "done"'completed'
            except Exception as e:
                task["status"] = TaskStatus.FAILED
                print(f"Task {task['name']} failed,error: {e}")

        return True

    def step(self, executor: Callable[[Any], bool] = None) -> bool:
        triggered = self.check_and_trigger_tasks()  # æ£€æŸ¥å¹¶è§¦å‘æ–°çš„ä»»åŠ¡
        executed = self.execute_ready_tasks(executor)
        return triggered or executed

    def run_scheduler(self, max_cycles: int = 100) -> None:
        if not self.graph.is_dag():
            raise ValueError("ä»»åŠ¡å›¾å­˜åœ¨å¾ªç¯ä¾èµ–ï¼Œæ— æ³•è°ƒåº¦")
        """è¿è¡Œä»»åŠ¡è°ƒåº¦ä¸»å¾ªç¯"""
        if max_cycles:
            for _ in range(max_cycles):
                if not self.step():
                    break
                time.sleep(1)
        else:
            while True:
                if not self.step():
                    break
                time.sleep(1)

        print("Scheduler finished")

        # æ£€æŸ¥ä¾èµ–å¹¶è§¦å‘ä»»åŠ¡


def get_children(g, node):
    # g.successors(node.index)
    return [g.vs[neighbor]["name"] for neighbor in g.neighbors(node, mode="OUT")]


def get_parent(g, node):
    # g.predecessors(node.index)
    neighbors = g.neighbors(node, mode="IN")
    return g.vs[neighbors[0]]["name"] if neighbors else None


def load_graph_from_dict(nodes: dict, adjacency_list: dict) -> ig.Graph:
    """
    ä»èŠ‚ç‚¹ä¿¡æ¯å’Œé‚»æ¥è¡¨æ„å»º igraph.Graph å¯¹è±¡ï¼Œå¹¶èµ‹å€¼ç»™ self.graph

    Args:
        nodes (dict): èŠ‚ç‚¹å­—å…¸ï¼Œå¦‚ {"n1": {"content": "Q1"}, "n2": {"content": "Q2"}}
        adjacency_list (dict): é‚»æ¥è¡¨ï¼Œå¦‚ {"node1": [{"name": "node2", "attrs": {...}}, ...], ...}

    Returns:
        ig.Graph: æ„å»ºå®Œæˆçš„å›¾å¯¹è±¡
    """
    g = ig.Graph(directed=True)

    node_names = list(nodes.keys())
    name_to_index = {name: i for i, name in enumerate(node_names)}

    g.add_vertices(len(node_names))
    g.vs["name"] = node_names
    # è®¾ç½®èŠ‚ç‚¹å±æ€§
    for attr in set(k for v in nodes.values() for k in v):
        g.vs[attr] = [nodes[name].get(attr) for name in node_names]

    # æ·»åŠ è¾¹åŠå…¶å±æ€§
    edge_list = []
    edge_attrs = defaultdict(list)

    for src, neighbors in adjacency_list.items():
        for neighbor in neighbors:
            tgt = neighbor["name"]
            edge_list.append((name_to_index[src], name_to_index[tgt]))

            # æå–æ‰€æœ‰è¾¹å±æ€§ï¼ˆå¦‚æœæ²¡æœ‰åˆ™ä¸º Noneï¼‰
            attrs = neighbor.get("attrs", {})
            for k in attrs:
                edge_attrs[k].append(attrs.get(k))
            # å¯¹æ²¡æœ‰ attrs çš„é”®ä¹Ÿä¿æŒé½æ•´æ€§
            for k in edge_attrs:
                if k not in attrs:
                    edge_attrs[k].append(None)

    g.add_edges(edge_list)
    # è®¾ç½®è¾¹å±æ€§
    for attr, values in edge_attrs.items():
        g.es[attr] = values

    return g


async def export_graph_to_neo4j(nodes: dict, adjacency_list: dict, driver, g: ig.Graph = None, label: str = 'Node',
                                rel_type: str = 'RELATED'):
    """
    å°†å›¾å¯¼å‡ºåˆ° Neo4j

    label: èŠ‚ç‚¹æ ‡ç­¾å
    rel_type: å…³ç³»ç±»å‹å
    nodes: {
        "n1": {"content": "Q1", "type": "root"},
        "n2": {"content": "Q2", "type": "search"}
    }
    adjacency_list: {
        "n1": [{"name": "n2", "attrs": {"relation": "connects"}}]
    }
    """
    async with driver.session() as session:
        if g:
            for v in g.vs:
                node_id = v["name"]
                props = {k: v[k] for k in g.vs.attributes() if k != "name"}
                await session.execute_write(merge_neo_node, node_id, props, label)

            for e in g.es:
                src_id = g.vs[e.source]["name"]
                tgt_id = g.vs[e.target]["name"]
                props = {k: e[k] for k in g.es.attributes()}
                await session.execute_write(merge_neo_relationship, src_id, tgt_id, props, rel_type, label, label)

        else:
            for node_id, props in nodes.items():
                await session.execute_write(merge_neo_node, node_id, props, label)

            for src, neighbors in adjacency_list.items():
                for neighbor in neighbors:
                    tgt = neighbor["name"]
                    rel_props = neighbor.get("attrs", {})
                    await session.execute_write(merge_neo_relationship, src, tgt, rel_props, rel_type, label, label)


def export_to_json_from_graph(graph, filename):
    data = {"nodes": [{"id": v.index, **v.attributes()} for v in graph.vs],
            "edges": [{"source": e.source, "target": e.target, **e.attributes()} for e in graph.es], }
    with open(filename, "w") as f:
        json.dump(data, f, indent=4)

    # graph.write_pickle("graph.pkl")


def import_to_graph_from_json(filename):
    with open(filename, "r") as f:
        data = json.load(f)

    g = ig.Graph(directed=True)  # ig.Graph.TupleList(
    g.add_vertices(len(data["nodes"]))
    # g.vs["name"] = [node["id"] for node in data["nodes"]]
    # ä¸ºèŠ‚ç‚¹è®¾ç½®å±æ€§
    for idx, node in enumerate(data["nodes"]):
        g.vs[idx].update_attributes(node)  # node.items()

    g.add_edges([(edge["source"], edge["target"]) for edge in data["edges"]])  # [("task1", "task2")]

    # ä¸ºè¾¹è®¾ç½®å±æ€§
    for idx, edge in enumerate(data["edges"]):
        # è·³è¿‡ source å’Œ target å±æ€§
        g.es[idx].update_attributes({k: v for k, v in edge.items() if k not in ["source", "target"]})
        # g.es[idx][key] = value

    return g


class WebSearchGraph:
    def __init__(self):
        import queue

        # åˆå§‹åŒ–èŠ‚ç‚¹å†…å®¹å­—å…¸
        self.nodes: Dict[str, Dict[str, str]] = {}
        # åˆå§‹åŒ–é‚»æ¥è¡¨
        self.adjacency_list: Dict[str, List[dict]] = defaultdict(list)
        self.task_queue = queue.Queue()
        self.n_active_tasks = 0

    async def add_root_node(self, node_content: str, node_name: str = 'root'):
        # æ·»åŠ æ ¹èŠ‚ç‚¹
        self.nodes[node_name] = dict(content=node_content, type="root")
        # åœ¨å›¾ä¸­æ·»åŠ èŠ‚ç‚¹

        self.adjacency_list[node_name] = []
        return node_name

    async def add_node(self, node_name: str, node_content: str):
        # æ·»åŠ å­é—®é¢˜èŠ‚ç‚¹
        self.nodes[node_name] = dict(content=node_content, type="search")

        self.adjacency_list[node_name] = []

        # å¤„ç†çˆ¶èŠ‚ç‚¹ï¼ŒæŸ¥æ‰¾ç›¸å…³çš„å†å²ä¸Šä¸‹æ–‡
        parent_response = []
        for start_node, adj in self.adjacency_list.items():
            for neighbor in adj:
                if (node_name == neighbor["name"]  # åˆ¤æ–­æ˜¯å¦æœ‰è¿æ¥,æ˜¯å¦æ˜¯å½“å‰èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹ï¼Œå¹¶ä¸”è¯¥çˆ¶èŠ‚ç‚¹åŒ…å« response ä¿¡æ¯
                        and start_node in self.nodes and "response" in self.nodes[start_node]):
                    parent_response.append(
                        dict(question=self.nodes[start_node]["content"], answer=self.nodes[start_node]["response"]))

        await self._async_node_stream(node_name, node_content)

        self.n_active_tasks += 1  # f"{node_name}-{node_content}"
        return self.n_active_tasks

    async def _async_node_stream(self, node_name: str, node_content: str, parent_response: List[dict]):
        """æ‰§è¡Œå¼‚æ­¥æœç´¢"""
        cfg = {"search_config": "value"}  # é…ç½®æœç´¢
        session_id = random.randint(0, 999999)  # ä¼šè¯ID
        agent = None

        try:
            # æ¨¡æ‹Ÿæœç´¢è¿‡ç¨‹
            searcher_message = "mock_search_message"  # å‡è®¾çš„æœç´¢æ¶ˆæ¯
            self.nodes[node_name]["response"] = searcher_message  # æ›´æ–°èŠ‚ç‚¹å“åº”
            self.nodes[node_name]["session_id"] = session_id
            self.task_queue.put((node_name, self.nodes[node_name]))  # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
        except Exception as exc:
            self.task_queue.put((exc, None))

    async def add_response_node(self, node_name: str = 'response'):
        # æ·»åŠ å›å¤èŠ‚ç‚¹
        self.nodes[node_name] = dict(content='Search completed, thought response added.', type="response")
        # self.adjacency_list[node_name] = []
        self.task_queue.put((node_name, self.nodes[node_name], []))

    async def add_edge(self, start_node: str, end_node: str):
        self.adjacency_list[start_node].append(dict(id=str(uuid.uuid4()), name=end_node, state=2))

        self.task_queue.put((start_node, self.nodes[start_node], self.adjacency_list[start_node]))

    async def reset(self):
        # é‡ç½®å›¾å’ŒèŠ‚ç‚¹
        self.nodes.clear()
        self.adjacency_list.clear()

    def node(self, node_name: str):
        # è·å–èŠ‚ç‚¹ä¿¡æ¯
        if node_name in self.nodes:
            return self.nodes[node_name].copy()

        return None

    def graph(self):
        """æ ¹æ®èŠ‚ç‚¹ä¿¡æ¯å’Œé‚»æ¥è¡¨ç”Ÿæˆå›¾
        nodes = {
            "root": {"content": "What is AI?", "type": "root"},
            "node1": {"content": "What is machine learning?", "type": "search"},
            "node2": {"content": "What is deep learning?", "type": "search"}
        }

        adjacency_list = {
            "root": [{"name": "node1"}, {"name": "node2"}],
            "node1": [{"name": "node2"}],
            "node2": []
        }

        Returns:
            ig.Graph: è¿”å›ç”Ÿæˆçš„å›¾å¯¹è±¡
        """

        return load_graph_from_dict(self.nodes, self.adjacency_list)


# class Task(Base):
#     __tablename__ = 'tasks'
#
#     task_id = Column(String, primary_key=True)
#     status = Column(Enum(TaskStatus), default=TaskStatus.PENDING)
#
#     # åˆ›å»ºä»»åŠ¡çš„æ–¹æ³•
#     @classmethod
#     def create_task(cls, session: Session):
#         task_id = str(uuid.uuid4())  # ç”Ÿæˆå”¯ä¸€ task_id
#         new_task = cls(task_id=task_id)  # åˆ›å»ºä»»åŠ¡å®ä¾‹
#         session.add(new_task)
#         session.commit()
#         return task_id
#
#     # æ›´æ–°ä»»åŠ¡çŠ¶æ€çš„æ–¹æ³•
#     @classmethod
#     def update_task_status(cls, session: Session, task_id: str, new_status: TaskStatus):
#         task = session.query(cls).filter_by(task_id=task_id).first()
#         if task:
#             task.status = new_status
#             session.commit()
#
#     # è·å–ä»»åŠ¡çŠ¶æ€çš„æ–¹æ³•
#     @classmethod
#     def get_task_status(cls, session: Session, task_id: str):
#         task = session.query(cls).filter_by(task_id=task_id).first()
#         if task:
#             return task.status
#         return None
#
#     # æ¨¡æ‹Ÿå¼‚æ­¥ä»»åŠ¡æ‰§è¡Œ
#     @classmethod
#     def async_task(cls, session: Session, task_id: str):
#         import time
#         try:
#             cls.update_task_status(session, task_id, TaskStatus.IN_PROGRESS)
#             time.sleep(1)
#             cls.update_task_status(session, task_id, TaskStatus.COMPLETED)
#         except Exception as e:
#             cls.update_task_status(session, task_id, TaskStatus.FAILED)

_StringLikeT = Union[bytes, str, memoryview]


def list_or_args_keys(keys: Union[_StringLikeT, Iterable[_StringLikeT]],
                      args: Tuple[_StringLikeT, ...] = None) -> List[_StringLikeT]:
    # å°† keys å’Œ args åˆå¹¶æˆä¸€ä¸ªæ–°çš„åˆ—è¡¨
    # returns a single new list combining keys and args
    try:
        iter(keys)
        # a string or bytes instance can be iterated, but indicates
        # keys wasn't passed as a list
        if isinstance(keys, (bytes, str)):
            keys = [keys]
        else:
            keys = list(keys)  # itertools.chain.from_iterable(keys)
    except TypeError:
        keys = [keys]
    if args:
        keys.extend(args)
    return keys


# async def setr(key, value, ex=None):
#     redis = get_redis()
#     await redis.set(name=key, value=value, ex=ex)
#     # await redis.delete(key) redis.get(key, encoding='utf-8')

# pip install celery[redis]
# celery = Celery('tasks', broker='redis://localhost:6379/0') #Celery ä»»åŠ¡
# message_queue = asyncio.Queue()


class MessageZeroMQ:

    def __init__(self, pull_port="7556", push_port="7557", req_port="7555", process_callback=None):
        self.context = zmq.asyncio.Context(io_threads=2)  # zmq.Context()

        # è®¾ç½®æ¥æ”¶æ¶ˆæ¯çš„ socket
        self.pull_socket = self.context.socket(zmq.PULL)
        self.pull_socket.bind(f"tcp://*:{pull_port}")  # ç»‘å®šæ¥æ”¶ç«¯å£

        # è®¾ç½®å‘é€æ¶ˆæ¯çš„ socket
        self.push_socket = self.context.socket(zmq.PUSH)
        self.push_socket.connect(f"tcp://localhost:{push_port}")  # è¿æ¥åˆ° Java çš„æ¥æ”¶ç«¯å£

        # è®¾ç½® REQ socket ç”¨äºè¯·æ±‚-å“åº”æ¨¡å¼
        self.req_socket = self.context.socket(zmq.REQ)  # zmq.DEALER
        if req_port:
            self.req_socket.connect(f"tcp://localhost:{req_port}")  # è¿æ¥åˆ°æœåŠ¡ç«¯

        self.process_callback = process_callback or self.default_process_message
        # self.push_socket.send_string('topic1 Hello, world!')

    def __del__(self):
        self.context.destroy(linger=0)

    @staticmethod
    def default_process_message(message):
        # å¤„ç†é€»è¾‘
        print(f"Processing message: {message}")
        # å¯¹æ¶ˆæ¯è¿›è¡ŒæŸäº›å¤„ç†åéœ€è¦å°†å…¶è½¬å‘å› Java
        return f"Processed: {message}"

    async def send_request(self, message: str = "Hello, server!", topic: str = 'Request'):
        """
        ä½¿ç”¨ REQ socket ä¸»åŠ¨å‘é€æ¶ˆæ¯å¹¶æ¥æ”¶å“åº”,ä¸»åŠ¨è¯·æ±‚-å“åº”
        """
        await self.req_socket.send_string(f'{topic} {message}')
        print(f"Sent request: {message} under topic: {topic}")
        response = await self.req_socket.recv_string()
        print(f"Received response: {response}")
        return response

    async def call_service(self, data):
        """
        ä½¿ç”¨ REQ socket å‘é€ JSON æ•°æ®å¹¶æ¥æ”¶ JSON å“åº” zmq.Context()
        """
        self.req_socket.send_json(data)
        response = self.req_socket.recv_json()
        # message = await self.req_socket.recv()
        # response = json.loads(message.decode())
        print(f"Received response: {response}")
        return response

    # ä¸»åŠ¨å‘é€æ¶ˆæ¯åˆ° ZeroMQ
    async def send_message(self, message: str, topic: str = "Default"):
        await self.push_socket.send_string(f'{topic} {message}', flags=zmq.DONTWAIT, encoding='utf-8')
        print(f"Sent message: {message}")

    async def send_data(self, data=b'Hello in binary'):
        await self.push_socket.send(data, flags=zmq.DONTWAIT, copy=True, track=False)

    async def recv_messages(self):
        while True:
            # å°†æ¥æ”¶åˆ°çš„æ¶ˆæ¯ yield å‡ºå»
            message = await self.pull_socket.recv_string()
            yield message

    async def stream_start(self):
        async for message in self.recv_messages():
            print(f"Received from ZeroMQ: {message}")
            processed_msg = self.process_callback(message)
            await self.send_message(processed_msg, topic="Processed")

    async def start(self):
        # ä½¿ç”¨ asyncio å’Œ run_in_executor è¿›è¡Œé˜»å¡æ“ä½œ
        # loop = asyncio.get_event_loop()

        while True:
            # æ¥æ”¶æ¶ˆæ¯
            message = await self.pull_socket.recv_string()
            print(f"Received from ZeroMQ: {message}")

            # å¤„ç†æ¶ˆæ¯
            processed_msg = self.process_callback(message)

            # å°†å¤„ç†åçš„æ¶ˆæ¯å‘é€
            await self.push_socket.send_string(processed_msg)
            print(f"Sent processed message back.")


#
# import pika
# https://www.rabbitmq.com/tutorials
#
# # Pika is a RabbitMQ,å‘é€æ¶ˆæ¯åˆ° RabbitMQ
# def send_event(event_data):
#     connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
#     channel = connection.channel()
#     channel.queue_declare(queue='event_queue')
#
#     channel.basic_publish(exchange='', routing_key='event_queue',
#                           body=event_data)
#     connection.close()
#
#
# # ç›‘å¬æ¶ˆæ¯å¹¶è°ƒç”¨æœåŠ¡
# def listen_for_events(callback):
#     connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
#     channel = connection.channel()
#     channel.queue_declare(queue='event_queue')
#
#     channel.basic_consume(queue='event_queue', on_message_callback=callback, auto_ack=True)
#
#     print('Waiting for events...')
#     channel.start_consuming()


if __name__ == "__main__":
    from utils import configure_event_loop

    configure_event_loop()


    # message_zero_mq = MessageZeroMQ()  # åˆ›å»ºæ¶ˆæ¯å¤„ç†å™¨å®ä¾‹
    # asyncio.run(message_zero_mq.stream_start())
    # asyncio.run(message_zero_mq.start())

    # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
    # async def main():
    #     await asyncio.gather(producer_push([f"Message {i}" for i in range(5)]), consumer())
    # redis = Redis(host="localhost", port=6379)
    # await redis.set("key", "value")
    # value = await redis.get("key")
    # print(value.decode())  # è¾“å‡º: value
    # await redis.close()

    #
    # asyncio.run(main())

    def test():
        import pprint
        scheduler = TaskScheduler()

        # æ·»åŠ ä»»åŠ¡èŠ‚ç‚¹
        scheduler.set_task_node("task1", {"priority": 1})
        scheduler.set_task_node("task2", {"priority": 2})
        scheduler.set_task_node("task3", {"priority": 3})

        # æ·»åŠ ä¾èµ–å…³ç³»
        scheduler.set_task_dependency(TaskEdge(
            source="task1",
            target="task2",
            condition="done",
            trigger_time={"relative": 1}
        ))

        scheduler.set_task_dependency(TaskEdge(
            source="task2",
            target="task3",
            condition="done",  # è‡ªå®šä¹‰æ¡ä»¶
            rule=lambda: time.localtime().tm_hour < 23  # æ™šä¸Š11ç‚¹å‰æ‰è§¦å‘
        ))

        # å¯åŠ¨ä»»åŠ¡
        scheduler.update_task_status("task1", TaskStatus.COMPLETED)  # "done"

        abs_time = time.time() + 5  # taskY ä¼šå»¶è¿Ÿ 5 ç§’åå˜ä¸º ready å¹¶æ‰§è¡Œ
        scheduler.set_task_node("taskX", {"action": 'X'})
        scheduler.set_task_node("taskY", {"action": 'y'})

        scheduler.set_task_dependency(TaskEdge("taskX", "taskY", condition="done", trigger_time={"absolute": abs_time}))
        scheduler.update_task_status("taskX", TaskStatus.READY)

        # scheduler.set_task_node("A", {})
        # scheduler.set_task_node("B", {})
        # scheduler.set_task_node("C", {})
        #
        # scheduler.add_task_dependency(TaskEdge("A", "B", condition="done"))
        # scheduler.add_task_dependency(TaskEdge("B", "C", condition="done"))
        # scheduler.add_task_dependency(TaskEdge("C", "A", condition="done"))  # é—­ç¯

        print(scheduler.graph)
        pprint.pprint([(v["name"], v["status"]) for v in scheduler.graph.vs])

        scheduler.run_scheduler()

        for v in scheduler.graph.vs:
            print(f"Task {v['name']}: {v['status']}")

        print(scheduler.export_nodes())
        print(scheduler.export_adjacency_list())

        async def test_save():
            await scheduler.set_node(task_node=TaskNode(name='taskX', action='X'))
            await scheduler.set_node(task_node=TaskNode(name='taskY', action='Y'))
            await scheduler.set_edge(task_edge=TaskEdge(source='taskX', target='taskY', condition='done',
                                                        trigger_time={"absolute": abs_time}))

        asyncio.run(test_save())


    test()
