from typing import List, Tuple, Dict, Union, Optional, Any, Callable
import asyncio, aiofiles, aiohttp, httpx
import json, requests, time, os
from pathlib import Path
from openai import AsyncOpenAI
import numpy as np

from agents.ai_prompt import System_content
from utils import build_prompt, create_analyze_messages, deduplicate_functions_by_name, run_togather, get_tokenizer, \
    lang_token_size, generate_hash_key, extract_json_struct, normalize_embeddings, cosine_similarity_np
from service import AI_Client, DB_Client, AliyunBucket, BaseMysql, AsyncAbortController, LRUCache, logger, get_redis, \
    get_httpx_client, post_aiohttp_stream, upload_file_to_oss, upload_file_to_oss_from_file, find_ai_model, \
    async_error_logger, async_polling_check
from database import BaseReBot
from config import Config


@async_error_logger(1)
async def ai_client_completions(messages: list[dict], client: Optional[AsyncOpenAI] = None,
                                model: str = 'deepseek-chat', get_content=True, tools: list[dict] = None,
                                max_tokens: int = 4096, top_p: float = 0.95, temperature: float = 0.1, dbpool=None,
                                **kwargs):
    """
    return: str or æ¨¡å‹å“åº”çš„æ¶ˆæ¯å¯¹è±¡, lastrowid
    """
    chat = True
    if not client:
        try:
            model_info, name = find_ai_model(model, 0, search_field='model')
        except:
            chat = False
            model_info, name = find_ai_model(model, search_field="generation")

        client = AI_Client.get(model_info['name'], None)
        if not client:
            raise ValueError(f"Client for model {model_info['name']}:{model} not found")
        model = name

    reference = kwargs.pop('reference', None)
    payload = dict(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        stream=False,
        **kwargs
    )
    try:
        if chat:
            if tools:
                filter_tools: list[dict] = deduplicate_functions_by_name(tools)
                if any(filter_tools):  # ä¸€æ—¦ä½ ä¼ äº† tools å­—æ®µï¼Œå®ƒ å¿…é¡» è‡³å°‘åŒ…å«ä¸€ä¸ªåˆæ³•çš„ tool
                    payload['tools'] = filter_tools
                    # tool_choice="auto",

            completion = await client.chat.completions.create(**payload)
            if completion is None:
                raise ValueError("OpenAI API returned None instead of a valid response")
            if not completion.choices or not hasattr(completion.choices[0], "message"):
                raise ValueError("Unexpected API response.Incomplete,missing choices or message")
            row_id = await BaseReBot.async_save(model=model, messages=messages, model_response=completion.model_dump(),
                                                reference=reference, dbpool=dbpool or DB_Client,
                                                user='local', agent='chat')
            return (completion.choices[0].message.content if get_content else completion), row_id
        else:
            payload['prompt'] = build_prompt(messages, use_role=False)
            payload.pop('messages', None)
            completion = await client.completions.create(**payload)
            row_id = await BaseReBot.async_save(model=model, messages=messages, model_response=completion.model_dump(),
                                                reference=reference, dbpool=dbpool or DB_Client,
                                                user='local', agent='generate')
            return (completion.choices[0].text if get_content else completion), row_id
    except Exception as e:
        logger.error(f"OpenAI error occurred: {e}")
        raise


async def ai_generate_metadata(function_code: str, metadata: dict = None, model_name=Config.DEFAULT_MODEL_METADATA,
                               description: str = None, code_type: str = "Python", dbpool=None, **kwargs) -> dict:
    if not model_name:
        model_name = Config.DEFAULT_MODEL_METADATA

    prompt = System_content.get('84').format(code_type=code_type.lower(), function_code=function_code)
    lines = [f"å¸®æˆ‘æ ¹æ®å‡½æ•°ä»£ç ç”Ÿæˆæå–å‡½æ•°å…ƒæ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰ã€‚"]
    if metadata:
        lines.append(f"å½“å‰å·²æœ‰åˆå§‹å…ƒæ•°æ®å¦‚ä¸‹ï¼Œè¯·åœ¨æ­¤åŸºç¡€ä¸Šè¡¥å…¨æˆ–ä¿®æ­£:\n{json.dumps(metadata, ensure_ascii=False)}")
    if description:
        lines.append(f"å·¥å…·æè¿°ä¸º:{description}ï¼Œè¯·ç»“åˆæ­¤ä¿¡æ¯å®Œå–„å‡½æ•°ç”¨é€”å’Œè¯´æ˜")

    messages = create_analyze_messages(prompt, "\n".join(lines))
    content, row_id = await ai_client_completions(messages, client=None, model=model_name, get_content=True,
                                                  reference=function_code, max_tokens=1000, temperature=0.3,
                                                  dbpool=dbpool, **kwargs)
    if content:
        parsed = extract_json_struct(content)
        if parsed:
            metadata = parsed
            await BaseReBot.async_save(data={"transform": BaseMysql.format_value(parsed)},
                                       row_id=row_id, dbpool=dbpool or DB_Client)
        else:
            logger.warning(f'metadata extract failed:{content}')

    return metadata or {}


async def ai_batch_run(variable_name: str, variable_values: list, messages: list[dict] = None,
                       model='deepseek-reasoner', max_tokens=4096, temperature: float = 0.2, get_content: bool = True,
                       max_retries: int = 2, max_concurrent: int = Config.MAX_CONCURRENT, dbpool=None, **kwargs):
    '''
    å¤šå˜é‡å¹¶å‘åˆ†æï¼Œå¯¹åŒä¸€æ•°æ®ä½¿ç”¨ä¸åŒå˜é‡å€¼è¿›è¡Œå¤šè§’åº¦åˆ†æ,è¿”å›åˆ†æç»“æœåˆ—è¡¨
    é¡ºåºä¸variable_valueså¯¹åº”,variable_name: è¦å˜åŒ–çš„å˜é‡å(è¦†ç›–)
    '''
    if variable_name == 'model':
        client = None
    else:
        model_info, name = find_ai_model(model, 0, search_field='model')
        client = AI_Client.get(model_info['name'], None)

    payload = {"messages": messages, "model": model, "max_tokens": max_tokens, "temperature": temperature, **kwargs}

    @run_togather(max_concurrent=max_concurrent, batch_size=-1, return_exceptions=True)
    async def _run(item):
        params = {**payload, variable_name: item}
        msg = params.pop("messages", messages)
        response, row_id = await ai_client_completions(msg, client=client, get_content=get_content,
                                                       dbpool=dbpool, **params, max_retries=max_retries)
        result = {"content": response} if get_content else response.model_dump()
        return {**result, f"variable_{variable_name}": item}

    results = await _run(inputs=variable_values)
    return [{'id': i, 'error': str(r)} if isinstance(r, Exception) else {'id': i, **r}
            for i, r in enumerate(results)]


async def ai_analyze(results: dict | list | str, system_prompt: str = None, desc: str = None,
                     model: str = 'deepseek-reasoner', max_tokens: int = 4096, temperature: float = 0.2,
                     dbpool=None, **kwargs):
    reference = None if isinstance(results, str) else results
    user_request = results
    if desc:
        user_request = f"{desc}:\n{user_request}"

    messages = create_analyze_messages(system_prompt, user_request)
    content, _ = await ai_client_completions(messages, client=None, model=model, get_content=True, reference=reference,
                                             max_tokens=max_tokens, temperature=temperature, dbpool=dbpool, **kwargs)
    print(f'</{desc}>: {content}')
    return content.split("</think>")[-1]


async def ai_files_messages(files: List[str], question: str = None, messages: list = None,
                            model_name: str = 'qwen-long', max_tokens: int = 4096, dbpool=None, **kwargs):
    """
    å¤„ç†æ–‡ä»¶å¹¶ç”Ÿæˆ AI æ¨¡å‹çš„å¯¹è¯ç»“æœã€‚

    :param files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    :param question:é—®é¢˜æå–
    :param messages
    :param model_name: æ¨¡å‹åç§°
    :param max_tokens
    :param dbpool
    :return: æ¨¡å‹ç”Ÿæˆçš„å¯¹è¯ç»“æœå’Œæ–‡ä»¶å¯¹è±¡åˆ—è¡¨
    """
    model_info, name = find_ai_model(model_name, -1)
    client = AI_Client.get(model_info['name'], None)
    messages = messages or []
    for file_path in files:
        file_path_obj = Path(file_path)
        if not file_path_obj.exists():  # .is_file()
            continue

        if client:  # purpose=Literal["assistants", "batch", "fine-tune", "vision","batch_output"]
            file_object = await client.files.create(file=file_path_obj, purpose="file-extract")
            if model_info['name'] in ('qwen', 'openai'):  # fileid å¼•ç”¨åè®®
                messages.append({"role": "system", "content": f"fileid://{file_object.id}", })
                # client.files.list()
                # æ–‡ä»¶ä¿¡æ¯ file_info = await client.files.retrieve(file_object.id)
            else:  # ç›´æ¥è¯»å–å†…å®¹æ³¨å…¥åˆ° prompt ä¸­,å†…å®¹å¤§å°æœ‰é™åˆ¶,é€šç”¨æ€§æœ€å¼º,Claudeã€Mistralã€Geminiã€moonshot
                file_content = await client.files.content(file_id=file_object.id)  # æ–‡ä»¶å†…å®¹
                messages.append({"role": "system", "content": file_content.text, })

            await asyncio.sleep(0.03)
        else:
            dashscope_file_upload(messages, file_path=str(file_path_obj))

    if question:
        messages.append({"role": "user", "content": question})
        content, _ = await ai_client_completions(messages, client, model=name, get_content=True, reference=files,
                                                 max_tokens=max_tokens, dbpool=dbpool, **kwargs)
        messages.append({"role": "assistant", "content": content})
        return messages

    return messages


def dashscope_file_upload(messages, file_path='.pdf', api_key=Config.DashScope_Service_Key):
    url = 'https://dashscope.aliyuncs.com/compatible-mode/v1/files'
    headers = {
        # 'Content-Type': 'application/json',
        "Authorization": f'Bearer {api_key}',
    }

    try:
        with open(file_path, 'rb') as f:
            files = {
                'file': f,
                'purpose': (None, 'file-extract'),
            }
            response = requests.post(url, headers=headers, files=files, timeout=Config.HTTP_TIMEOUT_SEC)
            response.raise_for_status()

            file_object = response.json()
            file_id = file_object.get('id', 'unknown_id')

            messages.append({"role": "system", "content": f"fileid://{file_id}"})
            return file_object, file_id

    except Exception as e:
        return {"error": str(e)}, None


async def ai_speech_analyze(file_obj, audio_url: str = None, results: dict = None, system_prompt: str = None,
                            model: str = 'deepseek-reasoner', max_tokens: int = 8192, oss_expires: int = 86400,
                            interval: int = 3, timeout: int = 300, dbpool=None, **kwargs) -> dict:
    from agents.ai_multi import tencent_speech_to_text
    from utils import extract_transcription_segments

    if audio_url:  # å¦‚æœæä¾›äº† audio_urlï¼Œä¼˜å…ˆä½¿ç”¨
        result = await tencent_speech_to_text(None, audio_url, interval, timeout)
    else:
        file_obj.seek(0, os.SEEK_END)
        total_size = file_obj.tell()  # os.path.getsize(file_path)
        file_obj.seek(0)

        if total_size > 1024 * 1024 * 5:  # å¤§æ–‡ä»¶ â†’ OSS URL æ¨¡å¼
            audio_url, _ = await asyncio.to_thread(upload_file_to_oss, AliyunBucket, file_obj, object_name=None,
                                                   expires=oss_expires, total_size=total_size)
            result = await tencent_speech_to_text(None, audio_url, interval, timeout)
        else:
            result = await tencent_speech_to_text(file_obj, None, interval, timeout)

    data = {"audio_url": audio_url}
    if "error" in result:
        data["error"] = result["error"] or "ASR service returned unexpected result"
        return data

    data["asr_task_id"] = result.get("task_id")
    transcription_cleand = extract_transcription_segments(result.get('text'))
    data["transcription"] = transcription_cleand
    if isinstance(results, dict):
        results["æ²Ÿé€šè®°å½•"] = transcription_cleand
        user_request = results
    else:
        user_request = f"æ²Ÿé€šè®°å½•:\n{transcription_cleand}"
    messages = create_analyze_messages(system_prompt, user_request)
    content, _ = await ai_client_completions(messages, client=None, model=model, get_content=True, reference=data,
                                             max_tokens=max_tokens, dbpool=dbpool, **kwargs)
    data["model_completion"] = content
    return data


Active_Stream_Events = AsyncAbortController(redis_client=None)


async def stream_chat_completion(client: AsyncOpenAI, payload: dict, stream_id: str = None):
    payload["stream"] = True
    set_usage = payload.get("stream_options", {}).get("include_usage", False)
    if not set_usage:
        if not isinstance(payload.get("stream_options"), dict):
            payload["stream_options"] = {}
        payload["stream_options"]["include_usage"] = True  # å¯é€‰ï¼Œé…ç½®ä»¥åä¼šåœ¨æµå¼è¾“å‡ºçš„æœ€åä¸€è¡Œå±•ç¤ºtokenä½¿ç”¨ä¿¡æ¯

    # print(payload, client)
    try:
        abort_event = await Active_Stream_Events.set_abort_event(stream_id)
        stream = await client.chat.completions.create(**payload)
        if not stream:
            raise ValueError("OpenAI API returned an empty response")
        if not hasattr(stream, "__aiter__"):
            raise TypeError("OpenAI API returned a non-streaming response")

        reasoning_content = ''
        assistant_content = []  # answer_content
        completion_chunk = None
        async for chunk in stream:
            if not chunk:
                continue

            completion_chunk = chunk
            if abort_event and abort_event.is_set():  # è¢« poll_abort è§¦å‘ or should_abort_async
                yield '[DONE]', chunk.model_dump_json()
                await stream.close()
                logger.warning(f"æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œå·²åœæ­¢æµ: {stream_id}")
                break  # return

            # è‹¥ choices ä¸ºç©ºä½†åŒ…å« usageï¼Œè¯´æ˜æ˜¯ stream æœ«å°¾æ•°æ®,qwen
            if not chunk.choices:
                if set_usage:
                    yield None, chunk.model_dump_json()  # '[DONE]' usage
                if assistant_content:
                    break
                else:
                    continue

            delta = chunk.choices[0].delta
            if delta.content:  # ä»¥ä¸¤ä¸ªæ¢è¡Œç¬¦ \n\n ç»“æŸå½“å‰ä¼ è¾“çš„æ•°æ®å—
                assistant_content.append(delta.content)
                yield delta.content, chunk.model_dump_json()  # è·å–å­—èŠ‚æµæ•°æ®

            if hasattr(delta, "reasoning_content") and delta.reasoning_content:
                reasoning_content += delta.reasoning_content
                yield delta.reasoning_content, chunk.model_dump_json()

        if not assistant_content and not reasoning_content:
            raise ValueError("OpenAI API returned an empty stream response")
        if completion_chunk:
            completion = completion_chunk.model_dump()
            completion.update({
                "object": "chat.completion",
                "choices": [
                    {
                        "index": 0,
                        "message": {"role": "assistant", "content": ''.join(assistant_content),
                                    "reasoning_content": reasoning_content},
                        "finish_reason": "stop",  # "length"
                        # "metadata": {"reasoning": reasoning}  # .split("</think>")[-1]
                    }
                ],
            })
            yield None, json.dumps(completion)

    except Exception as e:
        logger.error(f"OpenAI error:{e}, {payload}")
        error_message = f"OpenAI error occurred: {e}"
        fake_response = {
            "id": f"chatcmpl-{int(time.time())}", "object": "chat.completion.chunk",
            "created": int(time.time()), "model": payload.get("model", "unknown"),
            "choices": [{"index": 0, "delta": {"content": error_message}, "finish_reason": "stop"}],
        }
        yield error_message, json.dumps(fake_response)

    finally:
        if stream_id:
            await Active_Stream_Events.delete(stream_id)


async def ai_try(client: AsyncOpenAI, payload: dict, e: Exception) -> Tuple[str, Dict]:
    """
        å°è£… BadRequestError çš„é”™è¯¯ä¿¡æ¯è§£æå’Œä¿®æ­£å¤„ç†é€»è¾‘ã€‚
        # å°è¯•è§£æç»“æ„åŒ– JSON é”™è¯¯ä¿¡æ¯
        # This model only supports streaming responses. Please set stream=True.
        # Rate limit reached for TPM
        # Model disabled
        # Unsupported model
        # Invalid URL
        # does not exist or you do not have access to it
        # Authenticaton failed, please make sure that a valid ModelScope token is supplied.
        # You exceeded your current quota, please check your plan and billing details.
        # parameter.enable_thinking must be set to false for non-streaming calls
        # openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter_error', 'param': None, 'message': 'This model only support stream mode, please enable the stream parameter to access the model. ', 'type': 'invalid_request_error'}, 'id': 'chatcmpl-4c7a91c5-c35c-9e94-8ff8-6e1b09a8a151', 'request_id': '4c7a91c5-c35c-9e94-8ff8-6e1b09a8a151'}
        # openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter_error', 'param': None, 'message': '<400> InternalError.Algo.InvalidParameter: Range of input length should be [1, 3072]', 'type': 'invalid_request_error'},
        # openai.BadRequestError: Error code: 400 - {'error': {'message': 'Invalid max_tokens value, the valid range of max_tokens is [1, 8192]', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}

       Args:
           e: æ•è·çš„ BadRequestError å¼‚å¸¸å¯¹è±¡
           client:
           payload:æ¨¡å‹å‚æ•°

       Returns:
           fallback_callable çš„æ‰§è¡Œç»“æœï¼ˆå†…å®¹æˆ–å®Œæ•´å“åº”ï¼‰

       Raises:
           åŸå§‹å¼‚å¸¸æˆ– ValueErrorï¼ˆå½“è¾“å…¥é•¿åº¦è¿‡å¤§ï¼‰
       """
    try:
        if hasattr(e, "response") and hasattr(e.response, "json"):
            error_json = e.response.json()
        elif hasattr(e, "response") and hasattr(e.response, "text"):
            error_json = json.loads(e.response.text)
        else:
            error_json = {}

        error_message = error_json.get("error", {}).get("message", "") or str(e)
    except Exception:
        error_message = str(e)

    logger.error(f"[BadRequestError] æ•è·é”™è¯¯æ¶ˆæ¯ï¼š{error_message}")

    stream_required_phrases = [
        "only support stream mode",
        "only supports streaming",
        "non-streaming calls",  # +/no_think
        "stream=true",
        "æ¨¡å‹ä¸æ”¯æŒsyncè°ƒç”¨"
    ]
    length_required_phrases = [
        "range of input length",
        "input length should be",
        "max input characters",
        "invalid max_tokens value",
        "the max_tokens must be less than"
    ]
    not_exists_phrases = ['model not exists', 'invalid model id']

    error_msg = error_message.lower()

    if any(p in error_msg for p in stream_required_phrases):
        payload["extra_body"] = {"enable_thinking": True}
        try:
            last_value = None
            async for chunk in stream_chat_completion(client, payload):
                last_value = chunk
            if last_value is None:
                raise RuntimeError("[stream_chat_completion] No data received from stream.")
            _, completion = last_value
            completion_data = json.loads(completion)
            content = completion_data.get('choices', [{}])[0].get('message', {}).get('content')
            await BaseReBot.async_save(model=payload.get('model'), messages=payload.get('messages', []),
                                       assistant_content=content, model_response=completion_data, dbpool=DB_Client,
                                       user='local', agent='chat', robot_id='stream_chat')
        except Exception as e:
            raise RuntimeError(f"[stream_chat_completion] Failed to complete streaming call: {e}") from e
        return content, completion_data
    elif any(p in error_msg for p in length_required_phrases):
        raise ValueError(f"[InputTokenError] è¾“å…¥è¶…è¿‡æ¨¡å‹é™åˆ¶æˆ–æ— æ•ˆï¼š{error_message}")
        # payload['message']=cut_chat_history(payload['message'], max_size=33000)
    elif any(p in error_msg for p in not_exists_phrases):
        raise ValueError(f"[InputModelError] æ¨¡å‹æ— æ•ˆ<{payload.get('model')}>ï¼š{error_message}")
    else:
        # å†…éƒ¨ raise ä¼šç»§ç»­è¢«ä¸‹ä¸€ä¸ª except æ•è·
        raise e


async def ai_batch(inputs: List[list[dict] | tuple[str, str]], task_id: str, model: str = 'qwen-long',
                   search_field: str = 'model', **kwargs):
    model_info, name = find_ai_model(model, -1, search_field)
    endpoint_map = {  # é€‰æ‹©Embeddingæ–‡æœ¬å‘é‡æ¨¡å‹è¿›è¡Œè°ƒç”¨æ—¶ï¼Œurlçš„å€¼éœ€å¡«å†™"/v1/embeddings",å…¶ä»–æ¨¡å‹å¡«å†™/v1/chat/completions
        "model": "/v1/chat/completions",
        "embedding": "/v1/embeddings",
        "generation": "/v1/completions"
    }
    endpoint = endpoint_map.get(search_field, "/v1/chat/completions")
    input_filename = f'{task_id}_input.jsonl'
    file_path = Path(Config.DATA_FOLDER) / input_filename
    async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
        for i, msg in enumerate(inputs):
            messages = create_analyze_messages(msg[0], msg[1]) if isinstance(msg, tuple) else msg
            body = {"model": name, "messages": messages, **kwargs}
            request = {"custom_id": i, "method": "POST", "url": endpoint, "body": body}
            await f.write(json.dumps(request, separators=(',', ':'), ensure_ascii=False) + "\n")

    client = AI_Client.get(model_info['name'], None)
    # Step 1: ä¸Šä¼ åŒ…å«è¯·æ±‚ä¿¡æ¯çš„JSONLæ–‡ä»¶,å¾—åˆ°è¾“å…¥æ–‡ä»¶ID,å¦‚æœæ‚¨éœ€è¦è¾“å…¥OSSæ–‡ä»¶,å¯å°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šinput_file_id = "å®é™…çš„OSSæ–‡ä»¶URLæˆ–èµ„æºæ ‡è¯†ç¬¦"
    file_object = await client.files.create(file=file_path, purpose="batch")
    # Step 2: åŸºäºè¾“å…¥æ–‡ä»¶ID,åˆ›å»ºBatchä»»åŠ¡
    batch = await client.batches.create(input_file_id=file_object.id, endpoint=endpoint, completion_window="24h")
    return {
        "input_file_id": file_object.id,
        "batch_id": batch.id,
        "input_file": input_filename,
        "client_name": model_info['name']
    }


async def ai_batch_result(batch_id: str, task_id: str, client_name: str,
                          interval: int = 10,  # æ¯æ¬¡æŸ¥è¯¢é—´éš”ç§’æ•°
                          timeout: int = 3600,  # æœ€é•¿ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 1 å°æ—¶
                          oss_expires: int = 86400
                          ):
    """
    è½®è¯¢æ‰¹å¤„ç†ä»»åŠ¡çŠ¶æ€ï¼Œç­‰å¾…å®Œæˆåä¸‹è½½ç»“æœæ–‡ä»¶ã€‚
    - batch_id: æ‰¹å¤„ç†ä»»åŠ¡ID
    - client_name: AI_Client çš„ key
    - task_id: ç”¨æ¥å‘½åè¾“å‡ºæ–‡ä»¶ï¼Œé¿å…æ··æ·†
    - interval: è½®è¯¢é—´éš”ç§’æ•°ï¼Œ<=0 ä¸è½®è¯¢
    - timeout: è¶…æ—¶æ—¶é—´ï¼Œé˜²æ­¢æ°¸è¿œç­‰å¾…ï¼Œ<=0 æ°¸è¿œç­‰å¾…
    """
    client = AI_Client.get(client_name, None)
    if not client:
        raise ValueError(f"Client '{client_name}' not found")

    async def check_batch_status(_future=None):
        """
        è½®è¯¢æ‰¹å¤„ç†çŠ¶æ€ï¼Œä»…è¿”å›çŠ¶æ€æŒ‡ç¤º
        """
        batch = await client.batches.retrieve(batch_id=batch_id)
        status = batch.status

        if status in ["completed", "failed", "expired", "cancelled"]:
            # ç»“æŸçŠ¶æ€ â†’ è¿”å›ç»“æœå¯¹è±¡
            print(f"[BatchçŠ¶æ€] {batch_id}: {status}")
            return batch

        if interval <= 0:
            print(batch.model_dump())
            _future.set_result(batch)
            return batch

        return False  # ç»§ç»­è½®è¯¢

    polling_func = async_polling_check(interval=interval, timeout=timeout)(check_batch_status)
    future, handle = await polling_func()
    try:
        batch = await future  # ç­‰å¾…è½®è¯¢å®Œæˆ
    except TimeoutError:
        return {"status": "timeout"}
    except Exception as e:
        logger.exception(f"[Batchå¼‚å¸¸] {batch_id} å¼‚æ­¥è½®è¯¢å‡ºé”™: {e}")
        return {"status": "error", "error": str(e)}
    finally:
        handle['cancelled'] = True

    # âœ… ä»»åŠ¡å®Œæˆï¼Œä¸‹è½½ç»“æœæ–‡ä»¶
    status = batch.status
    result = {"status": status, "request_counts": batch.request_counts.model_dump()}

    if status == "failed":
        result["errors"] = str(batch.errors)
        return result

    output_filename = f'{task_id}_{batch_id}_result.jsonl'
    error_filename = f'{task_id}_{batch_id}_error.jsonl'
    output_file_path = Path(Config.DATA_FOLDER) / output_filename
    error_file_path = Path(Config.DATA_FOLDER) / error_filename
    if batch.error_file_id and not error_file_path.exists():
        content = await client.files.content(batch.error_file_id)
        logger.error(f"[Batch error] {batch_id} å‰1000å­—ç¬¦:\n{content.text[:1000]}...\n")
        content.write_to_file(error_file_path)
        result["error_file"] = error_filename
        result["error_file_id"] = batch.error_file_id
        result["batch_id"] = batch.id or batch_id

    if batch.output_file_id and not output_file_path.exists():
        content = await client.files.content(batch.output_file_id)
        print(f"[Batch result] {batch_id} å‰1000å­—ç¬¦:\n{content.text[:1000]}...\n")
        content.write_to_file(output_file_path)
        result["result_file"] = output_filename
        result["output_file_id"] = batch.output_file_id
        result["batch_id"] = batch.id or batch_id
        if oss_expires != 0:
            object_name = f"upload/{output_filename}"
            result["output_file_url"], _ = await asyncio.to_thread(
                upload_file_to_oss_from_file,
                AliyunBucket,
                output_file_path, object_name, oss_expires)
            # os.remove(output_file_path)

    return result


Embedding_Cache = LRUCache(1000, redis=get_redis(), prefix="embedding", expire_sec=Config.REDIS_CACHE_SEC)


# https://www.openaidoc.com.cn/docs/guides/embeddings
async def ai_embeddings(inputs: Union[str, List[str], Tuple[str], List[Dict[str, str]]],
                        model_name: str = Config.DEFAULT_MODEL_EMBEDDING,
                        model_id: int = 0, get_embedding: bool = True, normalize: bool = False,
                        **kwargs) -> Union[List[List[float]], Dict]:
    """
    text = text.replace("\n", " ")
    æ£€æŸ¥ç¼“å­˜ï¼Œå¦‚æœæ²¡æœ‰å°±è®¡ç®—åµŒå…¥
    ä»è¿œç¨‹æœåŠ¡è·å–åµŒå…¥ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†å’Œç¼“å­˜å’Œå¤šæ¨¡å‹å¤„ç†ã€‚
    :param inputs: è¾“å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
    :param model_name: æ¨¡å‹åç§°
    :param model_id: æ¨¡å‹åºå·
    :param get_embedding: è¿”å›å“åº”æ•°æ®ç±»å‹
    :param normalize: å½’ä¸€åŒ–
    :return: åµŒå…¥åˆ—è¡¨
    ï¼ˆ1ï¼‰æ–‡æœ¬æ•°é‡ä¸è¶…è¿‡ 16ã€‚
    ï¼ˆ2ï¼‰æ¯ä¸ªæ–‡æœ¬é•¿åº¦ä¸è¶…è¿‡ 512 ä¸ª tokenï¼Œè¶…å‡ºè‡ªåŠ¨æˆªæ–­ï¼Œtoken ç»Ÿè®¡ä¿¡æ¯ï¼Œtoken æ•° = æ±‰å­—æ•°+å•è¯æ•°*1.3 ï¼ˆä»…ä¸ºä¼°ç®—é€»è¾‘ï¼Œä»¥å®é™…è¿”å›ä¸ºå‡†)ã€‚
    ï¼ˆ3ï¼‰ æ‰¹é‡æœ€å¤š 16 ä¸ªï¼Œè¶…è¿‡ 16 åé»˜è®¤æˆªæ–­ã€‚
    """
    if not model_name:
        return []
    if not inputs:
        return []

    redis = get_redis()
    cache_key, embedding = await Embedding_Cache.get_cache(args=[model_name, model_id, get_embedding, normalize],
                                                           redis=redis, default=[], inputs=inputs)
    if embedding:
        # print(f"Embedding already cached for key: {cache_key}")
        if isinstance(embedding, list) and all(isinstance(vec, list) for vec in embedding):
            return embedding

    try:
        model_info, name = find_ai_model(model_name, model_id, 'embedding')
    except Exception as e:
        print(e)
        return []

    batch_size = 10  # DASHSCOPE_MAX_BATCH_SIZE = 16,25
    has_error = False
    payload = dict(
        model=name,
        encoding_format="float",
        # "dimensions":# æŒ‡å®šå‘é‡ç»´åº¦ï¼ˆtext-embedding-v3åŠ text-embedding-v4ï¼‰
    )
    payload.update(kwargs)

    client = AI_Client.get(model_info['name'], None)
    if client:  # openai.Embedding.create
        if isinstance(inputs, (list, tuple)) and len(inputs) > batch_size:
            create_embeddings = run_togather(max_concurrent=Config.MAX_CONCURRENT, batch_size=batch_size,
                                             input_key="input")(client.embeddings.create)
            results = await create_embeddings(inputs=inputs, **payload)
            if not get_embedding:
                results_data = results[0]
                all_data = []
                for i, result in enumerate(results):
                    for idx, item in enumerate(result.data):
                        item.index = len(all_data)
                        all_data.append(item)

                    if i > 0:
                        results_data.usage.prompt_tokens += result.usage.prompt_tokens
                        results_data.usage.total_tokens += result.usage.total_tokens

                results_data.data = all_data
                return results_data.model_dump()

            embeddings = [None] * len(inputs)
            input_idx = 0
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"Error encountered: {result}")
                    has_error = True
                    continue
                for idx, item in enumerate(result.data):
                    embeddings[input_idx] = item.embedding
                    input_idx += 1
        else:
            # await asyncio.to_thread(client.embeddings.create
            results = await client.embeddings.create(input=inputs, **payload)

            if not get_embedding:
                return results.model_dump()

            embeddings = [item.embedding for item in results.data]

        if normalize and not any(embedding is None for embedding in embeddings):
            embeddings = normalize_embeddings(embeddings, to_list=True)

        if not has_error:  # and len(embeddings) > batch_size
            await Embedding_Cache.set_cache(cache_key, embeddings, redis=redis)

        return embeddings

    # dashscope.TextEmbedding.call
    url = model_info['embedding_url'] if model_info.get('embedding_url') else model_info['base_url'] + '/embeddings'
    api_key = model_info['api_key']
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    payload["input"]: list | str = inputs
    embeddings = []
    cx = get_httpx_client()
    try:
        if isinstance(inputs, (list, tuple)) and len(inputs) > batch_size:
            for i in range(0, len(inputs), batch_size):
                batch = inputs[i:i + batch_size]
                payload["input"] = batch  # {"texts":batch}
                response = await cx.post(url, headers=headers, json=payload, timeout=Config.LLM_TIMEOUT_SEC)
                response.raise_for_status()
                data = response.json().get('data')  # "output"
                if data and len(data) == len(batch):
                    embeddings += [emb.get('embedding') for emb in data]
                else:
                    print(f"Error: Batch {i // batch_size + 1} response size mismatch.")
                    has_error = True
        else:
            response = await cx.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json().get('data')
            embeddings = [emb.get('embedding') for emb in data]

    except (httpx.HTTPStatusError, httpx.RequestError) as exc:
        print(exc)

    if normalize:
        embeddings = normalize_embeddings(embeddings, to_list=True)

    if not has_error:
        await Embedding_Cache.set_cache(cache_key, embeddings, redis=redis)

    return embeddings


@async_error_logger(1)
async def ai_reranker(query: str, documents: List[str], top_n: int, model_name="BAAI/bge-reranker-v2-m3", model_id=0,
                      **kwargs):
    if not model_name:
        return []
    model_info, name = find_ai_model(model_name, model_id, 'reranker')

    url = model_info['reranker_url']
    api_key = model_info.get('api_key')
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    payload = {
        "query": query,
        "model": name,
        "documents": documents,
        "top_n": top_n,
        "return_documents": False,
        # 'max_chunks_per_doc': 1024,  # æœ€å¤§å—æ•°
        # 'overlap_tokens': 80,  # é‡å æ•°é‡
    }
    payload.update(kwargs)
    async with httpx.AsyncClient(timeout=Config.HTTP_TIMEOUT_SEC) as cx:
        response = await cx.post(url, headers=headers, json=payload)
        if response.status_code == 200:
            results = response.json().get('results')
            matches = [(match["document"].get("text", match["document"])
                        if match.get("document") else documents[match["index"]],
                        match["relevance_score"], match["index"])
                       for match in results]
            return matches
        else:
            print(response.text)
    return []


async def similarity_score_embeddings(query, tokens: List[str], filter_idx: List[int] = None,
                                      tokens_vector: List[float] = None,
                                      embeddings_calls: Callable[[...], Any] = ai_embeddings, **kwargs):
    """
    è®¡ç®—æŸ¥è¯¢ä¸ä¸€ç»„æ ‡è®°ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚
    å‚æ•°:
        query (str): æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚
        tokens (List[str]): è¦æ¯”è¾ƒçš„æ ‡è®°åˆ—è¡¨ã€‚
        filter_idx (List[int], optional): è¦è¿‡æ»¤å‡ºçš„æ ‡è®°ç´¢å¼•ã€‚é»˜è®¤ä¸º `None`ï¼Œè¡¨ç¤ºä¸è¿›è¡Œè¿‡æ»¤ã€‚
        tokens_vector (np.ndarray, optional): é¢„è®¡ç®—çš„æ ‡è®°å‘é‡æ•°ç»„ã€‚é»˜è®¤ä¸º `None`ã€‚
        embeddings_calls (Callable, optional): ç”¨äºç”ŸæˆåµŒå…¥çš„å¼‚æ­¥å‡½æ•°ï¼Œé»˜è®¤ä¸º `None`ã€‚
        **kwargs: ä¼ é€’ç»™ `embeddings_calls` çš„é¢å¤–å‚æ•°ã€‚

    è¿”å›:
        np.ndarray: ç›¸ä¼¼åº¦å¾—åˆ†æ•°ç»„ï¼ˆä¸ `tokens` é•¿åº¦ä¸€è‡´ï¼‰ã€‚
    """
    if filter_idx is None:
        filter_idx = list(range(len(tokens)))
    else:
        filter_idx = [i for i in filter_idx if i < len(tokens)]
    similarity = np.full(len(tokens), np.nan)
    if not query:
        return similarity

    tokens_idx = [(i, token) for i, token in enumerate(tokens) if token]
    query_vector = await embeddings_calls(query, **kwargs)
    if tokens_vector is None:
        # list(np.array(idx_tokens)[:, 1])
        tokens_vector = await embeddings_calls([token for _, token in tokens_idx], **kwargs)

    matching_indices = np.isin([j[0] for j in tokens_idx], filter_idx)
    filter_embeddings = np.array(tokens_vector)[matching_indices]

    if len(filter_embeddings):
        # cosine_similarity_np(np.array(query_vector).reshape(1, -1), filter_embeddings).T
        sim_2d = np.array(query_vector).reshape(1, -1) @ filter_embeddings.T
        # matching_indices = np.isin(filter_idx, [j[0] for j in tokens_idx])
        # similarity[matching_indices] = sim_2d.reshape(-1)
        for idx, score in zip(filter_idx, sim_2d.flatten()):
            similarity[idx] = score

    return similarity


async def get_similar_embeddings(querys: Union[str, List[str]], tokens: List[str], topn: int = 10,
                                 embeddings_calls: Callable[[...], Any] = ai_embeddings,
                                 **kwargs) -> List[Tuple[str, List[Tuple[str, float, int]]]]:
    """
    ä½¿ç”¨åµŒå…¥è®¡ç®—æŸ¥è¯¢ä¸æ ‡è®°ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚
    :param querys: æŸ¥è¯¢è¯åˆ—è¡¨
    :param tokens: æ¯”è¾ƒè¯åˆ—è¡¨
    :param embeddings_calls: åµŒå…¥ç”Ÿæˆå‡½æ•°
    :param topn: è¿”å›ç›¸ä¼¼ç»“æœçš„æ•°é‡
    :param kwargs: å…¶ä»–å‚æ•°
    è¿”å›ï¼š
        List[Tuple[str, List[Tuple[str, float,int]]]]: æŸ¥è¯¢è¯ä¸ç›¸ä¼¼æ ‡è®°åŠå…¶åˆ†æ•°çš„æ˜ å°„ã€‚
    """
    if isinstance(querys, str):
        querys = [querys]

    query_vector, tokens_vector = await asyncio.gather(
        embeddings_calls(querys, **kwargs),
        embeddings_calls(tokens, **kwargs))

    if not query_vector or not tokens_vector:
        return []

    sim_matrix = np.array(query_vector) @ np.array(tokens_vector).T
    results = []
    for i, w in enumerate(querys):
        sim_scores = sim_matrix[i]
        top_indices = np.argsort(sim_scores)[::-1][:topn] if topn > 0 else np.arange(sim_scores.shape[0])
        top_scores = sim_scores[top_indices]
        top_match = [tokens[j] for j in top_indices]
        results.append((w, list(zip(top_match, top_scores, top_indices))))

    return results  # [(q,[(match,score,index),])]


def get_similar_words(querys: List[str], data: dict, exclude: List[str] = None, topn: int = 10,
                      cutoff: float = 0.0) -> List[Tuple[str, List[Tuple[str, float]]]]:
    '''
   è®¡ç®—æŸ¥è¯¢è¯ä¸æ•°æ®ä¸­çš„è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå¹¶è¿”å›æ¯ä¸ªæŸ¥è¯¢è¯çš„æœ€ç›¸ä¼¼è¯åŠå…¶ç›¸ä¼¼åº¦ã€‚

    :param querys: æŸ¥è¯¢è¯åˆ—è¡¨
    :param data: åŒ…å«è¯æ±‡å’Œå¯¹åº”å‘é‡çš„å­—å…¸ï¼Œæ ¼å¼ä¸ºï¼š
                 {
                     "name": ["word1", "word2", ...],
                     "vectors": [[0.1, 0.2, ...], [0.3, 0.4, ...], ...]
                 }
    :param exclude: è¦æ’é™¤çš„è¯åˆ—è¡¨ï¼Œè¿™äº›è¯ä¸ä¼šå‡ºç°åœ¨ç»“æœä¸­
    :param topn: è¿”å›æœ€ç›¸ä¼¼çš„å‰ n ä¸ªè¯
    :param cutoff: åªæœ‰ç›¸ä¼¼åº¦å¤§äºè¯¥å€¼çš„è¯æ‰ä¼šè¢«è¿”å›
    :return: æ¯ä¸ªæŸ¥è¯¢è¯çš„ç›¸ä¼¼è¯å’Œç›¸ä¼¼åº¦çš„å…ƒç»„åˆ—è¡¨
    è¿”å›ï¼š
        List[Tuple[str, List[Tuple[str, float]]]]: æŸ¥è¯¢è¯ä¸ç›¸ä¼¼æ ‡è®°åŠå…¶åˆ†æ•°çš„æ˜ å°„ã€‚
    '''
    index_to_key = data['name']
    vectors = np.array(data['vectors'])

    # è·å–ç´¢å¼•
    query_mask = np.array([w in index_to_key for w in querys])
    exclude_mask = np.array([w in querys + (exclude or []) for w in index_to_key])
    # np.delete(vectors, exclude_indices, axis=0)

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    sim_matrix = cosine_similarity_np(vectors[query_mask], vectors[~exclude_mask].T)

    results = []
    for i, w in enumerate(querys):
        if not query_mask[i]:
            continue
        sim_scores = sim_matrix[i]
        if topn > 0:
            top_indices = np.argsort(sim_scores)[::-1][:topn]  # è·å–å‰ topn ä¸ªç´¢å¼•
        else:
            top_indices = np.arange(sim_scores.shape[0])  # è·å–å…¨éƒ¨ç´¢å¼•ï¼Œä¸æ’åº
        top_scores = sim_scores[top_indices]

        valid_indices = top_scores > cutoff  # ä¿ç•™å¤§äº cutoff çš„ç›¸ä¼¼åº¦
        top_words = [index_to_key[j] for j in np.where(~exclude_mask)[0][top_indices[valid_indices]]]
        top_scores = top_scores[valid_indices]
        results.append((w, list(zip(top_words, top_scores))))

    return results  # [(w, list(sim[w].sort_values(ascending=False)[:topn].items())) for w in querys]


async def find_closest_matches_embeddings(querys: List[str], tokens: List[str],
                                          embeddings_calls: Callable[[...], Any] = ai_embeddings,
                                          **kwargs) -> Dict[str, Tuple[str, float]]:
    """
    ä½¿ç”¨åµŒå…¥è®¡ç®—æŸ¥è¯¢ä¸æ ‡è®°ä¹‹é—´çš„æœ€è¿‘åŒ¹é…ï¼Œæ‰¾åˆ°æ¯ä¸ªæŸ¥è¯¢çš„æœ€ä½³åŒ¹é…æ ‡è®°,topk=1ã€‚

    :param querys: æŸ¥è¯¢å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
    :param tokens: æ ‡è®°åˆ—è¡¨ï¼Œç”¨äºåŒ¹é…æŸ¥è¯¢ã€‚
    :param embeddings_calls: åµŒå…¥ç”Ÿæˆå‡½æ•°ï¼Œæ¥æ”¶æŸ¥è¯¢å’Œæ ‡è®°è¿”å›åµŒå…¥å‘é‡ï¼ˆé»˜è®¤ä¸º ai_embeddingsï¼‰ã€‚
    :param kwargs: å…¶ä»–å‚æ•°ï¼Œä¼ é€’ç»™ embeddings_calls å‡½æ•°ã€‚
    :return: è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­æ¯ä¸ªæŸ¥è¯¢çš„æœ€ä½³åŒ¹é…æ ‡è®°åŠå…¶ç›¸ä¼¼åº¦åˆ†æ•°è¢«æ˜ å°„åˆ°æŸ¥è¯¢ä¸Šã€‚
    è¿”å›ï¼š
        Dict[str, Tuple[str, float]]: æŸ¥è¯¢ä¸æœ€è¿‘åŒ¹é…æ ‡è®°çš„æ˜ å°„å­—å…¸ã€‚
    """
    matches = {x: (x, 1.0) for x in querys if x in tokens}
    unmatched_queries = list(set(querys) - matches.keys())
    # æ‰€æœ‰æŸ¥è¯¢éƒ½å·²åŒ¹é…ï¼Œç›´æ¥è¿”å›å®Œå…¨åŒ¹é…é¡¹
    if not unmatched_queries:
        return matches
    query_vector, tokens_vector = await asyncio.gather(
        embeddings_calls(unmatched_queries, **kwargs),
        embeddings_calls(tokens, **kwargs))
    sim_matrix = np.array(query_vector) @ np.array(tokens_vector).T

    closest_matches = tokens[sim_matrix.argmax(axis=1)]  # idxmax
    closest_scores = sim_matrix.max(axis=1)
    matches.update(zip(unmatched_queries, zip(closest_matches, closest_scores)))
    return matches


async def process_llm_stream(llm_responses_stream, token_size=20, model_name="gpt-3.5-turbo"):
    """
    å¤„ç†å¤§æ¨¡å‹è¿”å›çš„æ–‡æœ¬æµï¼Œå¹¶æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²äº¤ç»™ TTS æœ—è¯»ã€‚
    :param llm_responses_stream: å¤§æ¨¡å‹è¿”å›çš„æ–‡æœ¬æµ
    :param token_size: æ ‡ç‚¹ä¸è¶³æ—¶ï¼Œå…è®¸çš„æœ€å°ç¼“å†²åŒºé•¿åº¦
    :param  model_name: tokenizer model
    """
    from utils.utils import find_last_punctuation, get_string_no_punctuation_or_emoji
    response_message = []
    text_index = 0
    processed_chars = 0
    tokenizer = get_tokenizer(model_name)
    async for content in llm_responses_stream:
        response_message.append(content)
        if Active_Stream_Events.should_abort(stream_id='LLM_Controller'):  # å®æ—¶æ£€æŸ¥æ˜¯å¦ç»ˆæ­¢
            break

        # è·å–å½“å‰æœªå¤„ç†çš„æ–‡æœ¬
        full_text = "".join(response_message)
        current_text = full_text[processed_chars:]

        # æŸ¥æ‰¾æœ€åä¸€ä¸ªæœ‰æ•ˆæ ‡ç‚¹
        last_punct_pos = find_last_punctuation(current_text)
        if last_punct_pos != -1 or lang_token_size(current_text, tokenizer) > token_size:
            split_pos = last_punct_pos if last_punct_pos != -1 else token_size  # é€‰å–æœ€åˆé€‚çš„åˆ‡å‰²ç‚¹
            segment_text_raw = current_text[:split_pos + 1]
            segment_text = get_string_no_punctuation_or_emoji(segment_text_raw)  # å¤„ç†æ— æ•ˆå­—ç¬¦
            if segment_text:
                text_index += 1
                yield segment_text, text_index
                processed_chars += len(segment_text_raw)  # æ›´æ–°å·²å¤„ç†å­—ç¬¦ä½ç½®

    # å¤„ç†å‰©ä½™æœªåˆ†å‰²çš„æ–‡æœ¬
    remaining_text = "".join(response_message)[processed_chars:]
    if remaining_text:
        segment_text = get_string_no_punctuation_or_emoji(remaining_text)
        if segment_text:
            text_index += 1
            yield segment_text, text_index

    yield response_message, -1  # finish_task


async def start_llm_stream(new_llm_stream):
    """å¤ä½ç»ˆæ­¢ä¿¡å·ï¼Œå¹¶é‡æ–°å¯åŠ¨å¤§æ¨¡å‹æµ"""
    abort_event = Active_Stream_Events.get_abort_event(stream_id='LLM_Controller')
    if not abort_event:
        abort_event = await Active_Stream_Events.set_abort_event(stream_id='LLM_Controller')
    abort_event.clear()  # é‡æ–°å¯åŠ¨å‰å¤ä½
    async for text, idx in process_llm_stream(new_llm_stream):
        if idx > 0:
            print(f"ğŸ”Š æœ—è¯»: {text}")


async def call_ollama(prompt: str | list[str] = None, messages: list[dict] = None, model_name="mistral",
                      host='localhost', time_out: float = 100, stream=True, tools: list[dict] = None,
                      embed: bool = False, **kwargs):
    """
    https://github.com/ollama/ollama/blob/main/docs/api.md
    è¿”å›ä¸¤ç§æ¨¡å¼çš„ç»“æœï¼šasync generatorï¼ˆstreamï¼‰æˆ– JSON dictï¼ˆé streamï¼‰
    """
    if not prompt and not messages:
        raise ValueError("å¿…é¡»æä¾› prompt æˆ– messages ä¹‹ä¸€")
    # api/tags:åˆ—å‡ºæœ¬åœ°æ¨¡å‹,/api/ps:åˆ—å‡ºå½“å‰åŠ è½½åˆ°å†…å­˜ä¸­çš„æ¨¡å‹

    if embed:
        url = f"http://{host}:11434/api/embed"
        payload = {"model": model_name, "input": prompt}
    elif messages is not None:
        url = f'http://{host}:11434/api/chat'
        payload = {
            "model": model_name,
            "messages": messages,
            # å¦‚æœ messages æ•°ç»„ä¸ºç©ºï¼Œåˆ™æ¨¡å‹å°†è¢«åŠ è½½åˆ°å†…å­˜ä¸­, [{"role": "user","content": "Hello!","images": ["..."]}]
            "stream": stream
        }
        if tools:
            payload['tools'] = tools
    else:
        if isinstance(prompt, list):
            prompt = "\n".join(prompt)

        url = f"http://{host}:11434/api/generate"
        payload = {"model": model_name, "prompt": prompt, "stream": stream
                   # "suffix": "    return result",æ¨¡å‹å“åº”åçš„æ–‡æœ¬
                   # "images"ï¼šè¦åŒ…å«åœ¨æ¶ˆæ¯ä¸­çš„å›¾åƒåˆ—è¡¨ï¼ˆå¯¹äºå¤šæ¨¡æ€æ¨¡å‹ï¼Œä¾‹å¦‚llava)
                   # "keep_alive": æ§åˆ¶æ¨¡å‹åœ¨è¯·æ±‚ååŠ è½½åˆ°å†…å­˜ä¸­çš„æ—¶é—´ï¼ˆé»˜è®¤å€¼ï¼š5m)
                   # "keep_alive": 0ï¼Œå¦‚æœæä¾›äº†ç©ºæç¤ºï¼Œå°†ä»å†…å­˜ä¸­å¸è½½æ¨¡å‹
                   # "format": "json",
                   # "options": {
                   #     "temperature": 0,"seed": 123ï¼Œ"top_k": 20,"top_p": 0.9, "stop": ["\n", "user:"],
                   #  "num_batch": 2,"num_gpu": 1,"main_gpu": 0,"use_mmap": true,"num_thread": 8
                   # },
                   }

    payload.update(**kwargs)

    if stream:
        return post_aiohttp_stream(url, payload, time_out)
        # content=''
        # async for chunk in post_aiohttp_stream(url,payload):
        #     content += chunk.get("response", "")
        # return content
    timeout = aiohttp.ClientTimeout(total=time_out, sock_read=time_out, sock_connect=10.0)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        async with session.post(url, json=payload) as response:
            if response.status != 200:
                raise Exception(f"Ollama Error: {response.status}: {await response.text()}")
            return await response.json()


async def openai_ollama_chat_completions(messages: list, model_name="qwen3:14b", host='localhost', stream=False,
                                         time_out=300, **kwargs):
    """
      æ¨¡æ‹Ÿ OpenAI çš„è¿”å›ç»“æ„
      - å¦‚æœ stream=Trueï¼Œæ¨¡æ‹Ÿæµå¼è¿”å›
      - å¦‚æœ stream=Falseï¼Œè¿”å›å®Œæ•´ JSON å“åº”
      ['model', 'created_at', 'response', 'done', 'done_reason', 'context', 'total_duration', 'load_duration', 'prompt_eval_count', 'prompt_eval_duration', 'eval_count', 'eval_duration']
    """

    prompt = build_prompt(messages, use_role=True) + "\nAssistant:"
    response = await call_ollama(prompt, messages=messages, model_name=model_name, host=host, time_out=time_out,
                                 stream=stream, **kwargs)
    tokenizer = get_tokenizer(Config.DEFAULT_MODEL_ENCODING)
    if stream:
        async def stream_data():
            async for chunk in response:
                content = chunk.get("response", chunk.get('message', {}).get('content', ''))
                data = {"id": "chatcmpl-xyz", "object": "chat.completion.chunk",
                        "created": int(time.time()), "model": chunk.get('model', model_name),
                        "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]},
                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"  # (f"data: {data}\n\n").encode("utf-8")

        return stream_data()

    # print(json.dumps(response, indent=4))  # æ‰“å°å“åº”æ•°æ®
    content = response.get("response", response.get('message', {}).get('content', ''))
    return {
        "id": f"chatcmpl-{int(time.time())}",
        "object": "chat.completion",
        "created": int(time.time()),  # unix_timestamp(response["created_at"])
        "model": response.get('model', model_name),
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content,
                },
                "finish_reason": response.get('done_reason', "stop"),  # length/content_filter
            }
        ],
        "usage": {
            "prompt_tokens": lang_token_size(prompt, tokenizer),
            "completion_tokens": lang_token_size(content, tokenizer),  # .split()
            "total_tokens": lang_token_size(prompt, tokenizer) + lang_token_size(content, tokenizer),
        },
    }


__all__ = ['ai_analyze', 'ai_batch', 'ai_batch_result', 'ai_batch_run', 'ai_client_completions', 'ai_embeddings',
           'ai_files_messages', 'ai_generate_metadata', 'ai_reranker', 'ai_try', 'call_ollama', 'ai_speech_analyze',
           'dashscope_file_upload', 'find_closest_matches_embeddings',
           'get_similar_embeddings', 'get_similar_words',
           'openai_ollama_chat_completions', 'similarity_score_embeddings', 'stream_chat_completion',
           'Active_Stream_Events']

if __name__ == "__main__":
    from utils import get_module_functions

    funcs = get_module_functions('agents.ai_generates')
    print([i[0] for i in funcs])


    async def test():
        result = await call_ollama('ä½ å¥½', model_name="qwen3:14b", host='10.168.1.10', stream=False)

        print(result)
        print(result.keys())
        result = await call_ollama('ä½ å¥½å•Š', model_name="qwen3:14b", host='10.168.1.10', stream=True)
        # print(result)
        async for chunk in result:
            content = chunk.get("response")
            print(content, end="", flush=True)

        result = await call_ollama('ä½ å¥½å•Š', messages=[{"role": "user", "content": "Hello!"}], model_name="qwen3:14b",
                                   host='10.168.1.10', stream=True)
        print(result)
        async for chunk in result:
            content = chunk.get("message", [])
            print(content["content"], end="", flush=True)

        result = await openai_ollama_chat_completions(messages=[{"role": "user", "content": "ä½ å¥½å•Š"}],
                                                      model_name="qwen3:14b",
                                                      host='10.168.1.10', stream=False)
        print(result)

    # asyncio.run(test())
